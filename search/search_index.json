{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"foodspec","text":"<p>foodspec is a headless, research-grade Python toolkit for Raman and FTIR spectroscopy in food science. It provides a unified data model for 1D spectra and hyperspectral cubes, reproducible preprocessing pipelines, feature extraction, chemometrics, and domain-specific workflows such as oil authentication, heating degradation, and mixture modeling.</p> <p>Who is it for? - Food scientists and analytical chemists working with Raman/FTIR data. - Data scientists who want a clean, sklearn-style API for spectral analysis. - Researchers who care about reproducible, FAIR-compliant workflows aligned with a MethodsX-style protocol.</p> <p>What it solves - Consistent handling of spectra, metadata, and modalities (Raman/FTIR/NIR) via <code>FoodSpectrumSet</code> and <code>HyperSpectralCube</code>. - Pipeline-ready preprocessing (baseline, smoothing, scatter correction, normalization, FTIR/Raman helpers). - Chemometrics and ML (PCA/PLS, classifier factory, mixture analysis, QC/novelty detection). - Turnkey workflows (oil authentication, heating degradation, domain templates) with CLI + Python entry points. - Reporting, logging, configs, and spectral libraries for reproducible runs.</p> <p>Quick start</p> <pre><code>pip install foodspec\nfoodspec about\n</code></pre> <p>Where to start - Getting started \u2013 installation, basic examples, how to load data. - Libraries \u2013 building and loading spectral libraries, public dataset loaders. - Validation &amp; chemometrics \u2013 PCA and oil-authentication workflows. - MethodsX protocol \u2013 mapping between foodspec commands and the MethodsX article. - Citing foodspec \u2013 how to cite the software and the protocol paper.</p>"},{"location":"advanced_deep_learning/","title":"Advanced: Deep learning (optional)","text":"<p>Deep learning is not required to use foodspec or to reproduce the MethodsX protocol. However, an experimental 1D CNN classifier is provided for users who want to explore deep models on spectral data.</p>"},{"location":"advanced_deep_learning/#installation","title":"Installation","text":"<p>The deep module depends on TensorFlow and is not installed by default:</p> <pre><code>pip install 'foodspec[deep]'\n</code></pre> <p>If you call <code>Conv1DSpectrumClassifier</code> without TensorFlow installed, foodspec will raise a clear ImportError explaining how to install the extra.</p>"},{"location":"advanced_deep_learning/#conv1dspectrumclassifier","title":"Conv1DSpectrumClassifier","text":"<p>Example usage (assuming you have installed the deep extra):</p> <pre><code>import numpy as np\nfrom foodspec.chemometrics.deep import Conv1DSpectrumClassifier\nfrom foodspec.data.loader import load_example_oils\n\nds = load_example_oils()\nclf = Conv1DSpectrumClassifier(epochs=5, batch_size=16)\nclf.fit(ds.x, ds.metadata[\"oil_type\"])\nproba = clf.predict_proba(ds.x[:2])\npred = clf.predict(ds.x[:2])\nprint(pred, proba.shape)\n</code></pre> <p>This class follows an sklearn-like API (fit, predict, predict_proba), but is intended for experimental use only. For most applications, classical chemometric models (PLS, SVM, RF, etc.) are recommended.</p>"},{"location":"api_cli_apps/","title":"API reference: CLI &amp; applications","text":"<p>CLI entry point (<code>foodspec.cli</code>) exposes workflows; application modules orchestrate preprocessing, features, and models. For usage, see the CLI reference and workflow tutorials.</p>"},{"location":"api_cli_apps/#cli","title":"CLI","text":"<p>Command-line interface for foodspec.</p>"},{"location":"api_cli_apps/#foodspec.cli.about","title":"about","text":"<pre><code>about()\n</code></pre> <p>Print version and environment information for foodspec.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"about\")\ndef about() -&gt; None:\n    \"\"\"Print version and environment information for foodspec.\"\"\"\n\n    extras = _detect_optional_extras()\n    typer.echo(f\"foodspec version: {__version__}\")\n    typer.echo(f\"Python version: {sys.version.split()[0]}\")\n    typer.echo(f\"Optional extras detected: {', '.join(extras) if extras else 'none'}\")\n    typer.echo(\"Documentation: https://github.com/your-org/foodspec#documentation\")\n    typer.echo(\n        \"Description: foodspec is a headless, research-grade toolkit for Raman/FTIR in food science.\"\n    )\n</code></pre>"},{"location":"api_cli_apps/#foodspec.cli.csv_to_library","title":"csv_to_library","text":"<pre><code>csv_to_library(\n    csv_path=typer.Argument(\n        ..., help=\"Input CSV file with spectra.\"\n    ),\n    output_hdf5=typer.Argument(\n        ...,\n        help=\"Output HDF5 library path (will be created or overwritten).\",\n    ),\n    format=typer.Option(\n        \"wide\",\n        \"--format\",\n        help=\"CSV layout: 'wide' (one column per spectrum) or 'long' (tidy format).\",\n        case_sensitive=False,\n    ),\n    modality=typer.Option(\n        \"raman\",\n        \"--modality\",\n        help=\"Spectroscopy modality tag (e.g. 'raman', 'ftir').\",\n    ),\n    wavenumber_column=typer.Option(\n        \"wavenumber\",\n        \"--wavenumber-column\",\n        help=\"Name of the wavenumber column.\",\n    ),\n    sample_id_column=typer.Option(\n        \"sample_id\",\n        \"--sample-id-column\",\n        help=\"For 'long' format: sample identifier column.\",\n    ),\n    intensity_column=typer.Option(\n        \"intensity\",\n        \"--intensity-column\",\n        help=\"For 'long' format: intensity column.\",\n    ),\n    label_column=typer.Option(\n        \"\",\n        \"--label-column\",\n        help=\"Optional label column name (e.g. oil_type).\",\n    ),\n)\n</code></pre> <p>Convert a CSV file of spectra into an HDF5 library usable by foodspec workflows.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"csv-to-library\")\ndef csv_to_library(\n    csv_path: str = typer.Argument(..., help=\"Input CSV file with spectra.\"),\n    output_hdf5: str = typer.Argument(\n        ..., help=\"Output HDF5 library path (will be created or overwritten).\"\n    ),\n    format: str = typer.Option(\n        \"wide\",\n        \"--format\",\n        help=\"CSV layout: 'wide' (one column per spectrum) or 'long' (tidy format).\",\n        case_sensitive=False,\n    ),\n    modality: str = typer.Option(\n        \"raman\",\n        \"--modality\",\n        help=\"Spectroscopy modality tag (e.g. 'raman', 'ftir').\",\n    ),\n    wavenumber_column: str = typer.Option(\n        \"wavenumber\",\n        \"--wavenumber-column\",\n        help=\"Name of the wavenumber column.\",\n    ),\n    sample_id_column: str = typer.Option(\n        \"sample_id\",\n        \"--sample-id-column\",\n        help=\"For 'long' format: sample identifier column.\",\n    ),\n    intensity_column: str = typer.Option(\n        \"intensity\",\n        \"--intensity-column\",\n        help=\"For 'long' format: intensity column.\",\n    ),\n    label_column: str = typer.Option(\n        \"\",\n        \"--label-column\",\n        help=\"Optional label column name (e.g. oil_type).\",\n    ),\n):\n    \"\"\"\n    Convert a CSV file of spectra into an HDF5 library usable by foodspec workflows.\n    \"\"\"\n\n    label_column = label_column or None\n    logger.info(\"Loading CSV spectra from %s\", csv_path)\n    ds = load_csv_spectra(\n        csv_path=csv_path,\n        format=format,\n        wavenumber_column=wavenumber_column,\n        sample_id_column=sample_id_column,\n        intensity_column=intensity_column,\n        label_column=label_column,\n        modality=modality,\n    )\n\n    output_path = Path(output_hdf5)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    logger.info(\"Saving HDF5 library to %s\", output_hdf5)\n    create_library(path=output_hdf5, spectra=ds)\n    logger.info(\"Done. Library contains %s spectra.\", len(ds))\n</code></pre>"},{"location":"api_cli_apps/#foodspec.cli.domains_command","title":"domains_command","text":"<pre><code>domains_command(\n    input_hdf5=typer.Argument(\n        ..., help=\"Preprocessed spectra HDF5.\"\n    ),\n    domain=typer.Option(\n        ...,\n        \"--type\",\n        help=\"Domain type: dairy, meat, microbial.\",\n    ),\n    label_column=typer.Option(\n        \"label\", help=\"Metadata column with class labels.\"\n    ),\n    classifier_name=typer.Option(\n        \"rf\", help=\"Classifier name.\"\n    ),\n    cv_splits=typer.Option(5, help=\"Number of CV splits.\"),\n    output_dir=typer.Option(\n        \"./out\", help=\"Base output directory.\"\n    ),\n    save_model_path=typer.Option(\n        None,\n        \"--save-model\",\n        help=\"Optional base path to save the trained model (without extension).\",\n    ),\n    model_version=typer.Option(\n        __version__, help=\"Model version tag.\"\n    ),\n)\n</code></pre> <p>Run domain-specific authentication templates and write report.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"domains\")\ndef domains_command(\n    input_hdf5: str = typer.Argument(..., help=\"Preprocessed spectra HDF5.\"),\n    domain: str = typer.Option(..., \"--type\", help=\"Domain type: dairy, meat, microbial.\"),\n    label_column: str = typer.Option(\"label\", help=\"Metadata column with class labels.\"),\n    classifier_name: str = typer.Option(\"rf\", help=\"Classifier name.\"),\n    cv_splits: int = typer.Option(5, help=\"Number of CV splits.\"),\n    output_dir: str = typer.Option(\"./out\", help=\"Base output directory.\"),\n    save_model_path: Optional[str] = typer.Option(\n        None, \"--save-model\", help=\"Optional base path to save the trained model (without extension).\"\n    ),\n    model_version: str = typer.Option(__version__, help=\"Model version tag.\"),\n):\n    \"\"\"Run domain-specific authentication templates and write report.\"\"\"\n\n    ds = load_library(input_hdf5)\n    domain_lower = domain.lower()\n    if domain_lower == \"dairy\":\n        result = run_dairy_authentication_workflow(\n            ds, label_column=label_column, classifier_name=classifier_name, cv_splits=cv_splits\n        )\n    elif domain_lower == \"meat\":\n        result = run_meat_authentication_workflow(\n            ds, label_column=label_column, classifier_name=classifier_name, cv_splits=cv_splits\n        )\n    elif domain_lower == \"microbial\":\n        result = run_microbial_detection_workflow(\n            ds, label_column=label_column, classifier_name=classifier_name, cv_splits=cv_splits\n        )\n    else:\n        raise typer.BadParameter(\"domain must be one of: dairy, meat, microbial.\")\n\n    report_dir = _write_domain_report(result, Path(output_dir), domain=domain_lower, classifier_name=classifier_name)\n\n    if save_model_path is not None:\n        name = f\"{domain_lower}_{classifier_name.lower()}\"\n        registry_save_model(\n            result.pipeline,\n            save_model_path,\n            name=name,\n            version=model_version,\n            foodspec_version=__version__,\n            extra={\n                \"command\": \"domains\",\n                \"domain\": domain_lower,\n                \"classifier_name\": classifier_name,\n                \"label_column\": label_column,\n                \"cv_splits\": cv_splits,\n                \"class_labels\": list(result.class_labels),\n            },\n        )\n        typer.echo(f\"Model saved: {save_model_path}.joblib / {save_model_path}.json\")\n\n    typer.echo(f\"{domain} report: {report_dir}\")\n</code></pre>"},{"location":"api_cli_apps/#foodspec.cli.heating_command","title":"heating_command","text":"<pre><code>heating_command(\n    input_hdf5=typer.Argument(\n        ..., help=\"Preprocessed spectra HDF5.\"\n    ),\n    time_column=typer.Option(\n        \"heating_time\",\n        help=\"Metadata column for heating time.\",\n    ),\n    output_dir=typer.Option(\n        \"./out\", help=\"Base output directory.\"\n    ),\n)\n</code></pre> <p>Run heating degradation workflow and write report folder.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"heating\")\ndef heating_command(\n    input_hdf5: str = typer.Argument(..., help=\"Preprocessed spectra HDF5.\"),\n    time_column: str = typer.Option(\"heating_time\", help=\"Metadata column for heating time.\"),\n    output_dir: str = typer.Option(\"./out\", help=\"Base output directory.\"),\n):\n    \"\"\"Run heating degradation workflow and write report folder.\"\"\"\n\n    run_meta = log_run_metadata(logger, {\"command\": \"heating\"})\n    ds = load_library(input_hdf5)\n    result = run_heating_degradation_analysis(ds, time_column=time_column)\n    report_dir = _write_heating_report(result, Path(output_dir), time_column=time_column)\n    typer.echo(f\"Heating report: {report_dir}\")\n</code></pre>"},{"location":"api_cli_apps/#foodspec.cli.hyperspectral_command","title":"hyperspectral_command","text":"<pre><code>hyperspectral_command(\n    input_hdf5=typer.Argument(\n        ..., help=\"Flattened pixel spectra HDF5.\"\n    ),\n    height=typer.Option(\n        ..., help=\"Image height in pixels.\"\n    ),\n    width=typer.Option(..., help=\"Image width in pixels.\"),\n    target_wavenumber=typer.Option(\n        1655.0, help=\"Target wavenumber for intensity map.\"\n    ),\n    window=typer.Option(5.0, help=\"Integration window.\"),\n    output_dir=typer.Option(\n        \"./out\", help=\"Base output directory.\"\n    ),\n)\n</code></pre> <p>Create hyperspectral intensity map from flattened spectra.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"hyperspectral\")\ndef hyperspectral_command(\n    input_hdf5: str = typer.Argument(..., help=\"Flattened pixel spectra HDF5.\"),\n    height: int = typer.Option(..., help=\"Image height in pixels.\"),\n    width: int = typer.Option(..., help=\"Image width in pixels.\"),\n    target_wavenumber: float = typer.Option(1655.0, help=\"Target wavenumber for intensity map.\"),\n    window: float = typer.Option(5.0, help=\"Integration window.\"),\n    output_dir: str = typer.Option(\"./out\", help=\"Base output directory.\"),\n):\n    \"\"\"Create hyperspectral intensity map from flattened spectra.\"\"\"\n\n    ds = load_library(input_hdf5)\n    cube = HyperSpectralCube.from_spectrum_set(ds, image_shape=(height, width))\n    report_dir = _write_hyperspectral_report(\n        cube=cube, target_wavenumber=target_wavenumber, window=window, output_dir=Path(output_dir)\n    )\n    typer.echo(f\"Hyperspectral report: {report_dir}\")\n</code></pre>"},{"location":"api_cli_apps/#foodspec.cli.mixture_command","title":"mixture_command","text":"<pre><code>mixture_command(\n    input_hdf5=typer.Argument(\n        ..., help=\"Preprocessed spectra HDF5.\"\n    ),\n    pure_hdf5=typer.Option(\n        ..., help=\"HDF5 with pure component spectra.\"\n    ),\n    spectrum_index=typer.Option(\n        0,\n        help=\"Index of spectrum in input file to decompose.\",\n    ),\n    output_dir=typer.Option(\n        \"./out\", help=\"Base output directory.\"\n    ),\n)\n</code></pre> <p>Perform NNLS mixture analysis on a single spectrum and write report.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"mixture\")\ndef mixture_command(\n    input_hdf5: str = typer.Argument(..., help=\"Preprocessed spectra HDF5.\"),\n    pure_hdf5: str = typer.Option(..., help=\"HDF5 with pure component spectra.\"),\n    spectrum_index: int = typer.Option(0, help=\"Index of spectrum in input file to decompose.\"),\n    output_dir: str = typer.Option(\"./out\", help=\"Base output directory.\"),\n):\n    \"\"\"Perform NNLS mixture analysis on a single spectrum and write report.\"\"\"\n\n    spectra = load_library(input_hdf5)\n    pure = load_library(pure_hdf5)\n    if pure.wavenumbers.shape != spectra.wavenumbers.shape or not np.allclose(pure.wavenumbers, spectra.wavenumbers):\n        raise typer.BadParameter(\"Pure and input wavenumbers must match.\")\n    if spectrum_index &lt; 0 or spectrum_index &gt;= len(spectra):\n        raise typer.BadParameter(\"spectrum_index out of range.\")\n\n    spectrum = spectra.x[spectrum_index]\n    pure_mat = pure.x.T  # n_points x n_components\n    coeffs, res = nnls_mixture(spectrum, pure_mat)\n    reconstructed = pure_mat @ coeffs\n\n    fig, ax = plt.subplots()\n    ax.plot(spectra.wavenumbers, spectrum, label=\"original\")\n    ax.plot(spectra.wavenumbers, reconstructed, label=\"reconstructed\", linestyle=\"--\")\n    ax.set_xlabel(\"Wavenumber\")\n    ax.set_ylabel(\"Intensity\")\n    ax.legend()\n    report_dir = _write_mixture_report(\n        spectrum_index=spectrum_index,\n        coeffs=coeffs,\n        residual=res,\n        pure_labels=pure.metadata[\"sample_id\"] if \"sample_id\" in pure.metadata.columns else None,\n        output_dir=Path(output_dir),\n    )\n    save_figure(report_dir, \"mixture_fit\", fig)\n    plt.close(fig)\n    typer.echo(f\"Mixture report: {report_dir}\")\n</code></pre>"},{"location":"api_cli_apps/#foodspec.cli.model_info_command","title":"model_info_command","text":"<pre><code>model_info_command(\n    path=typer.Argument(\n        ...,\n        help=\"Base path of saved model (without extension).\",\n    )\n)\n</code></pre> <p>Inspect saved model metadata.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"model-info\")\ndef model_info_command(\n    path: str = typer.Argument(..., help=\"Base path of saved model (without extension).\"),\n):\n    \"\"\"Inspect saved model metadata.\"\"\"\n\n    model_base = Path(path)\n    joblib_path = model_base.with_suffix(\".joblib\")\n    json_path = model_base.with_suffix(\".json\")\n    if not joblib_path.exists() or not json_path.exists():\n        typer.echo(\"Model files not found (expected .joblib and .json).\", err=True)\n        raise typer.Exit(code=1)\n    try:\n        _, meta = registry_load_model(path)\n    except Exception as exc:  # pragma: no cover - defensive\n        typer.echo(f\"Failed to load model metadata: {exc}\", err=True)\n        raise typer.Exit(code=1)\n    typer.echo(f\"Name: {meta.name}\")\n    typer.echo(f\"Version: {meta.version}\")\n    typer.echo(f\"Foodspec version: {meta.foodspec_version}\")\n    typer.echo(f\"Created at: {meta.created_at}\")\n    typer.echo(\"Extra:\")\n    typer.echo(json.dumps(meta.extra, indent=2))\n</code></pre>"},{"location":"api_cli_apps/#foodspec.cli.oil_auth","title":"oil_auth","text":"<pre><code>oil_auth(\n    input_hdf5=typer.Argument(\n        ..., help=\"Input HDF5 file with spectra.\"\n    ),\n    label_column=typer.Option(\n        \"oil_type\", help=\"Metadata column for class labels.\"\n    ),\n    cv_splits=typer.Option(\n        5, help=\"CV splits for classifier.\"\n    ),\n    output_report=typer.Option(\n        \"oil_auth_report.html\",\n        help=\"Output HTML report path.\",\n    ),\n    save_model_path=typer.Option(\n        None,\n        \"--save-model\",\n        help=\"Optional base path to save the trained model (without extension).\",\n    ),\n    model_version=typer.Option(\n        __version__, help=\"Model version tag.\"\n    ),\n    config=typer.Option(\n        None,\n        \"--config\",\n        help=\"Optional YAML/JSON config file.\",\n    ),\n)\n</code></pre> <p>Run oil authentication workflow and save HTML report.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"oil-auth\")\ndef oil_auth(\n    input_hdf5: str = typer.Argument(..., help=\"Input HDF5 file with spectra.\"),\n    label_column: str = typer.Option(\"oil_type\", help=\"Metadata column for class labels.\"),\n    cv_splits: int = typer.Option(5, help=\"CV splits for classifier.\"),\n    output_report: str = typer.Option(\"oil_auth_report.html\", help=\"Output HTML report path.\"),\n    save_model_path: Optional[str] = typer.Option(\n        None, \"--save-model\", help=\"Optional base path to save the trained model (without extension).\"\n    ),\n    model_version: str = typer.Option(__version__, help=\"Model version tag.\"),\n    config: Optional[str] = typer.Option(None, \"--config\", help=\"Optional YAML/JSON config file.\"),\n):\n    \"\"\"Run oil authentication workflow and save HTML report.\"\"\"\n\n    run_meta = log_run_metadata(logger, {\"command\": \"oil-auth\"})\n    base_cfg = {\"input_hdf5\": input_hdf5, \"label_column\": label_column, \"output_report\": output_report, \"cv_splits\": cv_splits}\n    cfg = load_config(config) if config else base_cfg\n    cfg = merge_cli_overrides(cfg, base_cfg)\n\n    ds = load_library(cfg[\"input_hdf5\"])\n    result = run_oil_authentication_workflow(\n        spectra=ds,\n        label_column=cfg.get(\"label_column\", label_column),\n        cv_splits=cfg.get(\"cv_splits\", cv_splits),\n    )\n    render_html_report_oil_auth(result, cfg.get(\"output_report\", output_report))\n    classifier_name = result.pipeline.named_steps.get(\"clf\").__class__.__name__ if result.pipeline else \"unknown\"\n    report_dir = _write_oil_report(\n        result,\n        ds,\n        label_column=label_column,\n        output_report=Path(output_report),\n        classifier_name=classifier_name,\n        run_metadata=run_meta,\n    )\n    typer.echo(f\"Report folder: {report_dir}\")\n    if save_model_path is not None:\n        name = f\"oil_{classifier_name.lower()}\"\n        registry_save_model(\n            result.pipeline,\n            save_model_path,\n            name=name,\n            version=model_version,\n            foodspec_version=__version__,\n            extra={\n                \"command\": \"oil-auth\",\n                \"label_column\": label_column,\n                \"classifier_name\": classifier_name,\n                \"class_labels\": list(result.class_labels),\n            },\n        )\n        typer.echo(f\"Model saved: {save_model_path}.joblib / {save_model_path}.json\")\n    typer.echo(f\"HTML report written to {output_report}\")\n</code></pre>"},{"location":"api_cli_apps/#foodspec.cli.preprocess","title":"preprocess","text":"<pre><code>preprocess(\n    input_folder=typer.Argument(\n        ..., help=\"Folder containing spectra text files.\"\n    ),\n    metadata_csv=typer.Option(\n        None, help=\"Optional metadata CSV with sample_id.\"\n    ),\n    output_hdf5=typer.Argument(\n        ..., help=\"Output HDF5 path.\"\n    ),\n    modality=typer.Option(\n        \"raman\", help=\"Spectroscopy modality.\"\n    ),\n    min_wn=typer.Option(\n        600.0, help=\"Minimum wavenumber for cropping.\"\n    ),\n    max_wn=typer.Option(\n        1800.0, help=\"Maximum wavenumber for cropping.\"\n    ),\n)\n</code></pre> <p>Load spectra, apply default preprocessing, and save to HDF5.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"preprocess\")\ndef preprocess(\n    input_folder: str = typer.Argument(..., help=\"Folder containing spectra text files.\"),\n    metadata_csv: Optional[str] = typer.Option(None, help=\"Optional metadata CSV with sample_id.\"),\n    output_hdf5: str = typer.Argument(..., help=\"Output HDF5 path.\"),\n    modality: str = typer.Option(\"raman\", help=\"Spectroscopy modality.\"),\n    min_wn: float = typer.Option(600.0, help=\"Minimum wavenumber for cropping.\"),\n    max_wn: float = typer.Option(1800.0, help=\"Maximum wavenumber for cropping.\"),\n):\n    \"\"\"Load spectra, apply default preprocessing, and save to HDF5.\"\"\"\n\n    ds = load_folder(\n        folder=input_folder,\n        metadata_csv=metadata_csv,\n        modality=modality,\n    )\n    pipe = _default_preprocess_pipeline(ds.wavenumbers, min_wn=min_wn, max_wn=max_wn)\n    x_proc = pipe.fit_transform(ds.x)\n    cropper = pipe.named_steps[\"crop\"]\n    ds_out = FoodSpectrumSet(\n        x=x_proc,\n        wavenumbers=cropper.wavenumbers_,\n        metadata=ds.metadata.copy(),\n        modality=ds.modality,\n    )\n    to_hdf5(ds_out, output_hdf5)\n    typer.echo(f\"Preprocessed spectra saved to {output_hdf5}\")\n</code></pre>"},{"location":"api_cli_apps/#foodspec.cli.protocol_benchmarks","title":"protocol_benchmarks","text":"<pre><code>protocol_benchmarks(\n    output_dir=typer.Option(\n        \"./protocol_benchmarks\",\n        help=\"Directory to write benchmark metrics.\",\n    ),\n    random_state=typer.Option(42, help=\"Random seed.\"),\n    config=typer.Option(\n        None, \"--config\", help=\"Optional YAML/JSON config.\"\n    ),\n)\n</code></pre> <p>Run protocol benchmarks on public datasets and save reports.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"protocol-benchmarks\")\ndef protocol_benchmarks(\n    output_dir: str = typer.Option(\"./protocol_benchmarks\", help=\"Directory to write benchmark metrics.\"),\n    random_state: int = typer.Option(42, help=\"Random seed.\"),\n    config: Optional[str] = typer.Option(None, \"--config\", help=\"Optional YAML/JSON config.\"),\n):\n    \"\"\"Run protocol benchmarks on public datasets and save reports.\"\"\"\n\n    base_cfg = {\"output_dir\": output_dir, \"random_state\": random_state}\n    cfg = load_config(config) if config else base_cfg\n    cfg = merge_cli_overrides(cfg, base_cfg)\n\n    out_path = Path(cfg[\"output_dir\"])\n    run_meta = log_run_metadata(logger, {\"command\": \"protocol-benchmarks\"})\n    summary = run_protocol_benchmarks(out_path, random_state=cfg.get(\"random_state\", random_state))\n    # write run metadata alongside metrics\n    meta_path = out_path / \"run_metadata.json\"\n    meta_path.parent.mkdir(parents=True, exist_ok=True)\n    meta_path.write_text(json.dumps(run_meta, indent=2), encoding=\"utf-8\")\n    typer.echo(\"Protocol benchmarks summary:\")\n    typer.echo(json.dumps(summary, indent=2))\n</code></pre>"},{"location":"api_cli_apps/#foodspec.cli.qc_command","title":"qc_command","text":"<pre><code>qc_command(\n    input_hdf5=typer.Argument(\n        ..., help=\"Preprocessed spectra HDF5.\"\n    ),\n    model_type=typer.Option(\n        \"oneclass_svm\",\n        help=\"QC model type: oneclass_svm or isolation_forest.\",\n    ),\n    label_column=typer.Option(\n        None, help=\"Optional label column for inspection.\"\n    ),\n    output_dir=typer.Option(\n        \"./out\", help=\"Base output directory.\"\n    ),\n)\n</code></pre> <p>Run QC/novelty detection and write report.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"qc\")\ndef qc_command(\n    input_hdf5: str = typer.Argument(..., help=\"Preprocessed spectra HDF5.\"),\n    model_type: str = typer.Option(\"oneclass_svm\", help=\"QC model type: oneclass_svm or isolation_forest.\"),\n    label_column: Optional[str] = typer.Option(None, help=\"Optional label column for inspection.\"),\n    output_dir: str = typer.Option(\"./out\", help=\"Base output directory.\"),\n):\n    \"\"\"Run QC/novelty detection and write report.\"\"\"\n\n    ds = load_library(input_hdf5)\n    model = train_qc_model(ds, train_mask=None, model_type=model_type)\n    qc_result = apply_qc_model(ds, model=model, metadata=ds.metadata)\n    report_dir = _write_qc_report(qc_result, Path(output_dir), model_type=model_type, threshold=qc_result.threshold)\n    typer.echo(f\"QC report: {report_dir}\")\n</code></pre>"},{"location":"api_cli_apps/#foodspec.cli.reproduce_methodsx","title":"reproduce_methodsx","text":"<pre><code>reproduce_methodsx(\n    output_dir=typer.Option(\n        \"./methodsx_runs\",\n        help=\"Directory to write MethodsX reproduction artifacts.\",\n    ),\n    random_state=typer.Option(42, help=\"Random seed.\"),\n)\n</code></pre> <p>Run MethodsX protocol reproduction analyses and write artifacts.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"reproduce-methodsx\")\ndef reproduce_methodsx(\n    output_dir: str = typer.Option(\"./methodsx_runs\", help=\"Directory to write MethodsX reproduction artifacts.\"),\n    random_state: int = typer.Option(42, help=\"Random seed.\"),\n):\n    \"\"\"Run MethodsX protocol reproduction analyses and write artifacts.\"\"\"\n\n    run_meta = log_run_metadata(logger, {\"command\": \"reproduce-methodsx\"})\n    summary = run_methodsx_reproduction(output_dir=output_dir, random_state=random_state)\n    # Save run metadata\n    run_dir = Path(summary.get(\"run_dir\", output_dir))\n    run_dir.mkdir(parents=True, exist_ok=True)\n    (run_dir / \"run_metadata.json\").write_text(json.dumps(run_meta, indent=2), encoding=\"utf-8\")\n    typer.echo(\"MethodsX reproduction summary:\")\n    typer.echo(json.dumps(summary, indent=2))\n</code></pre>"},{"location":"api_cli_apps/#applications","title":"Applications","text":"<p>Edible oil authentication workflow.</p> <p>Heating degradation analysis (stub).</p> <p>Quality control / novelty detection utilities.</p> <p>Dairy authentication template.</p> <p>Meat authentication template.</p> <p>Microbial detection template.</p> <p>Protocol-level benchmarks on public datasets.</p> <p>Reproduce the core analyses from the MethodsX protocol.</p> <p>Status: Archived This page has been superseded by the unified API reference. See api_reference.md for current application and CLI APIs.</p>"},{"location":"api_cli_apps/#foodspec.apps.oils.OilAuthResult","title":"OilAuthResult  <code>dataclass</code>","text":"<p>Results of the oil authentication workflow.</p> Source code in <code>src/foodspec/apps/oils.py</code> <pre><code>@dataclass\nclass OilAuthResult:\n    \"\"\"Results of the oil authentication workflow.\"\"\"\n\n    pipeline: Pipeline\n    cv_metrics: pd.DataFrame\n    confusion_matrix: np.ndarray\n    class_labels: List[str]\n    feature_importances: Optional[pd.Series]\n</code></pre>"},{"location":"api_cli_apps/#foodspec.apps.oils.default_oil_feature_pipeline","title":"default_oil_feature_pipeline","text":"<pre><code>default_oil_feature_pipeline(wavenumbers)\n</code></pre> <p>Feature pipeline for oil peaks and ratios.</p> Source code in <code>src/foodspec/apps/oils.py</code> <pre><code>def default_oil_feature_pipeline(wavenumbers: np.ndarray) -&gt; Pipeline:\n    \"\"\"Feature pipeline for oil peaks and ratios.\"\"\"\n\n    # Typical oil bands (approx): 1655 (C=C), 1742 (C=O), 1450 (CH2 bend)\n    expected_peaks = [1655.0, 1742.0, 1450.0]\n    ratio_def = {\n        \"ratio_1655_1742\": (\"peak_1655.0_height\", \"peak_1742.0_height\"),\n        \"ratio_1450_1655\": (\"peak_1450.0_height\", \"peak_1655.0_height\"),\n    }\n\n    return Pipeline(\n        steps=[\n            (\"peaks\", _PeakFeatureTransformer(wavenumbers=wavenumbers, expected_peaks=expected_peaks)),\n            (\"ratios\", _RatioFeatureTransformer(ratio_def=ratio_def)),\n            (\"to_array\", _DataFrameToArray()),\n        ]\n    )\n</code></pre>"},{"location":"api_cli_apps/#foodspec.apps.oils.default_oil_preprocessing_pipeline","title":"default_oil_preprocessing_pipeline","text":"<pre><code>default_oil_preprocessing_pipeline(wavenumbers)\n</code></pre> <p>Baseline, smoothing, normalization, and fingerprint cropping.</p> Source code in <code>src/foodspec/apps/oils.py</code> <pre><code>def default_oil_preprocessing_pipeline(wavenumbers: np.ndarray) -&gt; Pipeline:\n    \"\"\"Baseline, smoothing, normalization, and fingerprint cropping.\"\"\"\n\n    return Pipeline(\n        steps=[\n            (\"als\", ALSBaseline(lambda_=1e5, p=0.01, max_iter=10)),\n            (\"savgol\", SavitzkyGolaySmoother(window_length=9, polyorder=3)),\n            (\"norm\", VectorNormalizer(norm=\"l2\")),\n            (\"crop\", _RangeCropperTransformer(wavenumbers=wavenumbers, min_wn=600, max_wn=1800)),\n        ]\n    )\n</code></pre>"},{"location":"api_cli_apps/#foodspec.apps.oils.run_oil_authentication_workflow","title":"run_oil_authentication_workflow","text":"<pre><code>run_oil_authentication_workflow(\n    spectra,\n    label_column=\"oil_type\",\n    classifier_name=\"rf\",\n    cv_splits=5,\n)\n</code></pre> <p>Run oil authentication pipeline with cross-validation.</p> Source code in <code>src/foodspec/apps/oils.py</code> <pre><code>def run_oil_authentication_workflow(\n    spectra: FoodSpectrumSet,\n    label_column: str = \"oil_type\",\n    classifier_name: str = \"rf\",\n    cv_splits: int = 5,\n) -&gt; OilAuthResult:\n    \"\"\"Run oil authentication pipeline with cross-validation.\"\"\"\n\n    validate_spectrum_set(spectra)\n    if label_column not in spectra.metadata.columns:\n        raise ValueError(f\"Label column '{label_column}' not found in metadata.\")\n\n    X = spectra.x\n    y = spectra.metadata[label_column].to_numpy()\n    classes = np.unique(y)\n\n    preproc = default_oil_preprocessing_pipeline(spectra.wavenumbers)\n    cropped_axis = preproc.named_steps[\"crop\"].wavenumbers_\n    feat_pipe = default_oil_feature_pipeline(cropped_axis)\n    clf = make_classifier(classifier_name)\n\n    pipeline = Pipeline(\n        steps=[\n            (\"preprocess\", preproc),\n            (\"features\", feat_pipe),\n            (\"clf\", clf),\n        ]\n    )\n\n    skf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=0)\n    fold_rows = []\n    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), start=1):\n        pipeline.fit(X[train_idx], y[train_idx])\n        preds = pipeline.predict(X[test_idx])\n        y_true_fold = y[test_idx]\n        metrics_df = compute_classification_metrics(y_true_fold, preds)\n        metrics_row = metrics_df.iloc[0].to_dict()\n        metrics_row[\"fold\"] = fold\n        fold_rows.append(metrics_row)\n\n    metrics_df = pd.DataFrame(fold_rows)\n    summary = metrics_df.drop(columns=[\"fold\"]).agg([\"mean\", \"std\"])\n    cv_metrics = pd.concat([metrics_df, summary.reset_index().rename(columns={\"index\": \"fold\"})])\n\n    # Fit on full dataset\n    pipeline.fit(X, y)\n    preds_full = pipeline.predict(X)\n    cm = confusion_matrix(y, preds_full, labels=classes)\n\n    feature_importances = None\n    clf_est = pipeline.named_steps[\"clf\"]\n    feature_names = pipeline.named_steps[\"features\"].named_steps[\"to_array\"].columns_\n    if hasattr(clf_est, \"feature_importances_\"):\n        feature_importances = pd.Series(\n            clf_est.feature_importances_, index=feature_names, name=\"importance\"\n        )\n\n    return OilAuthResult(\n        pipeline=pipeline,\n        cv_metrics=cv_metrics,\n        confusion_matrix=cm,\n        class_labels=classes.tolist(),\n        feature_importances=feature_importances,\n    )\n</code></pre>"},{"location":"api_cli_apps/#foodspec.apps.heating.run_heating_degradation_analysis","title":"run_heating_degradation_analysis","text":"<pre><code>run_heating_degradation_analysis(\n    spectra, time_column=\"heating_time\"\n)\n</code></pre> <p>Run heating degradation analysis.</p> <p>Applies baseline/smoothing/normalization/cropping, extracts a simple peak ratio versus heating time, fits trend regressions, and computes a basic ANOVA if groups are present.</p> Source code in <code>src/foodspec/apps/heating.py</code> <pre><code>def run_heating_degradation_analysis(\n    spectra: FoodSpectrumSet,\n    time_column: str = \"heating_time\",\n) -&gt; HeatingAnalysisResult:\n    \"\"\"Run heating degradation analysis.\n\n    Applies baseline/smoothing/normalization/cropping, extracts a simple\n    peak ratio versus heating time, fits trend regressions, and computes\n    a basic ANOVA if groups are present.\n    \"\"\"\n\n    validate_spectrum_set(spectra)\n    if time_column not in spectra.metadata.columns:\n        raise ValueError(f\"Metadata column '{time_column}' not found.\")\n\n    preproc = _default_heating_preprocess(spectra.wavenumbers)\n    X_proc = preproc.transform(spectra.x)\n    wn_proc = preproc.named_steps[\"crop\"].wavenumbers_\n\n    extractor = PeakFeatureExtractor(expected_peaks=[1655.0, 1742.0], tolerance=8.0)\n    extractor.fit(X_proc, wavenumbers=wn_proc)\n    peak_feats = extractor.transform(X_proc, wavenumbers=wn_proc)\n    peak_df = pd.DataFrame(\n        peak_feats, columns=extractor.get_feature_names_out(), index=spectra.metadata.index\n    )\n    ratios = RatioFeatureGenerator({\"ratio_1655_1742\": (\"peak_1655.0_height\", \"peak_1742.0_height\")})\n    ratio_df = ratios.transform(peak_df)\n\n    trend_models: Dict[str, Any] = {}\n    time_values = spectra.metadata[time_column].to_numpy().reshape(-1, 1)\n    for col in ratio_df.columns:\n        model = LinearRegression()\n        model.fit(time_values, ratio_df[col].to_numpy())\n        trend_models[col] = model\n\n    # Optional group-wise models if oil_type present\n    if \"oil_type\" in spectra.metadata.columns:\n        grouped_models: Dict[str, Dict[str, Any]] = {}\n        for col in ratio_df.columns:\n            grouped_models[col] = {}\n            for group, idxs in spectra.metadata.groupby(\"oil_type\").groups.items():\n                Xg = time_values[list(idxs)]\n                yg = ratio_df[col].iloc[list(idxs)].to_numpy()\n                if len(yg) &gt;= 2:\n                    m = LinearRegression()\n                    m.fit(Xg, yg)\n                    grouped_models[col][group] = m\n        trend_models[\"by_oil_type\"] = grouped_models\n\n    anova_results = None\n    if \"oil_type\" in spectra.metadata.columns and spectra.metadata[\"oil_type\"].nunique() &gt;= 2:\n        rows = []\n        for col in ratio_df.columns:\n            groups = []\n            for _, idxs in spectra.metadata.groupby(\"oil_type\").groups.items():\n                vals = ratio_df[col].iloc[list(idxs)].to_numpy()\n                if len(vals) &gt; 0:\n                    groups.append(vals)\n            if len(groups) &gt;= 2:\n                F, p = stats.f_oneway(*groups)\n                rows.append({\"factor\": \"oil_type\", \"metric\": col, \"F\": F, \"pvalue\": p})\n        if rows:\n            anova_results = pd.DataFrame(rows)\n\n    return HeatingAnalysisResult(\n        preprocessed_spectra=X_proc,\n        wavenumbers=wn_proc,\n        time_variable=spectra.metadata[time_column],\n        key_ratios=ratio_df,\n        trend_models=trend_models,\n        anova_results=anova_results,\n    )\n</code></pre>"},{"location":"api_cli_apps/#foodspec.apps.qc.apply_qc_model","title":"apply_qc_model","text":"<pre><code>apply_qc_model(\n    spectra,\n    model,\n    threshold=None,\n    higher_score_is_more_normal=True,\n    metadata=None,\n)\n</code></pre> <p>Score spectra with a novelty model and produce QC labels.</p> Source code in <code>src/foodspec/apps/qc.py</code> <pre><code>def apply_qc_model(\n    spectra: FoodSpectrumSet,\n    model: Any,\n    threshold: Optional[float] = None,\n    higher_score_is_more_normal: bool = True,\n    metadata: Optional[pd.DataFrame] = None,\n) -&gt; QCResult:\n    \"\"\"Score spectra with a novelty model and produce QC labels.\"\"\"\n\n    X = spectra.x\n    if hasattr(model, \"decision_function\"):\n        scores = model.decision_function(X)\n    elif hasattr(model, \"score_samples\"):\n        scores = model.score_samples(X)\n    else:\n        raise ValueError(\"Model must implement decision_function or score_samples.\")\n\n    scores_ser = pd.Series(scores, index=spectra.metadata.index)\n\n    if threshold is None:\n        if isinstance(model, OneClassSVM):\n            threshold = float(np.quantile(scores, 0.2))\n        else:\n            threshold = float(np.median(scores))\n    if higher_score_is_more_normal:\n        labels_arr = np.where(scores &gt;= threshold, \"authentic\", \"suspect\")\n    else:\n        labels_arr = np.where(scores &lt;= threshold, \"authentic\", \"suspect\")\n    labels_ser = pd.Series(labels_arr, index=spectra.metadata.index)\n\n    meta = metadata.copy() if metadata is not None else spectra.metadata.copy()\n\n    return QCResult(\n        scores=scores_ser,\n        labels_pred=labels_ser,\n        threshold=threshold,\n        model=model,\n        metadata=meta,\n    )\n</code></pre>"},{"location":"api_cli_apps/#foodspec.apps.qc.train_qc_model","title":"train_qc_model","text":"<pre><code>train_qc_model(\n    spectra,\n    train_mask=None,\n    model_type=\"oneclass_svm\",\n    **kwargs\n)\n</code></pre> <p>Train a novelty detection model on authentic spectra.</p> Source code in <code>src/foodspec/apps/qc.py</code> <pre><code>def train_qc_model(\n    spectra: FoodSpectrumSet,\n    train_mask: Optional[pd.Series] = None,\n    model_type: str = \"oneclass_svm\",\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Train a novelty detection model on authentic spectra.\"\"\"\n\n    X = spectra.x\n    if train_mask is not None:\n        train_idx = train_mask.to_numpy()\n        X = X[train_idx]\n\n    name = model_type.lower()\n    if name == \"oneclass_svm\":\n        gamma = kwargs.pop(\"gamma\", \"scale\")\n        nu = kwargs.pop(\"nu\", 0.05)\n        model = OneClassSVM(kernel=\"rbf\", gamma=gamma, nu=nu, **kwargs)\n    elif name == \"isolation_forest\":\n        model = IsolationForest(contamination=0.05, random_state=0, **kwargs)\n    else:\n        raise ValueError(\"model_type must be 'oneclass_svm' or 'isolation_forest'.\")\n\n    model.fit(X)\n    return model\n</code></pre>"},{"location":"api_cli_apps/#foodspec.apps.dairy.DairyAnalysisResult","title":"DairyAnalysisResult  <code>dataclass</code>","text":"<p>Result container for dairy authentication.</p> Source code in <code>src/foodspec/apps/dairy.py</code> <pre><code>@dataclass\nclass DairyAnalysisResult:\n    \"\"\"Result container for dairy authentication.\"\"\"\n\n    preprocessed_spectra: np.ndarray\n    wavenumbers: np.ndarray\n    cv_metrics: pd.DataFrame\n    confusion_matrix: np.ndarray\n    class_labels: list[str]\n</code></pre>"},{"location":"api_cli_apps/#foodspec.apps.dairy.run_dairy_authentication_workflow","title":"run_dairy_authentication_workflow","text":"<pre><code>run_dairy_authentication_workflow(\n    spectra,\n    label_column=\"label\",\n    classifier_name=\"rf\",\n    cv_splits=5,\n)\n</code></pre> <p>Apply a generic authentication workflow to dairy spectra.</p> Source code in <code>src/foodspec/apps/dairy.py</code> <pre><code>def run_dairy_authentication_workflow(\n    spectra: FoodSpectrumSet,\n    label_column: str = \"label\",\n    classifier_name: str = \"rf\",\n    cv_splits: int = 5,\n) -&gt; DairyAnalysisResult:\n    \"\"\"Apply a generic authentication workflow to dairy spectra.\"\"\"\n    result = run_oil_authentication_workflow(\n        spectra=spectra, label_column=label_column, classifier_name=classifier_name, cv_splits=cv_splits\n    )\n    preprocess = result.pipeline.named_steps.get(\"preprocess\")\n    if preprocess is not None:\n        x_proc = preprocess.transform(spectra.x)\n        wn_proc = preprocess.named_steps[\"crop\"].wavenumbers_\n    else:\n        x_proc = spectra.x\n        wn_proc = spectra.wavenumbers\n\n    return DairyAnalysisResult(\n        preprocessed_spectra=x_proc,\n        wavenumbers=wn_proc,\n        cv_metrics=result.cv_metrics,\n        confusion_matrix=result.confusion_matrix,\n        class_labels=result.class_labels,\n    )\n</code></pre>"},{"location":"api_cli_apps/#foodspec.apps.meat.MeatAnalysisResult","title":"MeatAnalysisResult  <code>dataclass</code>","text":"<p>Result container for meat authentication.</p> Source code in <code>src/foodspec/apps/meat.py</code> <pre><code>@dataclass\nclass MeatAnalysisResult:\n    \"\"\"Result container for meat authentication.\"\"\"\n\n    preprocessed_spectra: np.ndarray\n    wavenumbers: np.ndarray\n    cv_metrics: pd.DataFrame\n    confusion_matrix: np.ndarray\n    class_labels: list[str]\n</code></pre>"},{"location":"api_cli_apps/#foodspec.apps.meat.run_meat_authentication_workflow","title":"run_meat_authentication_workflow","text":"<pre><code>run_meat_authentication_workflow(\n    spectra,\n    label_column=\"label\",\n    classifier_name=\"rf\",\n    cv_splits=5,\n)\n</code></pre> <p>Apply a generic authentication workflow to meat spectra.</p> Source code in <code>src/foodspec/apps/meat.py</code> <pre><code>def run_meat_authentication_workflow(\n    spectra: FoodSpectrumSet,\n    label_column: str = \"label\",\n    classifier_name: str = \"rf\",\n    cv_splits: int = 5,\n) -&gt; MeatAnalysisResult:\n    \"\"\"Apply a generic authentication workflow to meat spectra.\"\"\"\n    result = run_oil_authentication_workflow(\n        spectra=spectra, label_column=label_column, classifier_name=classifier_name, cv_splits=cv_splits\n    )\n    preprocess = result.pipeline.named_steps.get(\"preprocess\")\n    if preprocess is not None:\n        x_proc = preprocess.transform(spectra.x)\n        wn_proc = preprocess.named_steps[\"crop\"].wavenumbers_\n    else:\n        x_proc = spectra.x\n        wn_proc = spectra.wavenumbers\n\n    return MeatAnalysisResult(\n        preprocessed_spectra=x_proc,\n        wavenumbers=wn_proc,\n        cv_metrics=result.cv_metrics,\n        confusion_matrix=result.confusion_matrix,\n        class_labels=result.class_labels,\n    )\n</code></pre>"},{"location":"api_cli_apps/#foodspec.apps.microbial.MicrobialAnalysisResult","title":"MicrobialAnalysisResult  <code>dataclass</code>","text":"<p>Result container for microbial detection.</p> Source code in <code>src/foodspec/apps/microbial.py</code> <pre><code>@dataclass\nclass MicrobialAnalysisResult:\n    \"\"\"Result container for microbial detection.\"\"\"\n\n    preprocessed_spectra: np.ndarray\n    wavenumbers: np.ndarray\n    cv_metrics: pd.DataFrame\n    confusion_matrix: np.ndarray\n    class_labels: list[str]\n</code></pre>"},{"location":"api_cli_apps/#foodspec.apps.microbial.run_microbial_detection_workflow","title":"run_microbial_detection_workflow","text":"<pre><code>run_microbial_detection_workflow(\n    spectra,\n    label_column=\"label\",\n    classifier_name=\"rf\",\n    cv_splits=5,\n)\n</code></pre> <p>Apply a generic authentication workflow to microbial spectra.</p> Source code in <code>src/foodspec/apps/microbial.py</code> <pre><code>def run_microbial_detection_workflow(\n    spectra: FoodSpectrumSet,\n    label_column: str = \"label\",\n    classifier_name: str = \"rf\",\n    cv_splits: int = 5,\n) -&gt; MicrobialAnalysisResult:\n    \"\"\"Apply a generic authentication workflow to microbial spectra.\"\"\"\n    result = run_oil_authentication_workflow(\n        spectra=spectra, label_column=label_column, classifier_name=classifier_name, cv_splits=cv_splits\n    )\n    preprocess = result.pipeline.named_steps.get(\"preprocess\")\n    if preprocess is not None:\n        x_proc = preprocess.transform(spectra.x)\n        wn_proc = preprocess.named_steps[\"crop\"].wavenumbers_\n    else:\n        x_proc = spectra.x\n        wn_proc = spectra.wavenumbers\n\n    return MicrobialAnalysisResult(\n        preprocessed_spectra=x_proc,\n        wavenumbers=wn_proc,\n        cv_metrics=result.cv_metrics,\n        confusion_matrix=result.confusion_matrix,\n        class_labels=result.class_labels,\n    )\n</code></pre>"},{"location":"api_cli_apps/#foodspec.apps.protocol_validation.run_protocol_benchmarks","title":"run_protocol_benchmarks","text":"<pre><code>run_protocol_benchmarks(output_dir, random_state=42)\n</code></pre> <p>Run core protocol benchmarks on public datasets and save reports.</p> <p>Returns a dict summarizing metrics.</p> Source code in <code>src/foodspec/apps/protocol_validation.py</code> <pre><code>def run_protocol_benchmarks(\n    output_dir: PathLike,\n    random_state: int = 42,\n) -&gt; dict:\n    \"\"\"\n    Run core protocol benchmarks on public datasets and save reports.\n\n    Returns a dict summarizing metrics.\n    \"\"\"\n\n    out_base = Path(output_dir)\n    run_dir = create_run_dir(out_base, \"protocol\")\n    summary: Dict[str, Dict] = {}\n\n    # Classification benchmark\n    try:\n        metrics_cls, cm_df = _classification_benchmark(random_state=random_state)\n        summary[\"classification\"] = metrics_cls\n        write_json(run_dir / \"classification_metrics.json\", metrics_cls)\n        cm_df.to_csv(run_dir / \"classification_confusion_matrix.csv\")\n    except FileNotFoundError as exc:\n        summary[\"classification_error\"] = str(exc)\n\n    # Mixture benchmark\n    try:\n        metrics_mix = _mixture_benchmark(random_state=random_state)\n        summary[\"mixture\"] = metrics_mix\n        write_json(run_dir / \"mixture_metrics.json\", metrics_mix)\n    except FileNotFoundError as exc:\n        summary[\"mixture_error\"] = str(exc)\n\n    # Report\n    report_sections = {\n        \"Overview\": \"Protocol benchmarks on public datasets (oil classification, mixture regression).\",\n        \"Metrics\": summarize_metrics_for_markdown(summary),\n    }\n    write_markdown_report(run_dir / \"report.md\", title=\"Protocol Benchmarks\", sections=report_sections)\n    summary[\"run_dir\"] = str(run_dir)\n    return summary\n</code></pre>"},{"location":"api_cli_apps/#foodspec.apps.methodsx_reproduction.run_methodsx_reproduction","title":"run_methodsx_reproduction","text":"<pre><code>run_methodsx_reproduction(output_dir, random_state=42)\n</code></pre> <p>Run the core analyses used in the MethodsX protocol article and save artifacts.</p>"},{"location":"api_cli_apps/#foodspec.apps.methodsx_reproduction.run_methodsx_reproduction--steps","title":"Steps","text":"<p>1) Oil-type classification on a public dataset (Raman/FTIR). 2) PCA visualization of the same dataset. 3) Mixture analysis on EVOO\u2013sunflower data via regression/NNLS-like fit.</p>"},{"location":"api_cli_apps/#foodspec.apps.methodsx_reproduction.run_methodsx_reproduction--returns","title":"Returns","text":"<p>dict     High-level metrics including accuracy/F1 for classification and     R\u00b2/RMSE for mixture regression. A <code>run_dir</code> key points to the     folder containing artifacts.</p> Source code in <code>src/foodspec/apps/methodsx_reproduction.py</code> <pre><code>def run_methodsx_reproduction(output_dir: PathLike, random_state: int = 42) -&gt; Dict:\n    \"\"\"\n    Run the core analyses used in the MethodsX protocol article and save artifacts.\n\n    Steps\n    -----\n    1) Oil-type classification on a public dataset (Raman/FTIR).\n    2) PCA visualization of the same dataset.\n    3) Mixture analysis on EVOO\u2013sunflower data via regression/NNLS-like fit.\n\n    Returns\n    -------\n    dict\n        High-level metrics including accuracy/F1 for classification and\n        R\u00b2/RMSE for mixture regression. A ``run_dir`` key points to the\n        folder containing artifacts.\n    \"\"\"\n\n    run_dir = create_run_dir(output_dir, \"methodsx\")\n    metrics: Dict[str, float] = {}\n\n    # Oil classification (classic ML only)\n    oil_ds = load_public_mendeley_oils()\n    oil_result = run_oil_authentication_workflow(oil_ds, label_column=\"oil_type\", cv_splits=3)\n    if \"accuracy\" in oil_result.cv_metrics.columns:\n        metrics[\"oil_accuracy\"] = float(oil_result.cv_metrics[\"accuracy\"].mean())\n    if \"f1\" in oil_result.cv_metrics.columns:\n        metrics[\"oil_f1\"] = float(oil_result.cv_metrics[\"f1\"].mean())\n\n    # Confusion matrix plot\n    fig_cm, ax_cm = plt.subplots()\n    plot_confusion_matrix(oil_result.confusion_matrix, class_names=oil_result.class_labels, ax=ax_cm)\n    save_figure(run_dir, \"oil_confusion_matrix\", fig_cm)\n    plt.close(fig_cm)\n\n    # PCA visualization\n    _, pca_res = run_pca(oil_ds.x, n_components=2)\n    fig_pca, ax_pca = plt.subplots()\n    plot_pca_scores(pca_res.scores, labels=oil_ds.metadata.get(\"oil_type\"), ax=ax_pca)\n    ax_pca.set_title(\"PCA scores (public oil dataset)\")\n    save_figure(run_dir, \"oil_pca_scores\", fig_pca)\n    plt.close(fig_pca)\n\n    # Mixture regression (EVOO\u2013sunflower)\n    mix_ds = load_public_evoo_sunflower_raman()\n    y = mix_ds.metadata[\"mixture_fraction_evoo\"].to_numpy()\n    mask = ~np.isnan(y)\n    X = mix_ds.x[mask]\n    y = y[mask]\n    # If values look like percentages, scale to 0-1\n    if np.nanmax(y) &gt; 1:\n        y = y / 100.0\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=random_state, stratify=None\n    )\n    reg = Pipeline([(\"scaler\", StandardScaler()), (\"linreg\", LinearRegression())])\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n    metrics[\"mixture_r2\"] = float(r2_score(y_test, y_pred))\n    metrics[\"mixture_rmse\"] = float(np.sqrt(mean_squared_error(y_test, y_pred)))\n\n    # Save metrics and report\n    metrics[\"run_dir\"] = str(run_dir)\n    write_json(run_dir / \"metrics.json\", metrics)\n    sections = {\n        \"Overview\": (\n            \"This run executes the core MethodsX protocol reproduction: oil-type \"\n            \"classification, PCA visualization, and EVOO\u2013sunflower mixture regression.\"\n        ),\n        \"Key metrics\": summarize_metrics_for_markdown(metrics),\n        \"Artifacts\": (\n            \"- oil_confusion_matrix.png\\n\"\n            \"- oil_pca_scores.png\\n\"\n            \"- metrics.json\"\n        ),\n        \"Assumptions\": (\n            \"Public datasets must be downloaded locally for the loaders to succeed.\"\n        ),\n    }\n    write_markdown_report(run_dir / \"report.md\", title=\"MethodsX Protocol Reproduction\", sections=sections)\n    return metrics\n</code></pre>"},{"location":"api_core/","title":"API reference: core objects","text":"<p>Core objects define the main data structures used by FoodSpec. For workflow examples, see the User Guide and Workflows (e.g., Oil authentication, Hyperspectral analysis).</p>"},{"location":"api_core/#foodspectrumset","title":"FoodSpectrumSet","text":"<p>Collection of spectra with aligned metadata and axis information.</p>"},{"location":"api_core/#foodspec.core.dataset.FoodSpectrumSet--parameters","title":"Parameters","text":"<p>x :     Array of shape (n_samples, n_wavenumbers) containing spectral intensities. wavenumbers :     Array of shape (n_wavenumbers,) with the spectral axis values. metadata :     DataFrame with one row per sample storing labels and acquisition info. modality :     Spectroscopy modality identifier: <code>\"raman\"</code>, <code>\"ftir\"</code>, or <code>\"nir\"</code>.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>@dataclass\nclass FoodSpectrumSet:\n    \"\"\"Collection of spectra with aligned metadata and axis information.\n\n    Parameters\n    ----------\n    x :\n        Array of shape (n_samples, n_wavenumbers) containing spectral intensities.\n    wavenumbers :\n        Array of shape (n_wavenumbers,) with the spectral axis values.\n    metadata :\n        DataFrame with one row per sample storing labels and acquisition info.\n    modality :\n        Spectroscopy modality identifier: ``\"raman\"``, ``\"ftir\"``, or ``\"nir\"``.\n    \"\"\"\n\n    x: np.ndarray\n    wavenumbers: np.ndarray\n    metadata: pd.DataFrame\n    modality: Modality\n\n    def __post_init__(self) -&gt; None:\n        self.validate()\n\n    def __len__(self) -&gt; int:\n        \"\"\"Number of spectra in the set.\"\"\"\n\n        return self.x.shape[0]\n\n    def __getitem__(self, index: IndexType) -&gt; \"FoodSpectrumSet\":\n        \"\"\"Return a subset by position index or slice.\"\"\"\n\n        indices = self._normalize_index(index)\n        return FoodSpectrumSet(\n            x=self.x[indices],\n            wavenumbers=self.wavenumbers.copy(),\n            metadata=self.metadata.iloc[indices].reset_index(drop=True),\n            modality=self.modality,\n        )\n\n    def subset(\n        self,\n        by: Optional[Dict[str, Any]] = None,\n        indices: Optional[Sequence[int]] = None,\n    ) -&gt; \"FoodSpectrumSet\":\n        \"\"\"Subset the dataset by metadata filters and/or explicit indices.\n\n        Parameters\n        ----------\n        by :\n            Mapping of metadata column names to desired values. For sequence-like\n            values, membership (``isin``) is applied; otherwise equality is used.\n        indices :\n            Explicit indices to retain. If provided together with ``by``, the\n            intersection is taken in the order of ``indices``.\n\n        Returns\n        -------\n        FoodSpectrumSet\n            A new dataset containing the selected spectra.\n        \"\"\"\n\n        if by is None and indices is None:\n            return self.copy(deep=False)\n\n        mask = np.ones(len(self), dtype=bool)\n        if by is not None:\n            for key, value in by.items():\n                if key not in self.metadata.columns:\n                    raise ValueError(f\"Metadata column '{key}' not found.\")\n                series = self.metadata[key]\n                if isinstance(value, (list, tuple, set, np.ndarray, pd.Series)):\n                    mask &amp;= series.isin(value).to_numpy()\n                else:\n                    mask &amp;= (series == value).to_numpy()\n\n        if indices is not None:\n            indices_array = np.asarray(indices, dtype=int)\n            if indices_array.ndim != 1:\n                raise ValueError(\"indices must be a 1D sequence of integers.\")\n            if np.any(indices_array &lt; 0) or np.any(indices_array &gt;= len(self)):\n                raise ValueError(\"indices contain out-of-range values.\")\n            if by is not None:\n                indices_array = np.array(\n                    [idx for idx in indices_array if mask[idx]], dtype=int\n                )\n            selected_indices = indices_array\n        else:\n            selected_indices = np.where(mask)[0]\n\n        return FoodSpectrumSet(\n            x=self.x[selected_indices],\n            wavenumbers=self.wavenumbers.copy(),\n            metadata=self.metadata.iloc[selected_indices].reset_index(drop=True),\n            modality=self.modality,\n        )\n\n    def copy(self, deep: bool = True) -&gt; \"FoodSpectrumSet\":\n        \"\"\"Return a copy of the dataset.\n\n        Parameters\n        ----------\n        deep :\n            If True, copy underlying arrays and metadata; otherwise reuse references.\n\n        Returns\n        -------\n        FoodSpectrumSet\n            Copied dataset.\n        \"\"\"\n\n        if deep:\n            x = np.array(self.x, copy=True)\n            wavenumbers = np.array(self.wavenumbers, copy=True)\n            metadata = self.metadata.copy(deep=True)\n        else:\n            x = self.x\n            wavenumbers = self.wavenumbers\n            metadata = self.metadata\n\n        return FoodSpectrumSet(\n            x=x, wavenumbers=wavenumbers, metadata=metadata, modality=self.modality\n        )\n\n    def to_wide_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Convert the dataset to a wide DataFrame.\n\n        Returns\n        -------\n        pandas.DataFrame\n            Metadata columns followed by one column per wavenumber named\n            ``int_&lt;wavenumber&gt;``.\n        \"\"\"\n\n        intensity_columns = [f\"int_{float(wn)}\" for wn in self.wavenumbers]\n        spectra_df = pd.DataFrame(self.x, columns=intensity_columns)\n        return pd.concat(\n            [self.metadata.reset_index(drop=True).copy(), spectra_df], axis=1\n        )\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate array shapes, metadata length, and modality.\"\"\"\n\n        if self.x.ndim != 2:\n            raise ValueError(\"x must be a 2D array of shape (n_samples, n_wavenumbers).\")\n        if self.wavenumbers.ndim != 1:\n            raise ValueError(\"wavenumbers must be a 1D array.\")\n        n_samples, n_wavenumbers = self.x.shape\n        if n_wavenumbers &lt; 3:\n            raise ValueError(\"At least three wavenumber points are required.\")\n        if self.wavenumbers.shape[0] != n_wavenumbers:\n            raise ValueError(\n                \"wavenumbers length does not match number of columns in x \"\n                f\"({self.wavenumbers.shape[0]} != {n_wavenumbers}).\"\n            )\n        if len(self.metadata) != n_samples:\n            raise ValueError(\n                \"metadata length does not match number of rows in x \"\n                f\"({len(self.metadata)} != {n_samples}).\"\n            )\n        if self.modality not in {\"raman\", \"ftir\", \"nir\"}:\n            raise ValueError(\n                \"modality must be one of {'raman', 'ftir', 'nir'}; \"\n                f\"got '{self.modality}'.\"\n            )\n\n    def _normalize_index(self, index: IndexType) -&gt; np.ndarray:\n        \"\"\"Normalize indexing input to an array of indices.\"\"\"\n\n        if isinstance(index, int):\n            if index &lt; 0 or index &gt;= len(self):\n                raise IndexError(\"index out of range.\")\n            return np.array([index])\n        if isinstance(index, slice):\n            return np.arange(len(self))[index]\n        raise TypeError(\"Index must be an integer or slice.\")\n\n    def to_X_y(self, target_col: str) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Return (X, y) for a target column in metadata.\"\"\"\n\n        if target_col not in self.metadata.columns:\n            raise ValueError(f\"Target column '{target_col}' not found in metadata.\")\n        return self.x, self.metadata[target_col].to_numpy()\n\n    def train_test_split(\n        self,\n        target_col: str,\n        test_size: float = 0.3,\n        stratify: bool = True,\n        random_state: Optional[int] = None,\n    ) -&gt; tuple[\"FoodSpectrumSet\", \"FoodSpectrumSet\"]:\n        \"\"\"Split into train/test FoodSpectrumSets.\"\"\"\n\n        X, y = self.to_X_y(target_col)\n        stratify_arg = y if stratify else None\n        X_train, X_test, y_train, y_test, meta_train, meta_test = train_test_split(\n            X,\n            y,\n            self.metadata,\n            test_size=test_size,\n            random_state=random_state,\n            stratify=stratify_arg,\n        )\n        train_ds = FoodSpectrumSet(\n            x=X_train,\n            wavenumbers=self.wavenumbers.copy(),\n            metadata=meta_train.reset_index(drop=True),\n            modality=self.modality,\n        )\n        test_ds = FoodSpectrumSet(\n            x=X_test,\n            wavenumbers=self.wavenumbers.copy(),\n            metadata=meta_test.reset_index(drop=True),\n            modality=self.modality,\n        )\n        return train_ds, test_ds\n</code></pre>"},{"location":"api_core/#foodspec.core.dataset.FoodSpectrumSet.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index)\n</code></pre> <p>Return a subset by position index or slice.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>def __getitem__(self, index: IndexType) -&gt; \"FoodSpectrumSet\":\n    \"\"\"Return a subset by position index or slice.\"\"\"\n\n    indices = self._normalize_index(index)\n    return FoodSpectrumSet(\n        x=self.x[indices],\n        wavenumbers=self.wavenumbers.copy(),\n        metadata=self.metadata.iloc[indices].reset_index(drop=True),\n        modality=self.modality,\n    )\n</code></pre>"},{"location":"api_core/#foodspec.core.dataset.FoodSpectrumSet.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Number of spectra in the set.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Number of spectra in the set.\"\"\"\n\n    return self.x.shape[0]\n</code></pre>"},{"location":"api_core/#foodspec.core.dataset.FoodSpectrumSet.copy","title":"copy","text":"<pre><code>copy(deep=True)\n</code></pre> <p>Return a copy of the dataset.</p>"},{"location":"api_core/#foodspec.core.dataset.FoodSpectrumSet.copy--parameters","title":"Parameters","text":"<p>deep :     If True, copy underlying arrays and metadata; otherwise reuse references.</p>"},{"location":"api_core/#foodspec.core.dataset.FoodSpectrumSet.copy--returns","title":"Returns","text":"<p>FoodSpectrumSet     Copied dataset.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>def copy(self, deep: bool = True) -&gt; \"FoodSpectrumSet\":\n    \"\"\"Return a copy of the dataset.\n\n    Parameters\n    ----------\n    deep :\n        If True, copy underlying arrays and metadata; otherwise reuse references.\n\n    Returns\n    -------\n    FoodSpectrumSet\n        Copied dataset.\n    \"\"\"\n\n    if deep:\n        x = np.array(self.x, copy=True)\n        wavenumbers = np.array(self.wavenumbers, copy=True)\n        metadata = self.metadata.copy(deep=True)\n    else:\n        x = self.x\n        wavenumbers = self.wavenumbers\n        metadata = self.metadata\n\n    return FoodSpectrumSet(\n        x=x, wavenumbers=wavenumbers, metadata=metadata, modality=self.modality\n    )\n</code></pre>"},{"location":"api_core/#foodspec.core.dataset.FoodSpectrumSet.subset","title":"subset","text":"<pre><code>subset(by=None, indices=None)\n</code></pre> <p>Subset the dataset by metadata filters and/or explicit indices.</p>"},{"location":"api_core/#foodspec.core.dataset.FoodSpectrumSet.subset--parameters","title":"Parameters","text":"<p>by :     Mapping of metadata column names to desired values. For sequence-like     values, membership (<code>isin</code>) is applied; otherwise equality is used. indices :     Explicit indices to retain. If provided together with <code>by</code>, the     intersection is taken in the order of <code>indices</code>.</p>"},{"location":"api_core/#foodspec.core.dataset.FoodSpectrumSet.subset--returns","title":"Returns","text":"<p>FoodSpectrumSet     A new dataset containing the selected spectra.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>def subset(\n    self,\n    by: Optional[Dict[str, Any]] = None,\n    indices: Optional[Sequence[int]] = None,\n) -&gt; \"FoodSpectrumSet\":\n    \"\"\"Subset the dataset by metadata filters and/or explicit indices.\n\n    Parameters\n    ----------\n    by :\n        Mapping of metadata column names to desired values. For sequence-like\n        values, membership (``isin``) is applied; otherwise equality is used.\n    indices :\n        Explicit indices to retain. If provided together with ``by``, the\n        intersection is taken in the order of ``indices``.\n\n    Returns\n    -------\n    FoodSpectrumSet\n        A new dataset containing the selected spectra.\n    \"\"\"\n\n    if by is None and indices is None:\n        return self.copy(deep=False)\n\n    mask = np.ones(len(self), dtype=bool)\n    if by is not None:\n        for key, value in by.items():\n            if key not in self.metadata.columns:\n                raise ValueError(f\"Metadata column '{key}' not found.\")\n            series = self.metadata[key]\n            if isinstance(value, (list, tuple, set, np.ndarray, pd.Series)):\n                mask &amp;= series.isin(value).to_numpy()\n            else:\n                mask &amp;= (series == value).to_numpy()\n\n    if indices is not None:\n        indices_array = np.asarray(indices, dtype=int)\n        if indices_array.ndim != 1:\n            raise ValueError(\"indices must be a 1D sequence of integers.\")\n        if np.any(indices_array &lt; 0) or np.any(indices_array &gt;= len(self)):\n            raise ValueError(\"indices contain out-of-range values.\")\n        if by is not None:\n            indices_array = np.array(\n                [idx for idx in indices_array if mask[idx]], dtype=int\n            )\n        selected_indices = indices_array\n    else:\n        selected_indices = np.where(mask)[0]\n\n    return FoodSpectrumSet(\n        x=self.x[selected_indices],\n        wavenumbers=self.wavenumbers.copy(),\n        metadata=self.metadata.iloc[selected_indices].reset_index(drop=True),\n        modality=self.modality,\n    )\n</code></pre>"},{"location":"api_core/#foodspec.core.dataset.FoodSpectrumSet.to_X_y","title":"to_X_y","text":"<pre><code>to_X_y(target_col)\n</code></pre> <p>Return (X, y) for a target column in metadata.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>def to_X_y(self, target_col: str) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return (X, y) for a target column in metadata.\"\"\"\n\n    if target_col not in self.metadata.columns:\n        raise ValueError(f\"Target column '{target_col}' not found in metadata.\")\n    return self.x, self.metadata[target_col].to_numpy()\n</code></pre>"},{"location":"api_core/#foodspec.core.dataset.FoodSpectrumSet.to_wide_dataframe","title":"to_wide_dataframe","text":"<pre><code>to_wide_dataframe()\n</code></pre> <p>Convert the dataset to a wide DataFrame.</p>"},{"location":"api_core/#foodspec.core.dataset.FoodSpectrumSet.to_wide_dataframe--returns","title":"Returns","text":"<p>pandas.DataFrame     Metadata columns followed by one column per wavenumber named     <code>int_&lt;wavenumber&gt;</code>.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>def to_wide_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Convert the dataset to a wide DataFrame.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Metadata columns followed by one column per wavenumber named\n        ``int_&lt;wavenumber&gt;``.\n    \"\"\"\n\n    intensity_columns = [f\"int_{float(wn)}\" for wn in self.wavenumbers]\n    spectra_df = pd.DataFrame(self.x, columns=intensity_columns)\n    return pd.concat(\n        [self.metadata.reset_index(drop=True).copy(), spectra_df], axis=1\n    )\n</code></pre>"},{"location":"api_core/#foodspec.core.dataset.FoodSpectrumSet.train_test_split","title":"train_test_split","text":"<pre><code>train_test_split(\n    target_col,\n    test_size=0.3,\n    stratify=True,\n    random_state=None,\n)\n</code></pre> <p>Split into train/test FoodSpectrumSets.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>def train_test_split(\n    self,\n    target_col: str,\n    test_size: float = 0.3,\n    stratify: bool = True,\n    random_state: Optional[int] = None,\n) -&gt; tuple[\"FoodSpectrumSet\", \"FoodSpectrumSet\"]:\n    \"\"\"Split into train/test FoodSpectrumSets.\"\"\"\n\n    X, y = self.to_X_y(target_col)\n    stratify_arg = y if stratify else None\n    X_train, X_test, y_train, y_test, meta_train, meta_test = train_test_split(\n        X,\n        y,\n        self.metadata,\n        test_size=test_size,\n        random_state=random_state,\n        stratify=stratify_arg,\n    )\n    train_ds = FoodSpectrumSet(\n        x=X_train,\n        wavenumbers=self.wavenumbers.copy(),\n        metadata=meta_train.reset_index(drop=True),\n        modality=self.modality,\n    )\n    test_ds = FoodSpectrumSet(\n        x=X_test,\n        wavenumbers=self.wavenumbers.copy(),\n        metadata=meta_test.reset_index(drop=True),\n        modality=self.modality,\n    )\n    return train_ds, test_ds\n</code></pre>"},{"location":"api_core/#foodspec.core.dataset.FoodSpectrumSet.validate","title":"validate","text":"<pre><code>validate()\n</code></pre> <p>Validate array shapes, metadata length, and modality.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate array shapes, metadata length, and modality.\"\"\"\n\n    if self.x.ndim != 2:\n        raise ValueError(\"x must be a 2D array of shape (n_samples, n_wavenumbers).\")\n    if self.wavenumbers.ndim != 1:\n        raise ValueError(\"wavenumbers must be a 1D array.\")\n    n_samples, n_wavenumbers = self.x.shape\n    if n_wavenumbers &lt; 3:\n        raise ValueError(\"At least three wavenumber points are required.\")\n    if self.wavenumbers.shape[0] != n_wavenumbers:\n        raise ValueError(\n            \"wavenumbers length does not match number of columns in x \"\n            f\"({self.wavenumbers.shape[0]} != {n_wavenumbers}).\"\n        )\n    if len(self.metadata) != n_samples:\n        raise ValueError(\n            \"metadata length does not match number of rows in x \"\n            f\"({len(self.metadata)} != {n_samples}).\"\n        )\n    if self.modality not in {\"raman\", \"ftir\", \"nir\"}:\n        raise ValueError(\n            \"modality must be one of {'raman', 'ftir', 'nir'}; \"\n            f\"got '{self.modality}'.\"\n        )\n</code></pre>"},{"location":"api_core/#hyperspectralcube","title":"HyperSpectralCube","text":"<p>Container for hyperspectral maps stored as (height, width, n_points).</p> Source code in <code>src/foodspec/core/hyperspectral.py</code> <pre><code>@dataclass\nclass HyperSpectralCube:\n    \"\"\"Container for hyperspectral maps stored as (height, width, n_points).\"\"\"\n\n    cube: np.ndarray  # shape (height, width, n_points)\n    wavenumbers: np.ndarray  # shape (n_points,)\n    metadata: pd.DataFrame\n    image_shape: Tuple[int, int]\n\n    def __post_init__(self) -&gt; None:\n        self.cube = np.asarray(self.cube, dtype=float)\n        self.wavenumbers = np.asarray(self.wavenumbers, dtype=float)\n        if self.cube.ndim != 3:\n            raise ValueError(\"cube must be 3D (height, width, n_points).\")\n        h, w, n_points = self.cube.shape\n        if self.wavenumbers.shape[0] != n_points:\n            raise ValueError(\"wavenumbers length must match cube spectral dimension.\")\n        if (h, w) != self.image_shape:\n            raise ValueError(\"image_shape does not match cube spatial dimensions.\")\n        if not isinstance(self.metadata, pd.DataFrame):\n            raise ValueError(\"metadata must be a pandas DataFrame.\")\n\n    @classmethod\n    def from_spectrum_set(cls, spectra: FoodSpectrumSet, image_shape: Tuple[int, int]) -&gt; \"HyperSpectralCube\":\n        \"\"\"Create cube from flattened spectra using image_shape (h, w).\"\"\"\n        h, w = image_shape\n        n_pixels = h * w\n        if len(spectra) != n_pixels:\n            raise ValueError(\"Number of spectra does not match image_shape pixels.\")\n        cube = spectra.x.reshape(h, w, -1)\n        return cls(\n            cube=cube,\n            wavenumbers=spectra.wavenumbers,\n            metadata=spectra.metadata.copy(),\n            image_shape=image_shape,\n        )\n\n    def to_spectrum_set(self, modality: str = \"raman\") -&gt; FoodSpectrumSet:\n        \"\"\"Flatten cube to FoodSpectrumSet, adding row/col coordinates.\"\"\"\n        h, w, n_points = self.cube.shape\n        flat = self.cube.reshape(h * w, n_points)\n        coords = pd.DataFrame({\"row\": np.repeat(np.arange(h), w), \"col\": np.tile(np.arange(w), h)})\n        meta = self.metadata.copy().reset_index(drop=True)\n        meta = pd.concat([coords, meta], axis=1)\n        return FoodSpectrumSet(x=flat, wavenumbers=self.wavenumbers, metadata=meta, modality=modality)\n\n    def to_pixel_spectra(self, modality: str = \"raman\") -&gt; FoodSpectrumSet:\n        \"\"\"Flatten to pixel spectra with row/col metadata.\"\"\"\n        return self.to_spectrum_set(modality=modality)\n\n    def from_pixel_labels(self, labels: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Reshape flat labels (n_pixels,) to (height, width) label image.\"\"\"\n        labels = np.asarray(labels)\n        h, w = self.image_shape\n        if labels.shape[0] != h * w:\n            raise ValueError(\"labels length must match number of pixels (height*width).\")\n        return labels.reshape(h, w)\n\n    def get_pixel_spectrum(self, row: int, col: int) -&gt; np.ndarray:\n        \"\"\"Return spectrum at a given pixel coordinate.\"\"\"\n        if row &lt; 0 or row &gt;= self.image_shape[0] or col &lt; 0 or col &gt;= self.image_shape[1]:\n            raise IndexError(\"Pixel indices out of range.\")\n        return self.cube[row, col, :]\n\n    def mean_spectrum(self) -&gt; np.ndarray:\n        \"\"\"Return mean spectrum over all pixels.\"\"\"\n        return self.cube.reshape(-1, self.cube.shape[-1]).mean(axis=0)\n</code></pre> <p>Status: Archived This page has been superseded by the unified API reference. See api_reference.md for current core data structure APIs.</p>"},{"location":"api_core/#foodspec.core.hyperspectral.HyperSpectralCube.from_pixel_labels","title":"from_pixel_labels","text":"<pre><code>from_pixel_labels(labels)\n</code></pre> <p>Reshape flat labels (n_pixels,) to (height, width) label image.</p> Source code in <code>src/foodspec/core/hyperspectral.py</code> <pre><code>def from_pixel_labels(self, labels: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Reshape flat labels (n_pixels,) to (height, width) label image.\"\"\"\n    labels = np.asarray(labels)\n    h, w = self.image_shape\n    if labels.shape[0] != h * w:\n        raise ValueError(\"labels length must match number of pixels (height*width).\")\n    return labels.reshape(h, w)\n</code></pre>"},{"location":"api_core/#foodspec.core.hyperspectral.HyperSpectralCube.from_spectrum_set","title":"from_spectrum_set  <code>classmethod</code>","text":"<pre><code>from_spectrum_set(spectra, image_shape)\n</code></pre> <p>Create cube from flattened spectra using image_shape (h, w).</p> Source code in <code>src/foodspec/core/hyperspectral.py</code> <pre><code>@classmethod\ndef from_spectrum_set(cls, spectra: FoodSpectrumSet, image_shape: Tuple[int, int]) -&gt; \"HyperSpectralCube\":\n    \"\"\"Create cube from flattened spectra using image_shape (h, w).\"\"\"\n    h, w = image_shape\n    n_pixels = h * w\n    if len(spectra) != n_pixels:\n        raise ValueError(\"Number of spectra does not match image_shape pixels.\")\n    cube = spectra.x.reshape(h, w, -1)\n    return cls(\n        cube=cube,\n        wavenumbers=spectra.wavenumbers,\n        metadata=spectra.metadata.copy(),\n        image_shape=image_shape,\n    )\n</code></pre>"},{"location":"api_core/#foodspec.core.hyperspectral.HyperSpectralCube.get_pixel_spectrum","title":"get_pixel_spectrum","text":"<pre><code>get_pixel_spectrum(row, col)\n</code></pre> <p>Return spectrum at a given pixel coordinate.</p> Source code in <code>src/foodspec/core/hyperspectral.py</code> <pre><code>def get_pixel_spectrum(self, row: int, col: int) -&gt; np.ndarray:\n    \"\"\"Return spectrum at a given pixel coordinate.\"\"\"\n    if row &lt; 0 or row &gt;= self.image_shape[0] or col &lt; 0 or col &gt;= self.image_shape[1]:\n        raise IndexError(\"Pixel indices out of range.\")\n    return self.cube[row, col, :]\n</code></pre>"},{"location":"api_core/#foodspec.core.hyperspectral.HyperSpectralCube.mean_spectrum","title":"mean_spectrum","text":"<pre><code>mean_spectrum()\n</code></pre> <p>Return mean spectrum over all pixels.</p> Source code in <code>src/foodspec/core/hyperspectral.py</code> <pre><code>def mean_spectrum(self) -&gt; np.ndarray:\n    \"\"\"Return mean spectrum over all pixels.\"\"\"\n    return self.cube.reshape(-1, self.cube.shape[-1]).mean(axis=0)\n</code></pre>"},{"location":"api_core/#foodspec.core.hyperspectral.HyperSpectralCube.to_pixel_spectra","title":"to_pixel_spectra","text":"<pre><code>to_pixel_spectra(modality='raman')\n</code></pre> <p>Flatten to pixel spectra with row/col metadata.</p> Source code in <code>src/foodspec/core/hyperspectral.py</code> <pre><code>def to_pixel_spectra(self, modality: str = \"raman\") -&gt; FoodSpectrumSet:\n    \"\"\"Flatten to pixel spectra with row/col metadata.\"\"\"\n    return self.to_spectrum_set(modality=modality)\n</code></pre>"},{"location":"api_core/#foodspec.core.hyperspectral.HyperSpectralCube.to_spectrum_set","title":"to_spectrum_set","text":"<pre><code>to_spectrum_set(modality='raman')\n</code></pre> <p>Flatten cube to FoodSpectrumSet, adding row/col coordinates.</p> Source code in <code>src/foodspec/core/hyperspectral.py</code> <pre><code>def to_spectrum_set(self, modality: str = \"raman\") -&gt; FoodSpectrumSet:\n    \"\"\"Flatten cube to FoodSpectrumSet, adding row/col coordinates.\"\"\"\n    h, w, n_points = self.cube.shape\n    flat = self.cube.reshape(h * w, n_points)\n    coords = pd.DataFrame({\"row\": np.repeat(np.arange(h), w), \"col\": np.tile(np.arange(w), h)})\n    meta = self.metadata.copy().reset_index(drop=True)\n    meta = pd.concat([coords, meta], axis=1)\n    return FoodSpectrumSet(x=flat, wavenumbers=self.wavenumbers, metadata=meta, modality=modality)\n</code></pre>"},{"location":"api_features_chemometrics/","title":"API reference: features &amp; chemometrics","text":"<p>Feature extractors and chemometrics models power FoodSpec workflows. For examples, see the Oil authentication and Mixture tutorials.</p>"},{"location":"api_features_chemometrics/#features","title":"Features","text":"<p>Peak detection and feature extraction utilities.</p> <p>Band integration utilities.</p> <p>Ratio feature utilities.</p> <p>Spectral similarity utilities.</p>"},{"location":"api_features_chemometrics/#foodspec.features.peaks.PeakFeatureExtractor","title":"PeakFeatureExtractor","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Extract peak height and area features around expected peaks.</p> Source code in <code>src/foodspec/features/peaks.py</code> <pre><code>class PeakFeatureExtractor(BaseEstimator, TransformerMixin):\n    \"\"\"Extract peak height and area features around expected peaks.\"\"\"\n\n    def __init__(\n        self,\n        expected_peaks: Sequence[float],\n        tolerance: float = 5.0,\n        features: Sequence[str] = (\"height\", \"area\"),\n    ):\n        self.expected_peaks = list(expected_peaks)\n        self.tolerance = tolerance\n        self.features = tuple(features)\n        self.feature_names_: list[str] = []\n\n    def fit(\n        self, X: np.ndarray, y: Optional[np.ndarray] = None, wavenumbers: Optional[np.ndarray] = None\n    ) -&gt; \"PeakFeatureExtractor\":\n        self._build_feature_names()\n        return self\n\n    def transform(\n        self, X: np.ndarray, wavenumbers: Optional[np.ndarray] = None\n    ) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D with shape (n_samples, n_wavenumbers).\")\n        if wavenumbers is None:\n            raise ValueError(\"wavenumbers is required to extract peak features.\")\n        wavenumbers = np.asarray(wavenumbers, dtype=float)\n        if wavenumbers.shape[0] != X.shape[1]:\n            raise ValueError(\"wavenumbers length must match X columns.\")\n        if self.tolerance &lt;= 0:\n            raise ValueError(\"tolerance must be positive.\")\n\n        self._build_feature_names()\n        feats = np.zeros((X.shape[0], len(self.feature_names_)), dtype=float)\n        for i, spectrum in enumerate(X):\n            col = 0\n            for peak_center in self.expected_peaks:\n                mask = (wavenumbers &gt;= peak_center - self.tolerance) &amp; (\n                    wavenumbers &lt;= peak_center + self.tolerance\n                )\n                if not np.any(mask):\n                    peak_idx = np.argmax(np.full_like(wavenumbers, -np.inf))\n                    area = np.nan\n                    height = np.nan\n                else:\n                    local_w = wavenumbers[mask]\n                    local_y = spectrum[mask]\n                    local_max_idx = np.argmax(local_y)\n                    height = local_y[local_max_idx]\n                    peak_idx = np.where(mask)[0][local_max_idx]\n                    area = np.trapezoid(local_y, x=local_w)\n\n                if \"height\" in self.features:\n                    feats[i, col] = height\n                    col += 1\n                if \"area\" in self.features:\n                    feats[i, col] = area\n                    col += 1\n\n        return feats\n\n    def get_feature_names_out(self, input_features=None):\n        self._build_feature_names()\n        return np.array(self.feature_names_, dtype=str)\n\n    def _build_feature_names(self) -&gt; None:\n        names: list[str] = []\n        for peak in self.expected_peaks:\n            if \"height\" in self.features:\n                names.append(f\"peak_{peak}_height\")\n            if \"area\" in self.features:\n                names.append(f\"peak_{peak}_area\")\n        self.feature_names_ = names\n</code></pre>"},{"location":"api_features_chemometrics/#foodspec.features.peaks.detect_peaks","title":"detect_peaks","text":"<pre><code>detect_peaks(x, wavenumbers, prominence=0.0, width=None)\n</code></pre> <p>Detect peaks and return their properties.</p>"},{"location":"api_features_chemometrics/#foodspec.features.peaks.detect_peaks--parameters","title":"Parameters","text":"<p>x :     1D intensity array. wavenumbers :     1D axis array aligned with <code>x</code>. prominence :     Minimum prominence passed to <code>scipy.signal.find_peaks</code>. width :     Optional width parameter for <code>find_peaks</code>.</p>"},{"location":"api_features_chemometrics/#foodspec.features.peaks.detect_peaks--returns","title":"Returns","text":"<p>pandas.DataFrame     Columns: <code>peak_index</code>, <code>peak_wavenumber</code>, <code>peak_intensity</code>,     <code>prominence</code>, <code>width</code>.</p> Source code in <code>src/foodspec/features/peaks.py</code> <pre><code>def detect_peaks(\n    x: np.ndarray,\n    wavenumbers: np.ndarray,\n    prominence: float = 0.0,\n    width: Optional[float] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Detect peaks and return their properties.\n\n    Parameters\n    ----------\n    x :\n        1D intensity array.\n    wavenumbers :\n        1D axis array aligned with ``x``.\n    prominence :\n        Minimum prominence passed to ``scipy.signal.find_peaks``.\n    width :\n        Optional width parameter for ``find_peaks``.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Columns: ``peak_index``, ``peak_wavenumber``, ``peak_intensity``,\n        ``prominence``, ``width``.\n    \"\"\"\n\n    x = np.asarray(x, dtype=float)\n    wavenumbers = np.asarray(wavenumbers, dtype=float)\n    if x.ndim != 1 or wavenumbers.ndim != 1:\n        raise ValueError(\"x and wavenumbers must be 1D.\")\n    if x.shape[0] != wavenumbers.shape[0]:\n        raise ValueError(\"x and wavenumbers must have the same length.\")\n\n    peak_indices, props = find_peaks(x, prominence=prominence, width=width)\n    prominences = props.get(\"prominences\", np.full_like(peak_indices, np.nan, dtype=float))\n    widths = props.get(\"widths\", np.full_like(peak_indices, np.nan, dtype=float))\n    return pd.DataFrame(\n        {\n            \"peak_index\": peak_indices,\n            \"peak_wavenumber\": wavenumbers[peak_indices],\n            \"peak_intensity\": x[peak_indices],\n            \"prominence\": prominences,\n            \"width\": widths,\n        }\n    )\n</code></pre>"},{"location":"api_features_chemometrics/#foodspec.features.bands.integrate_bands","title":"integrate_bands","text":"<pre><code>integrate_bands(X, wavenumbers, bands)\n</code></pre> <p>Integrate intensity over specified bands.</p>"},{"location":"api_features_chemometrics/#foodspec.features.bands.integrate_bands--parameters","title":"Parameters","text":"<p>X :     Array of shape (n_samples, n_wavenumbers). wavenumbers :     1D array of wavenumbers aligned with <code>X</code>. bands :     Sequence of (label, min_wn, max_wn) tuples.</p>"},{"location":"api_features_chemometrics/#foodspec.features.bands.integrate_bands--returns","title":"Returns","text":"<p>pandas.DataFrame     Columns named by band labels; one row per sample.</p> Source code in <code>src/foodspec/features/bands.py</code> <pre><code>def integrate_bands(\n    X: np.ndarray,\n    wavenumbers: np.ndarray,\n    bands: Sequence[Tuple[str, float, float]],\n) -&gt; pd.DataFrame:\n    \"\"\"Integrate intensity over specified bands.\n\n    Parameters\n    ----------\n    X :\n        Array of shape (n_samples, n_wavenumbers).\n    wavenumbers :\n        1D array of wavenumbers aligned with ``X``.\n    bands :\n        Sequence of (label, min_wn, max_wn) tuples.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Columns named by band labels; one row per sample.\n    \"\"\"\n\n    X = np.asarray(X, dtype=float)\n    wavenumbers = np.asarray(wavenumbers, dtype=float)\n    if X.ndim != 2:\n        raise ValueError(\"X must be 2D.\")\n    if wavenumbers.ndim != 1 or wavenumbers.shape[0] != X.shape[1]:\n        raise ValueError(\"wavenumbers must be 1D and match number of columns in X.\")\n\n    data = {}\n    for label, min_wn, max_wn in bands:\n        if min_wn &gt;= max_wn:\n            raise ValueError(f\"Band {label} has invalid range.\")\n        mask = (wavenumbers &gt;= min_wn) &amp; (wavenumbers &lt;= max_wn)\n        if not np.any(mask):\n            data[label] = np.full(X.shape[0], np.nan)\n            continue\n        data[label] = np.trapezoid(X[:, mask], x=wavenumbers[mask], axis=1)\n\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"api_features_chemometrics/#foodspec.features.ratios.RatioFeatureGenerator","title":"RatioFeatureGenerator","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Generate ratio features for use in pipelines.</p> Source code in <code>src/foodspec/features/ratios.py</code> <pre><code>class RatioFeatureGenerator(BaseEstimator, TransformerMixin):\n    \"\"\"Generate ratio features for use in pipelines.\"\"\"\n\n    def __init__(self, ratio_def: Dict[str, Tuple[str, str]]):\n        self.ratio_def = ratio_def\n\n    def fit(self, X: pd.DataFrame, y: Optional[np.ndarray] = None) -&gt; \"RatioFeatureGenerator\":\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"RatioFeatureGenerator expects a pandas DataFrame.\")\n        return compute_ratios(X, self.ratio_def)\n</code></pre>"},{"location":"api_features_chemometrics/#foodspec.features.ratios.compute_ratios","title":"compute_ratios","text":"<pre><code>compute_ratios(df, ratio_def)\n</code></pre> <p>Compute ratios of specified columns.</p>"},{"location":"api_features_chemometrics/#foodspec.features.ratios.compute_ratios--parameters","title":"Parameters","text":"<p>df :     DataFrame containing numerator and denominator columns. ratio_def :     Mapping from new column name to (numerator_col, denominator_col).</p>"},{"location":"api_features_chemometrics/#foodspec.features.ratios.compute_ratios--returns","title":"Returns","text":"<p>pandas.DataFrame     Original DataFrame with additional ratio columns.</p> Source code in <code>src/foodspec/features/ratios.py</code> <pre><code>def compute_ratios(\n    df: pd.DataFrame, ratio_def: Dict[str, Tuple[str, str]]\n) -&gt; pd.DataFrame:\n    \"\"\"Compute ratios of specified columns.\n\n    Parameters\n    ----------\n    df :\n        DataFrame containing numerator and denominator columns.\n    ratio_def :\n        Mapping from new column name to (numerator_col, denominator_col).\n\n    Returns\n    -------\n    pandas.DataFrame\n        Original DataFrame with additional ratio columns.\n    \"\"\"\n\n    result = df.copy()\n    for name, (num_col, denom_col) in ratio_def.items():\n        if num_col not in result.columns or denom_col not in result.columns:\n            raise ValueError(f\"Columns {num_col} and {denom_col} must exist in DataFrame.\")\n        denom = result[denom_col].to_numpy()\n        num = result[num_col].to_numpy()\n        ratio = np.divide(num, denom, out=np.full_like(num, np.nan, dtype=float), where=denom != 0)\n        result[name] = ratio\n    return result\n</code></pre>"},{"location":"api_features_chemometrics/#foodspec.features.fingerprint.correlation_similarity_matrix","title":"correlation_similarity_matrix","text":"<pre><code>correlation_similarity_matrix(X_ref, X_query)\n</code></pre> <p>Compute Pearson correlation similarity matrix.</p> Source code in <code>src/foodspec/features/fingerprint.py</code> <pre><code>def correlation_similarity_matrix(X_ref: np.ndarray, X_query: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute Pearson correlation similarity matrix.\"\"\"\n\n    X_ref = np.asarray(X_ref, dtype=float)\n    X_query = np.asarray(X_query, dtype=float)\n    X_ref_centered = X_ref - X_ref.mean(axis=1, keepdims=True)\n    X_query_centered = X_query - X_query.mean(axis=1, keepdims=True)\n    ref_norm = np.linalg.norm(X_ref_centered, axis=1, keepdims=True)\n    query_norm = np.linalg.norm(X_query_centered, axis=1, keepdims=True)\n    ref_norm = np.maximum(ref_norm, np.finfo(float).eps)\n    query_norm = np.maximum(query_norm, np.finfo(float).eps)\n    sims = (X_ref_centered @ X_query_centered.T) / (ref_norm * query_norm.T)\n    return sims\n</code></pre>"},{"location":"api_features_chemometrics/#foodspec.features.fingerprint.cosine_similarity_matrix","title":"cosine_similarity_matrix","text":"<pre><code>cosine_similarity_matrix(X_ref, X_query)\n</code></pre> <p>Compute cosine similarity matrix between reference and query spectra.</p> Source code in <code>src/foodspec/features/fingerprint.py</code> <pre><code>def cosine_similarity_matrix(X_ref: np.ndarray, X_query: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute cosine similarity matrix between reference and query spectra.\"\"\"\n\n    X_ref = np.asarray(X_ref, dtype=float)\n    X_query = np.asarray(X_query, dtype=float)\n    ref_norm = np.linalg.norm(X_ref, axis=1, keepdims=True)\n    query_norm = np.linalg.norm(X_query, axis=1, keepdims=True)\n    ref_norm = np.maximum(ref_norm, np.finfo(float).eps)\n    query_norm = np.maximum(query_norm, np.finfo(float).eps)\n    sims = (X_ref @ X_query.T) / (ref_norm * query_norm.T)\n    return sims\n</code></pre>"},{"location":"api_features_chemometrics/#chemometrics","title":"Chemometrics","text":"<p>Principal Component Analysis utilities.</p> <p>Model factories for chemometrics workflows.</p> <p>Validation utilities for chemometrics models.</p> <p>Mixture analysis utilities (NNLS and simplified MCR-ALS).</p> <p>Optional deep learning models for spectral classification.</p> <p>Provides a minimal 1D CNN classifier with a scikit-learn-like API. This module relies on TensorFlow/Keras or PyTorch; it is only intended for advanced users and examples. Dependencies are optional and must be installed separately.</p> <p>Status: Archived This page has been superseded by the unified API reference. See api_reference.md and api_reference.md for current feature and chemometrics APIs.</p>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.pca.PCAResult","title":"PCAResult  <code>dataclass</code>","text":"<p>Container for PCA outputs.</p> Source code in <code>src/foodspec/chemometrics/pca.py</code> <pre><code>@dataclass\nclass PCAResult:\n    \"\"\"Container for PCA outputs.\"\"\"\n\n    scores: np.ndarray\n    loadings: np.ndarray\n    explained_variance: np.ndarray\n    explained_variance_ratio: np.ndarray\n    mean_: np.ndarray\n</code></pre>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.pca.run_pca","title":"run_pca","text":"<pre><code>run_pca(X, n_components=2)\n</code></pre> <p>Run PCA on data matrix.</p>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.pca.run_pca--parameters","title":"Parameters","text":"<p>X :     Array of shape (n_samples, n_features). n_components :     Number of components to compute.</p>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.pca.run_pca--returns","title":"Returns","text":"<p>tuple     Fitted PCA estimator and PCAResult container.</p> Source code in <code>src/foodspec/chemometrics/pca.py</code> <pre><code>def run_pca(X: np.ndarray, n_components: int = 2) -&gt; Tuple[PCA, PCAResult]:\n    \"\"\"Run PCA on data matrix.\n\n    Parameters\n    ----------\n    X :\n        Array of shape (n_samples, n_features).\n    n_components :\n        Number of components to compute.\n\n    Returns\n    -------\n    tuple\n        Fitted PCA estimator and PCAResult container.\n    \"\"\"\n\n    if n_components &lt;= 0:\n        raise ValueError(\"n_components must be positive.\")\n\n    pca = PCA(n_components=n_components)\n    scores = pca.fit_transform(X)\n    result = PCAResult(\n        scores=scores,\n        loadings=pca.components_.T,\n        explained_variance=pca.explained_variance_,\n        explained_variance_ratio=pca.explained_variance_ratio_,\n        mean_=pca.mean_,\n    )\n    return pca, result\n</code></pre>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.models.make_classifier","title":"make_classifier","text":"<pre><code>make_classifier(model_name, **kwargs)\n</code></pre> <p>Factory for common classifiers.</p>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.models.make_classifier--parameters","title":"Parameters","text":"<p>model_name :     One of: <code>logreg</code>, <code>svm_linear</code>, <code>svm_rbf</code>, <code>rf</code>, <code>xgb</code>, <code>lgbm</code>, <code>knn</code>. kwargs :     Additional parameters forwarded to the model constructor.</p>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.models.make_classifier--returns","title":"Returns","text":"<p>BaseEstimator     Instantiated classifier.</p> Source code in <code>src/foodspec/chemometrics/models.py</code> <pre><code>def make_classifier(model_name: str, **kwargs: Any) -&gt; BaseEstimator:\n    \"\"\"Factory for common classifiers.\n\n    Parameters\n    ----------\n    model_name :\n        One of: ``logreg``, ``svm_linear``, ``svm_rbf``, ``rf``, ``xgb``, ``lgbm``, ``knn``.\n    kwargs :\n        Additional parameters forwarded to the model constructor.\n\n    Returns\n    -------\n    BaseEstimator\n        Instantiated classifier.\n    \"\"\"\n\n    name = model_name.lower()\n    if name == \"logreg\":\n        return LogisticRegression(max_iter=1000, **kwargs)\n    if name == \"svm_linear\":\n        return SVC(kernel=\"linear\", probability=True, **kwargs)\n    if name == \"svm_rbf\":\n        return SVC(kernel=\"rbf\", probability=True, **kwargs)\n    if name == \"rf\":\n        return RandomForestClassifier(**kwargs)\n    if name == \"knn\":\n        return KNeighborsClassifier(**kwargs)\n    if name == \"xgb\":\n        try:\n            from xgboost import XGBClassifier  # type: ignore\n        except ModuleNotFoundError as exc:\n            raise ImportError(\"xgboost is required for model_name='xgb'.\") from exc\n        return XGBClassifier(**kwargs)\n    if name == \"lgbm\":\n        try:\n            from lightgbm import LGBMClassifier  # type: ignore\n        except ModuleNotFoundError as exc:\n            raise ImportError(\"lightgbm is required for model_name='lgbm'.\") from exc\n        return LGBMClassifier(**kwargs)\n\n    raise ValueError(\n        \"model_name must be one of {'logreg','svm_linear','svm_rbf','rf','xgb','lgbm','knn'}\"\n    )\n</code></pre>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.models.make_pls_da","title":"make_pls_da","text":"<pre><code>make_pls_da(n_components=10)\n</code></pre> <p>Create a PLS-DA (PLS + Logistic Regression) pipeline.</p> Source code in <code>src/foodspec/chemometrics/models.py</code> <pre><code>def make_pls_da(n_components: int = 10) -&gt; Pipeline:\n    \"\"\"Create a PLS-DA (PLS + Logistic Regression) pipeline.\"\"\"\n\n    return Pipeline(\n        [\n            (\"scaler\", StandardScaler()),\n            (\"pls_proj\", _PLSProjector(n_components=n_components)),\n            (\"clf\", LogisticRegression(max_iter=1000)),\n        ]\n    )\n</code></pre>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.models.make_pls_regression","title":"make_pls_regression","text":"<pre><code>make_pls_regression(n_components=10)\n</code></pre> <p>Create a PLS regression pipeline with scaling.</p> Source code in <code>src/foodspec/chemometrics/models.py</code> <pre><code>def make_pls_regression(n_components: int = 10) -&gt; Pipeline:\n    \"\"\"Create a PLS regression pipeline with scaling.\"\"\"\n\n    return Pipeline(\n        [\n            (\"scaler\", StandardScaler()),\n            (\"pls\", PLSRegression(n_components=n_components)),\n        ]\n    )\n</code></pre>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.validation.compute_classification_metrics","title":"compute_classification_metrics","text":"<pre><code>compute_classification_metrics(\n    y_true, y_pred, y_proba=None\n)\n</code></pre> <p>Compute common classification metrics.</p> Source code in <code>src/foodspec/chemometrics/validation.py</code> <pre><code>def compute_classification_metrics(\n    y_true: np.ndarray, y_pred: np.ndarray, y_proba: Optional[np.ndarray] = None\n) -&gt; pd.DataFrame:\n    \"\"\"Compute common classification metrics.\"\"\"\n\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    labels = np.unique(np.concatenate([y_true, y_pred]))\n    average = \"binary\" if len(labels) == 2 else \"weighted\"\n    pos_label = labels[0] if len(labels) == 2 else None\n\n    results: dict[str, Any] = {\n        \"accuracy\": metrics.accuracy_score(y_true, y_pred),\n        \"precision\": metrics.precision_score(\n            y_true, y_pred, zero_division=0, average=average, pos_label=pos_label\n        ),\n        \"recall\": metrics.recall_score(\n            y_true, y_pred, zero_division=0, average=average, pos_label=pos_label\n        ),\n        \"f1\": metrics.f1_score(\n            y_true, y_pred, zero_division=0, average=average, pos_label=pos_label\n        ),\n    }\n    if y_proba is not None and len(labels) == 2:\n        y_proba = np.asarray(y_proba)\n        if y_proba.ndim == 2 and y_proba.shape[1] &gt; 1:\n            pos_scores = y_proba[:, 1]\n        else:\n            pos_scores = y_proba\n        results[\"roc_auc\"] = metrics.roc_auc_score(y_true, pos_scores)\n        results[\"average_precision\"] = metrics.average_precision_score(y_true, pos_scores)\n\n    return pd.DataFrame([results])\n</code></pre>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.validation.compute_regression_metrics","title":"compute_regression_metrics","text":"<pre><code>compute_regression_metrics(y_true, y_pred)\n</code></pre> <p>Compute regression metrics.</p> Source code in <code>src/foodspec/chemometrics/validation.py</code> <pre><code>def compute_regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -&gt; pd.Series:\n    \"\"\"Compute regression metrics.\"\"\"\n\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    rmse = np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n    mae = metrics.mean_absolute_error(y_true, y_pred)\n    r2 = metrics.r2_score(y_true, y_pred)\n    return pd.Series({\"rmse\": rmse, \"mae\": mae, \"r2\": r2})\n</code></pre>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.validation.cross_validate_pipeline","title":"cross_validate_pipeline","text":"<pre><code>cross_validate_pipeline(\n    pipeline, X, y, cv_splits=5, scoring=\"accuracy\"\n)\n</code></pre> <p>Cross-validate a pipeline and return fold scores plus summary.</p> Source code in <code>src/foodspec/chemometrics/validation.py</code> <pre><code>def cross_validate_pipeline(\n    pipeline,\n    X: np.ndarray,\n    y: np.ndarray,\n    cv_splits: int = 5,\n    scoring: str = \"accuracy\",\n) -&gt; pd.DataFrame:\n    \"\"\"Cross-validate a pipeline and return fold scores plus summary.\"\"\"\n\n    cv_results = cross_validate(\n        pipeline,\n        X,\n        y,\n        cv=cv_splits,\n        scoring=scoring,\n        return_train_score=False,\n    )\n    scores = cv_results[\"test_score\"]\n    rows = [{\"fold\": i + 1, \"score\": s} for i, s in enumerate(scores)]\n    rows.append({\"fold\": \"mean\", \"score\": np.mean(scores)})\n    rows.append({\"fold\": \"std\", \"score\": np.std(scores)})\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.validation.permutation_test_score_wrapper","title":"permutation_test_score_wrapper","text":"<pre><code>permutation_test_score_wrapper(\n    estimator,\n    X,\n    y,\n    scoring=\"accuracy\",\n    n_permutations=100,\n    random_state=None,\n)\n</code></pre> <p>Wrapper around sklearn's permutation_test_score.</p> Source code in <code>src/foodspec/chemometrics/validation.py</code> <pre><code>def permutation_test_score_wrapper(\n    estimator,\n    X: np.ndarray,\n    y: np.ndarray,\n    scoring: str = \"accuracy\",\n    n_permutations: int = 100,\n    random_state: Optional[int] = None,\n):\n    \"\"\"Wrapper around sklearn's permutation_test_score.\"\"\"\n\n    score, perm_scores, pvalue = permutation_test_score(\n        estimator,\n        X,\n        y,\n        scoring=scoring,\n        n_permutations=n_permutations,\n        random_state=random_state,\n    )\n    return score, perm_scores, pvalue\n</code></pre>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.mixture.mcr_als","title":"mcr_als","text":"<pre><code>mcr_als(\n    X,\n    n_components,\n    max_iter=100,\n    tol=1e-06,\n    random_state=None,\n)\n</code></pre> <p>Perform a simplified MCR-ALS decomposition with non-negativity clipping.</p>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.mixture.mcr_als--parameters","title":"Parameters","text":"<p>X:     Data matrix of shape (n_samples, n_points). n_components:     Number of components to estimate. max_iter:     Maximum number of ALS iterations. tol:     Convergence tolerance on reconstruction error. random_state:     Optional seed for reproducible initialization.</p>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.mixture.mcr_als--returns","title":"Returns","text":"<p>C:     Concentration profiles (n_samples, n_components). S:     Spectral profiles (n_points, n_components).</p> Source code in <code>src/foodspec/chemometrics/mixture.py</code> <pre><code>def mcr_als(\n    X: np.ndarray,\n    n_components: int,\n    max_iter: int = 100,\n    tol: float = 1e-6,\n    random_state: Optional[int] = None,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Perform a simplified MCR-ALS decomposition with non-negativity clipping.\n\n    Parameters\n    ----------\n    X:\n        Data matrix of shape (n_samples, n_points).\n    n_components:\n        Number of components to estimate.\n    max_iter:\n        Maximum number of ALS iterations.\n    tol:\n        Convergence tolerance on reconstruction error.\n    random_state:\n        Optional seed for reproducible initialization.\n\n    Returns\n    -------\n    C:\n        Concentration profiles (n_samples, n_components).\n    S:\n        Spectral profiles (n_points, n_components).\n    \"\"\"\n\n    rng = np.random.default_rng(random_state)\n    X = np.asarray(X, dtype=float)\n    n_samples, n_points = X.shape\n    S = np.abs(rng.standard_normal(size=(n_points, n_components)))\n    C = np.abs(rng.standard_normal(size=(n_samples, n_components)))\n\n    prev_err = np.inf\n    for _ in range(max_iter):\n        # Update C\n        S_pinv = np.linalg.pinv(S)\n        C = np.maximum(0, X @ S_pinv)\n        # Update S\n        C_pinv = np.linalg.pinv(C)\n        S = np.maximum(0, (C_pinv @ X).T)\n\n        recon = C @ S.T\n        err = np.linalg.norm(X - recon)\n        if abs(prev_err - err) &lt; tol:\n            break\n        prev_err = err\n    return C, S\n</code></pre>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.mixture.nnls_mixture","title":"nnls_mixture","text":"<pre><code>nnls_mixture(spectrum, pure_spectra)\n</code></pre> <p>Fit a non-negative least squares mixture.</p>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.mixture.nnls_mixture--parameters","title":"Parameters","text":"<p>spectrum:     Array of shape (n_points,) representing the mixture spectrum. pure_spectra:     Array of shape (n_points, n_components) containing pure component spectra as columns.</p>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.mixture.nnls_mixture--returns","title":"Returns","text":"<p>coefficients:     Non-negative coefficients for each component (shape (n_components,)). residual_norm:     Euclidean norm of the residual.</p> Source code in <code>src/foodspec/chemometrics/mixture.py</code> <pre><code>def nnls_mixture(spectrum: np.ndarray, pure_spectra: np.ndarray) -&gt; Tuple[np.ndarray, float]:\n    \"\"\"\n    Fit a non-negative least squares mixture.\n\n    Parameters\n    ----------\n    spectrum:\n        Array of shape (n_points,) representing the mixture spectrum.\n    pure_spectra:\n        Array of shape (n_points, n_components) containing pure component spectra as columns.\n\n    Returns\n    -------\n    coefficients:\n        Non-negative coefficients for each component (shape (n_components,)).\n    residual_norm:\n        Euclidean norm of the residual.\n    \"\"\"\n\n    spectrum = np.asarray(spectrum, dtype=float).ravel()\n    pure_spectra = np.asarray(pure_spectra, dtype=float)\n    if pure_spectra.shape[0] != spectrum.shape[0]:\n        raise ValueError(\"pure_spectra rows must match spectrum length.\")\n\n    if scipy_nnls is not None:\n        coeffs, res = scipy_nnls(pure_spectra, spectrum)\n    else:  # simple non-negative least squares fallback\n        coeffs, *_ = np.linalg.lstsq(pure_spectra, spectrum, rcond=None)\n        coeffs = np.clip(coeffs, 0, None)\n        res = np.linalg.norm(spectrum - pure_spectra @ coeffs)\n    return coeffs, float(res)\n</code></pre>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.deep.Conv1DSpectrumClassifier","title":"Conv1DSpectrumClassifier","text":"<p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Opinionated 1D CNN classifier for spectra (optional dependency: Keras).</p>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.deep.Conv1DSpectrumClassifier--parameters","title":"Parameters","text":"<p>epochs : int     Number of training epochs. batch_size : int     Batch size for training. validation_split : float     Fraction of training data used for validation. random_state : int | None     Optional random seed.</p>"},{"location":"api_features_chemometrics/#foodspec.chemometrics.deep.Conv1DSpectrumClassifier--notes","title":"Notes","text":"<ul> <li>Uses a fixed shallow 1D CNN architecture.</li> <li>Requires TensorFlow/Keras (<code>pip install tensorflow</code>).</li> <li>Not used in core workflows; for advanced experimentation only.</li> </ul> Source code in <code>src/foodspec/chemometrics/deep.py</code> <pre><code>class Conv1DSpectrumClassifier(BaseEstimator, ClassifierMixin):\n    \"\"\"Opinionated 1D CNN classifier for spectra (optional dependency: Keras).\n\n    Parameters\n    ----------\n    epochs : int\n        Number of training epochs.\n    batch_size : int\n        Batch size for training.\n    validation_split : float\n        Fraction of training data used for validation.\n    random_state : int | None\n        Optional random seed.\n\n    Notes\n    -----\n    - Uses a fixed shallow 1D CNN architecture.\n    - Requires TensorFlow/Keras (`pip install tensorflow`).\n    - Not used in core workflows; for advanced experimentation only.\n    \"\"\"\n\n    def __init__(\n        self,\n        epochs: int = 20,\n        batch_size: int = 32,\n        validation_split: float = 0.1,\n        random_state: Optional[int] = None,\n    ):\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.validation_split = validation_split\n        self.random_state = random_state\n        self.model_ = None\n        self.classes_: Optional[np.ndarray] = None\n        if importlib.util.find_spec(\"tensorflow\") is None:\n            raise ImportError(\n                \"Conv1DSpectrumClassifier requires TensorFlow. \"\n                \"Please install the deep extra: pip install 'foodspec[deep]'.\"\n            )\n\n    def fit(self, X: np.ndarray, y: np.ndarray):\n        try:\n            import tensorflow as tf  # type: ignore\n        except ImportError as exc:  # pragma: no cover - optional dep\n            raise ImportError(\n                \"Conv1DSpectrumClassifier requires TensorFlow. \"\n                \"Please install the deep extra: pip install 'foodspec[deep]'.\"\n            ) from exc\n\n        X = np.asarray(X, dtype=np.float32)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D (n_samples, n_wavenumbers).\")\n        y = np.asarray(y)\n        self.classes_, y_idx = np.unique(y, return_inverse=True)\n\n        if self.random_state is not None:\n            try:\n                tf.keras.utils.set_random_seed(self.random_state)\n            except Exception:\n                pass\n\n        n_points = X.shape[1]\n        model = tf.keras.Sequential(\n            [\n                tf.keras.layers.Input(shape=(n_points, 1)),\n                tf.keras.layers.Conv1D(32, 5, activation=\"relu\", padding=\"same\"),\n                tf.keras.layers.MaxPool1D(pool_size=2),\n                tf.keras.layers.Conv1D(64, 5, activation=\"relu\", padding=\"same\"),\n                tf.keras.layers.MaxPool1D(pool_size=2),\n                tf.keras.layers.Flatten(),\n                tf.keras.layers.Dense(64, activation=\"relu\"),\n                tf.keras.layers.Dense(len(self.classes_), activation=\"softmax\"),\n            ]\n        )\n        model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n        model.fit(\n            X[..., None],\n            y_idx,\n            epochs=self.epochs,\n            batch_size=self.batch_size,\n            validation_split=self.validation_split,\n            verbose=0,\n        )\n        self.model_ = model\n        return self\n\n    def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:\n        if self.model_ is None or self.classes_ is None:\n            raise RuntimeError(\"Model is not fitted. Call fit() first.\")\n        X = np.asarray(X, dtype=np.float32)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D (n_samples, n_wavenumbers).\")\n        probs = self.model_.predict(X[..., None], verbose=0)\n        return probs\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        probs = self.predict_proba(X)\n        idx = np.argmax(probs, axis=1)\n        return self.classes_[idx]\n</code></pre>"},{"location":"api_io_data/","title":"API reference: IO &amp; datasets","text":"<p>IO and data modules handle conversion between raw files, CSV, HDF5 libraries, and example/public datasets. For practical steps, see the CSV\u2192Library guide and Libraries section.</p>"},{"location":"api_io_data/#io","title":"IO","text":"<p>I/O loaders for spectral data.</p> <p>CSV import utilities for converting public spectral datasets into FoodSpectrumSet.</p> <p>Export utilities for spectral datasets.</p>"},{"location":"api_io_data/#foodspec.io.loaders.load_folder","title":"load_folder","text":"<pre><code>load_folder(\n    folder,\n    pattern=\"*.txt\",\n    modality=\"raman\",\n    metadata_csv=None,\n    wavenumber_column=0,\n    intensity_columns=None,\n)\n</code></pre> <p>Load spectra from a folder of text files into a <code>FoodSpectrumSet</code>.</p>"},{"location":"api_io_data/#foodspec.io.loaders.load_folder--parameters","title":"Parameters","text":"<p>folder :     Directory containing spectra files. pattern :     Glob pattern for spectra files. modality :     Spectroscopy modality string. metadata_csv :     Optional CSV with a <code>sample_id</code> column used to merge metadata by file basename. wavenumber_column :     Column index for wavenumbers in the spectra files. intensity_columns :     Optional indices for intensity columns. If multiple are provided, their mean     is taken. When omitted, all columns except <code>wavenumber_column</code> are used.</p>"},{"location":"api_io_data/#foodspec.io.loaders.load_folder--returns","title":"Returns","text":"<p>FoodSpectrumSet     Loaded dataset with a common wavenumber axis.</p> Source code in <code>src/foodspec/io/loaders.py</code> <pre><code>def load_folder(\n    folder: PathLike,\n    pattern: str = \"*.txt\",\n    modality: str = \"raman\",\n    metadata_csv: Optional[PathLike] = None,\n    wavenumber_column: int = 0,\n    intensity_columns: Optional[Sequence[int]] = None,\n) -&gt; FoodSpectrumSet:\n    \"\"\"Load spectra from a folder of text files into a ``FoodSpectrumSet``.\n\n    Parameters\n    ----------\n    folder :\n        Directory containing spectra files.\n    pattern :\n        Glob pattern for spectra files.\n    modality :\n        Spectroscopy modality string.\n    metadata_csv :\n        Optional CSV with a ``sample_id`` column used to merge metadata by file basename.\n    wavenumber_column :\n        Column index for wavenumbers in the spectra files.\n    intensity_columns :\n        Optional indices for intensity columns. If multiple are provided, their mean\n        is taken. When omitted, all columns except ``wavenumber_column`` are used.\n\n    Returns\n    -------\n    FoodSpectrumSet\n        Loaded dataset with a common wavenumber axis.\n    \"\"\"\n\n    folder_path = Path(folder)\n    files = sorted(folder_path.glob(pattern))\n    if not files:\n        raise ValueError(f\"No files matching pattern '{pattern}' found in {folder_path}.\")\n\n    w_axes = []\n    spectra = []\n    sample_ids = []\n    for file in files:\n        wav, inten = _read_spectrum(\n            file,\n            wavenumber_column=wavenumber_column,\n            intensity_columns=intensity_columns,\n        )\n        w_axes.append(wav)\n        spectra.append(inten)\n        sample_ids.append(file.stem)\n\n    common_axis, stacked = _stack_spectra_on_common_axis(w_axes, spectra)\n\n    metadata = pd.DataFrame({\"sample_id\": sample_ids})\n    if metadata_csv is not None:\n        meta_df = pd.read_csv(metadata_csv)\n        if \"sample_id\" not in meta_df.columns:\n            raise ValueError(\"metadata_csv must contain a 'sample_id' column.\")\n        metadata = metadata.merge(meta_df, on=\"sample_id\", how=\"left\")\n\n    return FoodSpectrumSet(\n        x=stacked,\n        wavenumbers=common_axis,\n        metadata=metadata,\n        modality=modality,\n    )\n</code></pre>"},{"location":"api_io_data/#foodspec.io.loaders.load_from_metadata_table","title":"load_from_metadata_table","text":"<pre><code>load_from_metadata_table(\n    metadata_csv,\n    modality=\"raman\",\n    wavenumber_column=0,\n    intensity_columns=None,\n)\n</code></pre> <p>Load spectra listed in a metadata table.</p>"},{"location":"api_io_data/#foodspec.io.loaders.load_from_metadata_table--parameters","title":"Parameters","text":"<p>metadata_csv :     CSV file with a <code>file_path</code> column and optional metadata columns. modality :     Spectroscopy modality string. wavenumber_column :     Column index for wavenumbers in the spectra files. intensity_columns :     Optional indices for intensity columns. If multiple are provided, their mean     is taken.</p>"},{"location":"api_io_data/#foodspec.io.loaders.load_from_metadata_table--returns","title":"Returns","text":"<p>FoodSpectrumSet     Loaded dataset with a common wavenumber axis.</p> Source code in <code>src/foodspec/io/loaders.py</code> <pre><code>def load_from_metadata_table(\n    metadata_csv: PathLike,\n    modality: str = \"raman\",\n    wavenumber_column: int = 0,\n    intensity_columns: Optional[Sequence[int]] = None,\n) -&gt; FoodSpectrumSet:\n    \"\"\"Load spectra listed in a metadata table.\n\n    Parameters\n    ----------\n    metadata_csv :\n        CSV file with a ``file_path`` column and optional metadata columns.\n    modality :\n        Spectroscopy modality string.\n    wavenumber_column :\n        Column index for wavenumbers in the spectra files.\n    intensity_columns :\n        Optional indices for intensity columns. If multiple are provided, their mean\n        is taken.\n\n    Returns\n    -------\n    FoodSpectrumSet\n        Loaded dataset with a common wavenumber axis.\n    \"\"\"\n\n    table_path = Path(metadata_csv)\n    table = pd.read_csv(table_path)\n    if \"file_path\" not in table.columns:\n        raise ValueError(\"metadata_csv must contain a 'file_path' column.\")\n\n    spectra = []\n    w_axes = []\n    sample_ids = []\n    resolved_paths = []\n    for file_entry in table[\"file_path\"]:\n        file_path = Path(file_entry)\n        if not file_path.is_absolute():\n            file_path = table_path.parent / file_path\n        resolved_paths.append(file_path)\n        wav, inten = _read_spectrum(\n            file_path,\n            wavenumber_column=wavenumber_column,\n            intensity_columns=intensity_columns,\n        )\n        w_axes.append(wav)\n        spectra.append(inten)\n        sample_ids.append(file_path.stem)\n\n    common_axis, stacked = _stack_spectra_on_common_axis(w_axes, spectra)\n\n    metadata = table.drop(columns=[\"file_path\"]).copy()\n    if \"sample_id\" not in metadata.columns:\n        metadata.insert(0, \"sample_id\", sample_ids)\n    else:\n        metadata = metadata.reset_index(drop=True)\n\n    return FoodSpectrumSet(\n        x=stacked,\n        wavenumbers=common_axis,\n        metadata=metadata,\n        modality=modality,\n    )\n</code></pre>"},{"location":"api_io_data/#foodspec.io.csv_import--supported-formats","title":"Supported formats","text":"<p>1) 'wide' format (one wavenumber column, one column per spectrum):    wavenumber, sample_001, sample_002, ...    500,       123.4,      98.1,       ...    502,       124.0,      99.2,       ...</p> <p>2) 'long' / tidy format (one row per (sample, wavenumber)):    sample_id, wavenumber, intensity, [label], [modality], [any other metadata...]</p> <p>The loader returns a FoodSpectrumSet, which can be saved as an HDF5 library via create_library() and used by all foodspec workflows.</p>"},{"location":"api_io_data/#foodspec.io.csv_import.load_csv_spectra","title":"load_csv_spectra","text":"<pre><code>load_csv_spectra(\n    csv_path,\n    format=\"wide\",\n    *,\n    wavenumber_column=\"wavenumber\",\n    intensity_columns=None,\n    sample_id_column=\"sample_id\",\n    intensity_column=\"intensity\",\n    label_column=None,\n    modality=\"raman\"\n)\n</code></pre> <p>Load spectra from a CSV file into a FoodSpectrumSet.</p>"},{"location":"api_io_data/#foodspec.io.csv_import.load_csv_spectra--parameters","title":"Parameters","text":"<p>csv_path:     Path to the CSV file. format:     'wide'  \u2013 one row per wavenumber, one column per sample spectrum.     'long'  \u2013 one row per (sample, wavenumber) with an intensity column. wavenumber_column:     Name of the wavenumber column (both formats). intensity_columns:     For 'wide' format: which columns contain intensities.     If None, all non-wavenumber columns are treated as spectra. sample_id_column:     For 'long' format: column giving sample identifiers. intensity_column:     For 'long' format: column giving intensity values. label_column:     Optional column to copy into metadata (e.g., oil_type). modality:     'raman', 'ftir', etc. Used to tag the FoodSpectrumSet.</p>"},{"location":"api_io_data/#foodspec.io.csv_import.load_csv_spectra--returns","title":"Returns","text":"<p>FoodSpectrumSet     Spectral dataset ready for preprocessing and modeling.</p> Source code in <code>src/foodspec/io/csv_import.py</code> <pre><code>def load_csv_spectra(\n    csv_path: str | Path,\n    format: str = \"wide\",\n    *,\n    wavenumber_column: str = \"wavenumber\",\n    intensity_columns: Optional[Iterable[str]] = None,\n    sample_id_column: str = \"sample_id\",\n    intensity_column: str = \"intensity\",\n    label_column: Optional[str] = None,\n    modality: str = \"raman\",\n) -&gt; FoodSpectrumSet:\n    \"\"\"\n    Load spectra from a CSV file into a FoodSpectrumSet.\n\n    Parameters\n    ----------\n    csv_path:\n        Path to the CSV file.\n    format:\n        'wide'  \u2013 one row per wavenumber, one column per sample spectrum.\n        'long'  \u2013 one row per (sample, wavenumber) with an intensity column.\n    wavenumber_column:\n        Name of the wavenumber column (both formats).\n    intensity_columns:\n        For 'wide' format: which columns contain intensities.\n        If None, all non-wavenumber columns are treated as spectra.\n    sample_id_column:\n        For 'long' format: column giving sample identifiers.\n    intensity_column:\n        For 'long' format: column giving intensity values.\n    label_column:\n        Optional column to copy into metadata (e.g., oil_type).\n    modality:\n        'raman', 'ftir', etc. Used to tag the FoodSpectrumSet.\n\n    Returns\n    -------\n    FoodSpectrumSet\n        Spectral dataset ready for preprocessing and modeling.\n    \"\"\"\n\n    csv_path = Path(csv_path)\n    if not csv_path.exists():\n        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n\n    df = pd.read_csv(csv_path)\n\n    if format.lower() == \"wide\":\n        if wavenumber_column not in df.columns:\n            raise ValueError(f\"Expected wavenumber column '{wavenumber_column}' in CSV.\")\n        wn = df[wavenumber_column].to_numpy(dtype=float)\n\n        if intensity_columns is None:\n            intensity_columns = [c for c in df.columns if c != wavenumber_column]\n        intensity_columns = list(intensity_columns)\n        if not intensity_columns:\n            raise ValueError(\"No intensity columns found for 'wide' CSV format.\")\n\n        # Transpose so rows = samples, cols = wavenumbers\n        spectra = df[intensity_columns].to_numpy(dtype=float).T  # shape: (n_samples, n_wn)\n        metadata = pd.DataFrame({\"sample_id\": intensity_columns})\n        if label_column and label_column in df.columns:\n            metadata[label_column] = np.nan\n\n    elif format.lower() == \"long\":\n        for col in (sample_id_column, wavenumber_column, intensity_column):\n            if col not in df.columns:\n                raise ValueError(f\"Expected column '{col}' in 'long' CSV format.\")\n\n        wn = df[wavenumber_column].drop_duplicates().sort_values().to_numpy(dtype=float)\n\n        pivot = df.pivot_table(\n            index=sample_id_column,\n            columns=wavenumber_column,\n            values=intensity_column,\n        )\n        pivot = pivot.reindex(columns=wn)\n        spectra = pivot.to_numpy(dtype=float)\n        metadata = pd.DataFrame({sample_id_column: pivot.index.to_list()})\n\n        if label_column and label_column in df.columns:\n            labels = (\n                df[[sample_id_column, label_column]]\n                .drop_duplicates(subset=[sample_id_column])\n                .set_index(sample_id_column)[label_column]\n            )\n            metadata[label_column] = metadata[sample_id_column].map(labels)\n    else:\n        raise ValueError(\"format must be 'wide' or 'long'.\")\n\n    ds = FoodSpectrumSet(\n        x=spectra,\n        wavenumbers=wn,\n        metadata=metadata,\n        modality=modality,\n    )\n    validate_spectrum_set(ds)\n    return ds\n</code></pre>"},{"location":"api_io_data/#foodspec.io.exporters.to_hdf5","title":"to_hdf5","text":"<pre><code>to_hdf5(spectra, path)\n</code></pre> <p>Persist spectra to an HDF5 file.</p> <p>Stores datasets: <code>x</code>, <code>wavenumbers</code>; attribute <code>modality</code>; dataset <code>metadata_json</code> with metadata serialized via <code>DataFrame.to_json</code>.</p> Source code in <code>src/foodspec/io/exporters.py</code> <pre><code>def to_hdf5(spectra: FoodSpectrumSet, path: PathLike) -&gt; None:\n    \"\"\"Persist spectra to an HDF5 file.\n\n    Stores datasets: ``x``, ``wavenumbers``; attribute ``modality``; dataset\n    ``metadata_json`` with metadata serialized via ``DataFrame.to_json``.\n    \"\"\"\n\n    try:\n        import h5py\n    except ModuleNotFoundError as exc:  # pragma: no cover - tested via importorskip\n        raise ImportError(\"h5py is required for HDF5 export.\") from exc\n\n    metadata_json = spectra.metadata.to_json(orient=\"table\")\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    with h5py.File(path, \"w\") as h5:\n        h5.create_dataset(\"x\", data=spectra.x)\n        h5.create_dataset(\"wavenumbers\", data=spectra.wavenumbers)\n        h5.create_dataset(\"metadata_json\", data=np.bytes_(metadata_json))\n        h5.attrs[\"modality\"] = spectra.modality\n</code></pre>"},{"location":"api_io_data/#foodspec.io.exporters.to_tidy_csv","title":"to_tidy_csv","text":"<pre><code>to_tidy_csv(spectra, path)\n</code></pre> <p>Export spectra to a tidy (long-form) CSV.</p> <p>Columns: <code>sample_id</code>, all metadata fields, <code>wavenumber</code>, <code>intensity</code>.</p> Source code in <code>src/foodspec/io/exporters.py</code> <pre><code>def to_tidy_csv(spectra: FoodSpectrumSet, path: PathLike) -&gt; None:\n    \"\"\"Export spectra to a tidy (long-form) CSV.\n\n    Columns: ``sample_id``, all metadata fields, ``wavenumber``, ``intensity``.\n    \"\"\"\n\n    metadata = spectra.metadata.copy()\n    if \"sample_id\" not in metadata.columns:\n        metadata.insert(0, \"sample_id\", metadata.index.astype(str))\n\n    n_samples, n_wavenumbers = spectra.x.shape\n    repeated_metadata = {}\n    for col in metadata.columns:\n        repeated_metadata[col] = np.repeat(metadata[col].to_numpy(), n_wavenumbers)\n\n    tidy = pd.DataFrame(repeated_metadata)\n    tidy[\"wavenumber\"] = np.tile(spectra.wavenumbers, n_samples)\n    tidy[\"intensity\"] = spectra.x.reshape(-1)\n\n    # Order columns: sample_id, other metadata, wavenumber, intensity\n    metadata_cols = list(metadata.columns)\n    other_cols = [c for c in metadata_cols if c != \"sample_id\"]\n    tidy = tidy[[\"sample_id\", *other_cols, \"wavenumber\", \"intensity\"]]\n\n    Path(path).parent.mkdir(parents=True, exist_ok=True)\n    tidy.to_csv(path, index=False)\n</code></pre>"},{"location":"api_io_data/#data-loaders","title":"Data loaders","text":"<p>Example data loaders.</p> <p>Public dataset loaders for foodspec.</p> <p>These functions expect users to manually download open datasets to a known location on disk. Each loader performs light parsing into a FoodSpectrumSet and attaches basic metadata. They will raise clear errors with instructions if the expected files are not present.</p>"},{"location":"api_io_data/#foodspec.data.loader.load_example_oils","title":"load_example_oils","text":"<pre><code>load_example_oils(seed=0)\n</code></pre> <p>Load a small synthetic edible oils dataset.</p>"},{"location":"api_io_data/#foodspec.data.loader.load_example_oils--notes","title":"Notes","text":"<p>This currently generates synthetic data; in future it can be replaced with reads from packaged resources via <code>importlib.resources.files</code>.</p> Source code in <code>src/foodspec/data/loader.py</code> <pre><code>def load_example_oils(seed: Optional[int] = 0) -&gt; FoodSpectrumSet:\n    \"\"\"Load a small synthetic edible oils dataset.\n\n    Notes\n    -----\n    This currently generates synthetic data; in future it can be replaced with\n    reads from packaged resources via ``importlib.resources.files``.\n    \"\"\"\n\n    rng = np.random.default_rng(seed)\n    wavenumbers = np.linspace(800, 1800, 250)\n    n_per_class = 15\n    classes = [\"olive\", \"sunflower\"]\n    spectra = []\n    labels = []\n    for cls in classes:\n        for _ in range(n_per_class):\n            base = np.zeros_like(wavenumbers)\n            if cls == \"olive\":\n                base += 1.2 * np.exp(-0.5 * ((wavenumbers - 1655) / 12) ** 2)\n                base += 1.0 * np.exp(-0.5 * ((wavenumbers - 1742) / 14) ** 2)\n            else:\n                base += 0.8 * np.exp(-0.5 * ((wavenumbers - 1655) / 12) ** 2)\n                base += 1.3 * np.exp(-0.5 * ((wavenumbers - 1742) / 14) ** 2)\n            noise = rng.normal(0, 0.02, size=wavenumbers.shape)\n            spectra.append(base + noise)\n            labels.append(cls)\n\n    X = np.vstack(spectra)\n    metadata = pd.DataFrame(\n        {\n            \"sample_id\": [f\"{lbl}_{i}\" for i, lbl in enumerate(labels)],\n            \"oil_type\": labels,\n            \"matrix\": [\"pure-oil\"] * len(labels),\n        }\n    )\n    return FoodSpectrumSet(x=X, wavenumbers=wavenumbers, metadata=metadata, modality=\"raman\")\n</code></pre>"},{"location":"api_io_data/#foodspec.data.public.load_public_evoo_sunflower_raman","title":"load_public_evoo_sunflower_raman","text":"<pre><code>load_public_evoo_sunflower_raman(root=None)\n</code></pre> <p>Load a public French EVOO\u2013sunflower Raman mixture dataset.</p> <p>Expected layout (user-provided): root/   *.csv  (first column wavenumbers, second column intensity; one spectrum per file)</p> <p>Metadata: - mixture_fraction_evoo (parsed from filename if numeric; else NaN) - dataset_name = \"EVOO-Sunflower Raman\" - doi = \"10.5281/zenodo.\" (replace with actual) - instrument = \"\" - oil_type (optional: derived from filename) - modality = \"raman\""},{"location":"api_io_data/#foodspec.data.public.load_public_evoo_sunflower_raman--notes","title":"Notes","text":"<p>Download the dataset manually (e.g., from Zenodo with the DOI above) and place files under <code>~/foodspec_datasets/evoo_sunflower_raman</code> or pass <code>root=...</code>.</p> Source code in <code>src/foodspec/data/public.py</code> <pre><code>def load_public_evoo_sunflower_raman(root: PathLike | None = None) -&gt; FoodSpectrumSet:\n    \"\"\"Load a public French EVOO\u2013sunflower Raman mixture dataset.\n\n    Expected layout (user-provided):\n    root/\n      *.csv  (first column wavenumbers, second column intensity; one spectrum per file)\n\n    Metadata:\n    - mixture_fraction_evoo (parsed from filename if numeric; else NaN)\n    - dataset_name = \"EVOO-Sunflower Raman\"\n    - doi = \"10.5281/zenodo.&lt;record_id&gt;\" (replace with actual)\n    - instrument = \"&lt;insert instrument name&gt;\"\n    - oil_type (optional: derived from filename)\n    - modality = \"raman\"\n\n    Notes\n    -----\n    Download the dataset manually (e.g., from Zenodo with the DOI above) and place files under\n    ``~/foodspec_datasets/evoo_sunflower_raman`` or pass ``root=...``.\n    \"\"\"\n\n    data_root = _default_root(\"evoo_sunflower_raman\", root)\n    csv_files = sorted(data_root.glob(\"*.csv\"))\n    if not csv_files:\n        raise FileNotFoundError(\n            f\"No CSV files found under {data_root}. \"\n            \"Download the EVOO\u2013sunflower Raman dataset (DOI placeholder) and place CSVs here.\"\n        )\n\n    spectra = []\n    labels = []\n    fractions = []\n    for fpath in csv_files:\n        df = pd.read_csv(fpath)\n        if df.shape[1] &lt; 2:\n            continue\n        wn = df.iloc[:, 0].to_numpy(dtype=float)\n        intensity = df.iloc[:, 1].to_numpy(dtype=float)\n        spectra.append(intensity)\n        # parse mixture fraction from filename (e.g., evoo_70.csv -&gt; 70)\n        stem_parts = fpath.stem.split(\"_\")\n        frac = None\n        for part in stem_parts:\n            try:\n                frac = float(part)\n                if frac &gt; 1:\n                    frac = frac / 100.0\n                break\n            except ValueError:\n                continue\n        fractions.append(frac if frac is not None else np.nan)\n        labels.append(fpath.stem)\n\n    if not spectra:\n        raise FileNotFoundError(f\"No usable spectra found in {data_root}.\")\n\n    X = np.vstack(spectra)\n    metadata = pd.DataFrame(\n        {\n            \"sample_id\": labels,\n            \"mixture_fraction_evoo\": fractions,\n            \"dataset_name\": [\"EVOO-Sunflower Raman\"] * len(labels),\n            \"doi\": [\"10.5281/zenodo.&lt;record_id&gt;\"] * len(labels),\n            \"instrument\": [\"&lt;insert instrument&gt;\"] * len(labels),\n            \"modality\": [\"raman\"] * len(labels),\n        }\n    )\n    fs = FoodSpectrumSet(x=X, wavenumbers=wn, metadata=metadata, modality=\"raman\")\n    validate_spectrum_set(fs, allow_nan=True)\n    validate_public_evoo_sunflower(fs)\n    return fs\n</code></pre>"},{"location":"api_io_data/#foodspec.data.public.load_public_ftir_oils","title":"load_public_ftir_oils","text":"<pre><code>load_public_ftir_oils(root=None)\n</code></pre> <p>Load a public FTIR edible-oil dataset.</p> <p>Expected layout (user-provided): root/   *.csv (first column wavenumbers, remaining columns spectra) OR   one spectrum per CSV (first col wn, second col intensity)</p> <p>Metadata: - oil_type (from filename prefix) - modality = \"ftir\" - source = \"public\" - doi = \"10.5281/zenodo.\" (replace with actual)"},{"location":"api_io_data/#foodspec.data.public.load_public_ftir_oils--notes","title":"Notes","text":"<p>Download the dataset manually (e.g., from Zenodo or institutional repository) and place files under <code>~/foodspec_datasets/ftir_oils</code> or pass <code>root=...</code>.</p> Source code in <code>src/foodspec/data/public.py</code> <pre><code>def load_public_ftir_oils(root: PathLike | None = None) -&gt; FoodSpectrumSet:\n    \"\"\"Load a public FTIR edible-oil dataset.\n\n    Expected layout (user-provided):\n    root/\n      *.csv (first column wavenumbers, remaining columns spectra) OR\n      one spectrum per CSV (first col wn, second col intensity)\n\n    Metadata:\n    - oil_type (from filename prefix)\n    - modality = \"ftir\"\n    - source = \"public\"\n    - doi = \"10.5281/zenodo.&lt;record_id&gt;\" (replace with actual)\n\n    Notes\n    -----\n    Download the dataset manually (e.g., from Zenodo or institutional repository) and place files under\n    ``~/foodspec_datasets/ftir_oils`` or pass ``root=...``.\n    \"\"\"\n\n    data_root = _default_root(\"ftir_oils\", root)\n    csv_files = sorted(data_root.glob(\"*.csv\"))\n    if not csv_files:\n        raise FileNotFoundError(\n            f\"No CSV files found under {data_root}. \"\n            \"Download the public FTIR oils dataset (DOI placeholder) and place CSVs here.\"\n        )\n\n    spectra_list = []\n    labels = []\n    wn_ref = None\n    for fpath in csv_files:\n        df = pd.read_csv(fpath)\n        if df.shape[1] &lt; 2:\n            continue\n        oil_label = fpath.stem.split(\"_\")[0]\n        if df.shape[1] == 2:\n            wn = df.iloc[:, 0].to_numpy(dtype=float)\n            intensity = df.iloc[:, 1].to_numpy(dtype=float)\n            spectra_list.append(intensity)\n            labels.append(oil_label)\n            if wn_ref is None:\n                wn_ref = wn\n        else:\n            wn = df.iloc[:, 0].to_numpy(dtype=float)\n            Xcols = df.iloc[:, 1:].to_numpy(dtype=float).T\n            spectra_list.append(Xcols)\n            labels.extend([oil_label] * Xcols.shape[0])\n            if wn_ref is None:\n                wn_ref = wn\n\n    if not spectra_list:\n        raise FileNotFoundError(f\"No usable spectra found in {data_root}.\")\n\n    X_concat = np.vstack([s if s.ndim == 2 else s.reshape(1, -1) for s in spectra_list])\n    metadata = pd.DataFrame(\n        {\n            \"sample_id\": [f\"{lbl}_{i}\" for i, lbl in enumerate(labels)],\n            \"oil_type\": labels,\n            \"modality\": [\"ftir\"] * len(labels),\n            \"source\": [\"public\"] * len(labels),\n            \"doi\": [\"10.5281/zenodo.&lt;record_id&gt;\"] * len(labels),\n        }\n    )\n    fs = FoodSpectrumSet(x=X_concat, wavenumbers=wn_ref, metadata=metadata, modality=\"ftir\")\n    validate_spectrum_set(fs)\n    return fs\n</code></pre>"},{"location":"api_io_data/#foodspec.data.public.load_public_mendeley_oils","title":"load_public_mendeley_oils","text":"<pre><code>load_public_mendeley_oils(root=None)\n</code></pre> <p>Load Raman/FTIR edible oils from a public Mendeley dataset.</p> <p>Expected layout (user-provided): root/   *.csv  (each CSV: first column wavenumbers, remaining columns spectra for a single modality)</p> <p>Metadata: - oil_type (from filename stem up to first underscore) - modality (\"raman\" or \"ftir\" inferred from filename if possible; default \"raman\") - source = \"Mendeley\" - doi = \"10.17632/\" (replace with actual)"},{"location":"api_io_data/#foodspec.data.public.load_public_mendeley_oils--notes","title":"Notes","text":"<p>This loader does not download files. Download the dataset from Mendeley (e.g., https://data.mendeley.com/ with the DOI above) and place the CSVs under the default path: <code>~/foodspec_datasets/mendeley_oils</code> or pass <code>root=...</code> explicitly.</p> Source code in <code>src/foodspec/data/public.py</code> <pre><code>def load_public_mendeley_oils(root: PathLike | None = None) -&gt; FoodSpectrumSet:\n    \"\"\"Load Raman/FTIR edible oils from a public Mendeley dataset.\n\n    Expected layout (user-provided):\n    root/\n      *.csv  (each CSV: first column wavenumbers, remaining columns spectra for a single modality)\n\n    Metadata:\n    - oil_type (from filename stem up to first underscore)\n    - modality (\"raman\" or \"ftir\" inferred from filename if possible; default \"raman\")\n    - source = \"Mendeley\"\n    - doi = \"10.17632/&lt;dataset_id&gt;\" (replace with actual)\n\n    Notes\n    -----\n    This loader does not download files. Download the dataset from Mendeley\n    (e.g., https://data.mendeley.com/ with the DOI above) and place the CSVs under the default path:\n    ``~/foodspec_datasets/mendeley_oils`` or pass ``root=...`` explicitly.\n    \"\"\"\n\n    data_root = _default_root(\"mendeley_oils\", root)\n    csv_files = sorted(data_root.glob(\"*.csv\"))\n    if not csv_files:\n        raise FileNotFoundError(\n            f\"No CSV files found under {data_root}. \"\n            \"Download the Mendeley oils dataset (DOI placeholder) and place CSVs here.\"\n        )\n\n    spectra_list = []\n    labels = []\n    modality_list = []\n    for fpath in csv_files:\n        df = pd.read_csv(fpath)\n        if df.shape[1] &lt; 2:\n            continue\n        wn = df.iloc[:, 0].to_numpy(dtype=float)\n        X = df.iloc[:, 1:].to_numpy(dtype=float).T  # each column is a spectrum\n        oil_label = fpath.stem.split(\"_\")[0]\n        # Simple modality heuristic\n        mod = \"ftir\" if \"ftir\" in fpath.stem.lower() else \"raman\"\n        spectra_list.append((X, wn, oil_label, mod))\n        labels.extend([oil_label] * X.shape[0])\n        modality_list.extend([mod] * X.shape[0])\n\n    if not spectra_list:\n        raise FileNotFoundError(f\"No usable spectra found in {data_root}.\")\n\n    # Assume all share a common axis (or first file as reference)\n    X_concat = np.vstack([s[0] for s in spectra_list])\n    wn_ref = spectra_list[0][1]\n    metadata = pd.DataFrame(\n        {\n            \"sample_id\": [f\"{lbl}_{i}\" for i, lbl in enumerate(labels)],\n            \"oil_type\": labels,\n            \"modality\": modality_list,\n            \"source\": [\"Mendeley\"] * len(labels),\n            \"doi\": [\"10.17632/&lt;dataset_id&gt;\"] * len(labels),\n        }\n    )\n    fs = FoodSpectrumSet(x=X_concat, wavenumbers=wn_ref, metadata=metadata, modality=\"raman\")\n    validate_spectrum_set(fs)\n    return fs\n</code></pre> <p>Status: Archived This page has been superseded by the unified API reference. See api_reference.md and api_reference.md for current IO and data loader APIs.</p>"},{"location":"api_preprocess/","title":"API reference: preprocessing","text":"<p>Preprocessing modules provide sklearn-compatible transformers for baseline correction, smoothing, normalization, cropping, FTIR/Raman helpers, and derivatives. For recommended pipelines, see the Preprocessing guide and workflow tutorials.</p>"},{"location":"api_preprocess/#baseline","title":"Baseline","text":"<p>Baseline correction transformers.</p>"},{"location":"api_preprocess/#foodspec.preprocess.baseline.ALSBaseline","title":"ALSBaseline","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Asymmetric Least Squares baseline correction (Eilers, 2005).</p> Source code in <code>src/foodspec/preprocess/baseline.py</code> <pre><code>class ALSBaseline(BaseEstimator, TransformerMixin):\n    \"\"\"Asymmetric Least Squares baseline correction (Eilers, 2005).\"\"\"\n\n    def __init__(self, lambda_: float = 1e5, p: float = 0.001, max_iter: int = 10):\n        self.lambda_ = lambda_\n        self.p = p\n        self.max_iter = max_iter\n\n    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; \"ALSBaseline\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        n_samples, n_wavenumbers = X.shape\n        if self.lambda_ &lt;= 0:\n            raise ValueError(\"lambda_ must be positive.\")\n        if not (0 &lt; self.p &lt; 1):\n            raise ValueError(\"p must be in (0, 1).\")\n        if self.max_iter &lt;= 0:\n            raise ValueError(\"max_iter must be positive.\")\n\n        D = _second_derivative_matrix(n_wavenumbers)\n        baselines = np.zeros_like(X)\n        for i, y in enumerate(X):\n            w = np.ones(n_wavenumbers)\n            for _ in range(self.max_iter):\n                W = diags(w, 0, shape=(n_wavenumbers, n_wavenumbers))\n                Z = W + self.lambda_ * (D.T @ D)\n                z = spsolve(Z, w * y)\n                w = self.p * (y &gt; z) + (1 - self.p) * (y &lt; z)\n            baseline = z\n            corrected_candidate = y - baseline\n            edge_mean = corrected_candidate[: min(20, n_wavenumbers)].mean()\n            if not np.isfinite(baseline).all() or abs(edge_mean) &gt; 1.0:\n                baseline = _poly_baseline(y, degree=2)\n            baselines[i, :] = baseline\n        return X - baselines\n</code></pre>"},{"location":"api_preprocess/#foodspec.preprocess.baseline.PolynomialBaseline","title":"PolynomialBaseline","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Baseline correction by polynomial fitting.</p> Source code in <code>src/foodspec/preprocess/baseline.py</code> <pre><code>class PolynomialBaseline(BaseEstimator, TransformerMixin):\n    \"\"\"Baseline correction by polynomial fitting.\"\"\"\n\n    def __init__(self, degree: int = 3):\n        self.degree = degree\n\n    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; \"PolynomialBaseline\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        if self.degree &lt; 0:\n            raise ValueError(\"degree must be non-negative.\")\n\n        n_samples, n_wavenumbers = X.shape\n        x_axis = np.linspace(0, 1, n_wavenumbers)\n        corrected = np.zeros_like(X)\n        for i, y in enumerate(X):\n            coefs = np.polyfit(x_axis, y, deg=self.degree)\n            baseline = np.polyval(coefs, x_axis)\n            corrected[i, :] = y - baseline\n        return corrected\n</code></pre>"},{"location":"api_preprocess/#foodspec.preprocess.baseline.RubberbandBaseline","title":"RubberbandBaseline","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Baseline correction using convex hull (rubberband) approach.</p> Source code in <code>src/foodspec/preprocess/baseline.py</code> <pre><code>class RubberbandBaseline(BaseEstimator, TransformerMixin):\n    \"\"\"Baseline correction using convex hull (rubberband) approach.\"\"\"\n\n    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; \"RubberbandBaseline\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n\n        n_samples, n_wavenumbers = X.shape\n        wavenumbers = np.arange(n_wavenumbers)\n        corrected = np.zeros_like(X)\n\n        for i, y in enumerate(X):\n            lower = _lower_hull_indices(wavenumbers, y)\n            baseline = np.interp(wavenumbers, wavenumbers[lower], y[lower])\n            corrected[i, :] = y - baseline\n\n        return corrected\n</code></pre>"},{"location":"api_preprocess/#smoothing","title":"Smoothing","text":"<p>Smoothing transformers.</p>"},{"location":"api_preprocess/#foodspec.preprocess.smoothing.MovingAverageSmoother","title":"MovingAverageSmoother","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Simple moving average smoother.</p> Source code in <code>src/foodspec/preprocess/smoothing.py</code> <pre><code>class MovingAverageSmoother(BaseEstimator, TransformerMixin):\n    \"\"\"Simple moving average smoother.\"\"\"\n\n    def __init__(self, window_size: int = 5):\n        self.window_size = window_size\n\n    def fit(self, X: np.ndarray, y=None) -&gt; \"MovingAverageSmoother\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        if self.window_size &lt;= 0:\n            raise ValueError(\"window_size must be positive.\")\n        if self.window_size &gt; X.shape[1]:\n            raise ValueError(\"window_size cannot exceed number of wavenumbers.\")\n\n        def _smooth_row(row: np.ndarray) -&gt; np.ndarray:\n            out = np.empty_like(row)\n            last_val = row[-1]\n            for i in range(row.shape[0]):\n                window = row[i : i + self.window_size]\n                if window.shape[0] &lt; self.window_size:\n                    window = np.concatenate([window, np.full(self.window_size - window.shape[0], last_val)])\n                out[i] = window.mean()\n            return out\n\n        return np.apply_along_axis(_smooth_row, 1, X)\n</code></pre>"},{"location":"api_preprocess/#foodspec.preprocess.smoothing.SavitzkyGolaySmoother","title":"SavitzkyGolaySmoother","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Savitzky-Golay smoothing.</p> Source code in <code>src/foodspec/preprocess/smoothing.py</code> <pre><code>class SavitzkyGolaySmoother(BaseEstimator, TransformerMixin):\n    \"\"\"Savitzky-Golay smoothing.\"\"\"\n\n    def __init__(self, window_length: int = 7, polyorder: int = 3):\n        self.window_length = window_length\n        self.polyorder = polyorder\n\n    def fit(self, X: np.ndarray, y=None) -&gt; \"SavitzkyGolaySmoother\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        if self.window_length &lt;= 0 or self.window_length % 2 == 0:\n            raise ValueError(\"window_length must be a positive odd integer.\")\n        if self.polyorder &gt;= self.window_length:\n            raise ValueError(\"polyorder must be less than window_length.\")\n\n        if self.window_length &gt; X.shape[1]:\n            raise ValueError(\"window_length cannot exceed number of wavenumbers.\")\n\n        return savgol_filter(\n            X, window_length=self.window_length, polyorder=self.polyorder, axis=1\n        )\n</code></pre>"},{"location":"api_preprocess/#normalization-scatter-correction","title":"Normalization &amp; scatter correction","text":"<p>Normalization transformers.</p> <p>Includes vector, area, internal-peak, standard normal variate (SNV), and multiplicative scatter correction (MSC) methods commonly used in spectroscopy.</p>"},{"location":"api_preprocess/#foodspec.preprocess.normalization.AreaNormalizer","title":"AreaNormalizer","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Normalize spectra to unit area.</p> Source code in <code>src/foodspec/preprocess/normalization.py</code> <pre><code>class AreaNormalizer(BaseEstimator, TransformerMixin):\n    \"\"\"Normalize spectra to unit area.\"\"\"\n\n    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; \"AreaNormalizer\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n\n        area = np.trapezoid(X, axis=1, dx=1.0).reshape(-1, 1)\n        area = np.maximum(np.abs(area), np.finfo(float).eps)\n        return X / area\n</code></pre>"},{"location":"api_preprocess/#foodspec.preprocess.normalization.InternalPeakNormalizer","title":"InternalPeakNormalizer","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Normalize spectra using an internal reference peak window.</p> Source code in <code>src/foodspec/preprocess/normalization.py</code> <pre><code>class InternalPeakNormalizer(BaseEstimator, TransformerMixin):\n    \"\"\"Normalize spectra using an internal reference peak window.\"\"\"\n\n    def __init__(self, target_wavenumber: float, window: float = 10.0):\n        self.target_wavenumber = target_wavenumber\n        self.window = window\n\n    def fit(\n        self,\n        X: np.ndarray,\n        y: Optional[np.ndarray] = None,\n        wavenumbers: Optional[np.ndarray] = None,\n    ) -&gt; \"InternalPeakNormalizer\":\n        return self\n\n    def transform(\n        self, X: np.ndarray, wavenumbers: Optional[np.ndarray] = None\n    ) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        if wavenumbers is None:\n            raise ValueError(\"wavenumbers array is required for InternalPeakNormalizer.\")\n\n        wavenumbers = np.asarray(wavenumbers, dtype=float)\n        if wavenumbers.ndim != 1:\n            raise ValueError(\"wavenumbers must be 1D.\")\n        if wavenumbers.shape[0] != X.shape[1]:\n            raise ValueError(\"wavenumbers length must match number of columns in X.\")\n        if self.window &lt;= 0:\n            raise ValueError(\"window must be positive.\")\n\n        half = self.window / 2.0\n        mask = (wavenumbers &gt;= self.target_wavenumber - half) &amp; (\n            wavenumbers &lt;= self.target_wavenumber + half\n        )\n        if not np.any(mask):\n            raise ValueError(\"No points found within the specified window.\")\n\n        ref = np.mean(X[:, mask], axis=1, keepdims=True)\n        ref = np.maximum(np.abs(ref), np.finfo(float).eps)\n        return X / ref\n</code></pre>"},{"location":"api_preprocess/#foodspec.preprocess.normalization.MSCNormalizer","title":"MSCNormalizer","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Multiplicative scatter correction (MSC) using a reference mean spectrum.</p> Source code in <code>src/foodspec/preprocess/normalization.py</code> <pre><code>class MSCNormalizer(BaseEstimator, TransformerMixin):\n    \"\"\"Multiplicative scatter correction (MSC) using a reference mean spectrum.\"\"\"\n\n    def __init__(self):\n        self.reference_: Optional[np.ndarray] = None\n\n    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; \"MSCNormalizer\":\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        self.reference_ = X.mean(axis=0)\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        if self.reference_ is None:\n            raise RuntimeError(\"MSCNormalizer has not been fitted. Call fit() first.\")\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        ref = self.reference_\n        ref_mean = ref.mean()\n        ref_centered = ref - ref_mean\n        denom = np.dot(ref_centered, ref_centered)\n        denom = np.maximum(denom, np.finfo(float).eps)\n\n        corrected = np.empty_like(X, dtype=float)\n        for i, x in enumerate(X):\n            x_mean = x.mean()\n            b = np.dot(ref_centered, x - x_mean) / denom\n            b = np.maximum(b, np.finfo(float).eps)\n            a = x_mean - b * ref_mean\n            corrected[i, :] = (x - a) / b\n        return corrected\n</code></pre>"},{"location":"api_preprocess/#foodspec.preprocess.normalization.SNVNormalizer","title":"SNVNormalizer","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Standard normal variate (SNV) normalization per spectrum.</p> Source code in <code>src/foodspec/preprocess/normalization.py</code> <pre><code>class SNVNormalizer(BaseEstimator, TransformerMixin):\n    \"\"\"Standard normal variate (SNV) normalization per spectrum.\"\"\"\n\n    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; \"SNVNormalizer\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        mean = X.mean(axis=1, keepdims=True)\n        std = X.std(axis=1, keepdims=True)\n        std = np.maximum(std, np.finfo(float).eps)\n        return (X - mean) / std\n</code></pre>"},{"location":"api_preprocess/#foodspec.preprocess.normalization.VectorNormalizer","title":"VectorNormalizer","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Vector normalization across spectral axis.</p> Source code in <code>src/foodspec/preprocess/normalization.py</code> <pre><code>class VectorNormalizer(BaseEstimator, TransformerMixin):\n    \"\"\"Vector normalization across spectral axis.\"\"\"\n\n    def __init__(self, norm: Literal[\"l1\", \"l2\", \"max\"] = \"l2\"):\n        self.norm = norm\n\n    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; \"VectorNormalizer\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        if self.norm not in {\"l1\", \"l2\", \"max\"}:\n            raise ValueError(\"norm must be one of {'l1', 'l2', 'max'}.\")\n\n        if self.norm == \"l1\":\n            denom = np.sum(np.abs(X), axis=1, keepdims=True)\n        elif self.norm == \"l2\":\n            denom = np.linalg.norm(X, ord=2, axis=1, keepdims=True)\n        else:\n            denom = np.max(np.abs(X), axis=1, keepdims=True)\n\n        denom = np.maximum(denom, np.finfo(float).eps)\n        return X / denom\n</code></pre>"},{"location":"api_preprocess/#cropping","title":"Cropping","text":"<p>Cropping utilities for spectral ranges.</p>"},{"location":"api_preprocess/#foodspec.preprocess.cropping.RangeCropper","title":"RangeCropper","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Crop spectra to a specified wavenumber range.</p> Source code in <code>src/foodspec/preprocess/cropping.py</code> <pre><code>class RangeCropper(BaseEstimator):\n    \"\"\"Crop spectra to a specified wavenumber range.\"\"\"\n\n    def __init__(self, min_wn: float, max_wn: float):\n        if min_wn &gt;= max_wn:\n            raise ValueError(\"min_wn must be less than max_wn.\")\n        self.min_wn = min_wn\n        self.max_wn = max_wn\n\n    def fit(self, X: np.ndarray, y=None, wavenumbers: np.ndarray | None = None):\n        return self\n\n    def transform(\n        self, X: np.ndarray, wavenumbers: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        X = np.asarray(X, dtype=float)\n        wavenumbers = np.asarray(wavenumbers, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        if wavenumbers.ndim != 1 or wavenumbers.shape[0] != X.shape[1]:\n            raise ValueError(\"wavenumbers must be 1D and match columns of X.\")\n\n        mask = (wavenumbers &gt;= self.min_wn) &amp; (wavenumbers &lt;= self.max_wn)\n        if not np.any(mask):\n            raise ValueError(\"No wavenumbers within the specified range.\")\n        return X[:, mask], wavenumbers[mask]\n</code></pre>"},{"location":"api_preprocess/#foodspec.preprocess.cropping.crop_spectrum_set","title":"crop_spectrum_set","text":"<pre><code>crop_spectrum_set(spectra, min_wn, max_wn)\n</code></pre> <p>Crop a FoodSpectrumSet to a wavenumber range.</p> Source code in <code>src/foodspec/preprocess/cropping.py</code> <pre><code>def crop_spectrum_set(spectra: FoodSpectrumSet, min_wn: float, max_wn: float) -&gt; FoodSpectrumSet:\n    \"\"\"Crop a FoodSpectrumSet to a wavenumber range.\"\"\"\n\n    cropper = RangeCropper(min_wn=min_wn, max_wn=max_wn)\n    x_cropped, wn_cropped = cropper.transform(spectra.x, spectra.wavenumbers)\n    return FoodSpectrumSet(\n        x=x_cropped,\n        wavenumbers=wn_cropped,\n        metadata=spectra.metadata.copy(),\n        modality=spectra.modality,\n    )\n</code></pre>"},{"location":"api_preprocess/#ftir-raman-helpers","title":"FTIR / Raman helpers","text":"<p>Simplified FTIR-specific corrections.</p> <p>Raman-specific preprocessing helpers.</p>"},{"location":"api_preprocess/#foodspec.preprocess.ftir.AtmosphericCorrector","title":"AtmosphericCorrector","text":"<p>               Bases: <code>WavenumberAwareMixin</code>, <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Atmospheric correction using synthetic or user-provided water/CO2 bases.</p> <p>This is a simplified approach (not vendor-grade). You may supply explicit water/co2 basis arrays (shape n_points x n_bases); otherwise, broad Gaussian bases are generated at typical water/CO2 positions.</p> Source code in <code>src/foodspec/preprocess/ftir.py</code> <pre><code>class AtmosphericCorrector(WavenumberAwareMixin, BaseEstimator, TransformerMixin):\n    \"\"\"Atmospheric correction using synthetic or user-provided water/CO2 bases.\n\n    This is a simplified approach (not vendor-grade). You may supply explicit\n    water/co2 basis arrays (shape n_points x n_bases); otherwise, broad Gaussian\n    bases are generated at typical water/CO2 positions.\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha_water: float = 1.0,\n        alpha_co2: float = 1.0,\n        water_center: float = 1900.0,\n        co2_center: float = 2350.0,\n        width: float = 30.0,\n        water_basis: Optional[np.ndarray] = None,\n        co2_basis: Optional[np.ndarray] = None,\n        normalize_bases: bool = True,\n    ):\n        self.alpha_water = alpha_water\n        self.alpha_co2 = alpha_co2\n        self.water_center = water_center\n        self.co2_center = co2_center\n        self.width = width\n        self.water_basis = water_basis\n        self.co2_basis = co2_basis\n        self.normalize_bases = normalize_bases\n\n    def fit(self, X, y=None, wavenumbers: Optional[np.ndarray] = None):\n        if wavenumbers is not None:\n            self.set_wavenumbers(wavenumbers)\n        self._assert_wavenumbers_set()\n        bases = self._build_bases(self.wavenumbers_)\n        if self.normalize_bases:\n            norms = np.linalg.norm(bases, axis=0, keepdims=True)\n            norms = np.maximum(norms, np.finfo(float).eps)\n            bases = bases / norms\n        self._bases = bases\n        return self\n\n    def transform(self, X):\n        self._assert_wavenumbers_set()\n        X = np.asarray(X, dtype=float)\n        bases = self._bases\n        BtB = bases.T @ bases\n        pseudo = np.linalg.pinv(BtB) @ bases.T\n        corrected = []\n        for spectrum in X:\n            coeffs = pseudo @ spectrum\n            resid = spectrum - bases @ coeffs\n            corrected.append(resid)\n        return np.vstack(corrected)\n\n    def _build_bases(self, wn: np.ndarray) -&gt; np.ndarray:\n        if self.water_basis is not None or self.co2_basis is not None:\n            parts = []\n            if self.water_basis is not None:\n                parts.append(np.asarray(self.water_basis, dtype=float))\n            if self.co2_basis is not None:\n                parts.append(np.asarray(self.co2_basis, dtype=float))\n            return np.column_stack(parts)\n        water = self.alpha_water * np.exp(-0.5 * ((wn - self.water_center) / self.width) ** 2)\n        co2 = self.alpha_co2 * np.exp(-0.5 * ((wn - self.co2_center) / self.width) ** 2)\n        return np.vstack([water, co2]).T\n</code></pre>"},{"location":"api_preprocess/#foodspec.preprocess.ftir.SimpleATRCorrector","title":"SimpleATRCorrector","text":"<p>               Bases: <code>WavenumberAwareMixin</code>, <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Approximate ATR correction using heuristic scaling.</p> Source code in <code>src/foodspec/preprocess/ftir.py</code> <pre><code>class SimpleATRCorrector(WavenumberAwareMixin, BaseEstimator, TransformerMixin):\n    \"\"\"Approximate ATR correction using heuristic scaling.\"\"\"\n\n    def __init__(\n        self,\n        refractive_index_sample: float = 1.5,\n        refractive_index_crystal: float = 2.4,\n        angle_of_incidence: float = 45.0,\n        wavenumber_scale: str = \"linear\",\n    ):\n        self.refractive_index_sample = refractive_index_sample\n        self.refractive_index_crystal = refractive_index_crystal\n        self.angle_of_incidence = angle_of_incidence\n        self.wavenumber_scale = wavenumber_scale\n\n    def fit(self, X, y=None, wavenumbers: Optional[np.ndarray] = None):\n        if wavenumbers is not None:\n            self.set_wavenumbers(wavenumbers)\n        self._assert_wavenumbers_set()\n        self._scale = self._compute_scale(self.wavenumbers_)\n        return self\n\n    def transform(self, X):\n        self._assert_wavenumbers_set()\n        X = np.asarray(X, dtype=float)\n        return X * self._scale\n\n    def _compute_scale(self, wn: np.ndarray) -&gt; np.ndarray:\n        ratio = self.refractive_index_sample / self.refractive_index_crystal\n        angle_factor = 1.0 + 0.01 * (self.angle_of_incidence - 45.0)\n        scale = 1.0 / (1.0 + angle_factor * ratio * (wn / wn.max()))\n        return scale\n</code></pre>"},{"location":"api_preprocess/#foodspec.preprocess.raman.CosmicRayRemover","title":"CosmicRayRemover","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Basic cosmic ray spike removal via thresholding.</p> <p>Detects spikes as points exceeding local median by <code>sigma_thresh</code> times the local MAD and replaces them by linear interpolation of neighbors.</p> Source code in <code>src/foodspec/preprocess/raman.py</code> <pre><code>class CosmicRayRemover(BaseEstimator, TransformerMixin):\n    \"\"\"Basic cosmic ray spike removal via thresholding.\n\n    Detects spikes as points exceeding local median by `sigma_thresh` times\n    the local MAD and replaces them by linear interpolation of neighbors.\n    \"\"\"\n\n    def __init__(self, window: int = 5, sigma_thresh: float = 8.0):\n        self.window = window\n        self.sigma_thresh = sigma_thresh\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D with shape (n_samples, n_points).\")\n        window = max(3, int(self.window))\n        corrected = []\n        for spectrum in X:\n            corrected.append(self._despike(spectrum, window))\n        return np.vstack(corrected)\n\n    def _despike(self, y: np.ndarray, window: int) -&gt; np.ndarray:\n        half = window // 2\n        y_clean = y.copy()\n        for i in range(len(y)):\n            start = max(0, i - half)\n            end = min(len(y), i + half + 1)\n            local = y[start:end]\n            median = np.median(local)\n            mad = np.median(np.abs(local - median)) + 1e-8\n            if abs(y[i] - median) &gt; self.sigma_thresh * mad:\n                # interpolate neighbors if available\n                left = y_clean[i - 1] if i &gt; 0 else median\n                right = y_clean[i + 1] if i + 1 &lt; len(y) else median\n                y_clean[i] = 0.5 * (left + right)\n        return y_clean\n</code></pre>"},{"location":"api_preprocess/#derivatives","title":"Derivatives","text":"<p>Derivative transformers using Savitzky-Golay.</p> <p>Status: Archived This page has been superseded by the unified API reference. See api_reference.md for current preprocessing APIs.</p>"},{"location":"api_preprocess/#foodspec.preprocess.derivatives.DerivativeTransformer","title":"DerivativeTransformer","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Savitzky-Golay derivative transformer.</p> Source code in <code>src/foodspec/preprocess/derivatives.py</code> <pre><code>class DerivativeTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Savitzky-Golay derivative transformer.\"\"\"\n\n    def __init__(\n        self,\n        order: Literal[1, 2] = 1,\n        window_length: int = 7,\n        polyorder: int = 3,\n    ):\n        self.order = order\n        self.window_length = window_length\n        self.polyorder = polyorder\n\n    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; \"DerivativeTransformer\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        if self.order not in {1, 2}:\n            raise ValueError(\"order must be 1 or 2.\")\n        if self.window_length &lt;= 0 or self.window_length % 2 == 0:\n            raise ValueError(\"window_length must be a positive odd integer.\")\n        if self.polyorder &gt;= self.window_length:\n            raise ValueError(\"polyorder must be less than window_length.\")\n        if self.window_length &gt; X.shape[1]:\n            raise ValueError(\"window_length cannot exceed number of wavenumbers.\")\n\n        return savgol_filter(\n            X,\n            window_length=self.window_length,\n            polyorder=self.polyorder,\n            deriv=self.order,\n            axis=1,\n        )\n</code></pre>"},{"location":"api_reference/","title":"API reference hub","text":"<p>This section lists the main public classes and functions in foodspec via mkdocstrings. Use the keyword index for concept-based lookup, and the CLI page for command usage.</p>"},{"location":"api_reference/#core-data-structures","title":"Core data structures","text":"<p>Collection of spectra with aligned metadata and axis information.</p> <p>Container for hyperspectral maps stored as (height, width, n_points).</p> Source code in <code>src/foodspec/core/hyperspectral.py</code> <pre><code>@dataclass\nclass HyperSpectralCube:\n    \"\"\"Container for hyperspectral maps stored as (height, width, n_points).\"\"\"\n\n    cube: np.ndarray  # shape (height, width, n_points)\n    wavenumbers: np.ndarray  # shape (n_points,)\n    metadata: pd.DataFrame\n    image_shape: Tuple[int, int]\n\n    def __post_init__(self) -&gt; None:\n        self.cube = np.asarray(self.cube, dtype=float)\n        self.wavenumbers = np.asarray(self.wavenumbers, dtype=float)\n        if self.cube.ndim != 3:\n            raise ValueError(\"cube must be 3D (height, width, n_points).\")\n        h, w, n_points = self.cube.shape\n        if self.wavenumbers.shape[0] != n_points:\n            raise ValueError(\"wavenumbers length must match cube spectral dimension.\")\n        if (h, w) != self.image_shape:\n            raise ValueError(\"image_shape does not match cube spatial dimensions.\")\n        if not isinstance(self.metadata, pd.DataFrame):\n            raise ValueError(\"metadata must be a pandas DataFrame.\")\n\n    @classmethod\n    def from_spectrum_set(cls, spectra: FoodSpectrumSet, image_shape: Tuple[int, int]) -&gt; \"HyperSpectralCube\":\n        \"\"\"Create cube from flattened spectra using image_shape (h, w).\"\"\"\n        h, w = image_shape\n        n_pixels = h * w\n        if len(spectra) != n_pixels:\n            raise ValueError(\"Number of spectra does not match image_shape pixels.\")\n        cube = spectra.x.reshape(h, w, -1)\n        return cls(\n            cube=cube,\n            wavenumbers=spectra.wavenumbers,\n            metadata=spectra.metadata.copy(),\n            image_shape=image_shape,\n        )\n\n    def to_spectrum_set(self, modality: str = \"raman\") -&gt; FoodSpectrumSet:\n        \"\"\"Flatten cube to FoodSpectrumSet, adding row/col coordinates.\"\"\"\n        h, w, n_points = self.cube.shape\n        flat = self.cube.reshape(h * w, n_points)\n        coords = pd.DataFrame({\"row\": np.repeat(np.arange(h), w), \"col\": np.tile(np.arange(w), h)})\n        meta = self.metadata.copy().reset_index(drop=True)\n        meta = pd.concat([coords, meta], axis=1)\n        return FoodSpectrumSet(x=flat, wavenumbers=self.wavenumbers, metadata=meta, modality=modality)\n\n    def to_pixel_spectra(self, modality: str = \"raman\") -&gt; FoodSpectrumSet:\n        \"\"\"Flatten to pixel spectra with row/col metadata.\"\"\"\n        return self.to_spectrum_set(modality=modality)\n\n    def from_pixel_labels(self, labels: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Reshape flat labels (n_pixels,) to (height, width) label image.\"\"\"\n        labels = np.asarray(labels)\n        h, w = self.image_shape\n        if labels.shape[0] != h * w:\n            raise ValueError(\"labels length must match number of pixels (height*width).\")\n        return labels.reshape(h, w)\n\n    def get_pixel_spectrum(self, row: int, col: int) -&gt; np.ndarray:\n        \"\"\"Return spectrum at a given pixel coordinate.\"\"\"\n        if row &lt; 0 or row &gt;= self.image_shape[0] or col &lt; 0 or col &gt;= self.image_shape[1]:\n            raise IndexError(\"Pixel indices out of range.\")\n        return self.cube[row, col, :]\n\n    def mean_spectrum(self) -&gt; np.ndarray:\n        \"\"\"Return mean spectrum over all pixels.\"\"\"\n        return self.cube.reshape(-1, self.cube.shape[-1]).mean(axis=0)\n</code></pre>"},{"location":"api_reference/#foodspec.core.dataset.FoodSpectrumSet--parameters","title":"Parameters","text":"<p>x :     Array of shape (n_samples, n_wavenumbers) containing spectral intensities. wavenumbers :     Array of shape (n_wavenumbers,) with the spectral axis values. metadata :     DataFrame with one row per sample storing labels and acquisition info. modality :     Spectroscopy modality identifier: <code>\"raman\"</code>, <code>\"ftir\"</code>, or <code>\"nir\"</code>.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>@dataclass\nclass FoodSpectrumSet:\n    \"\"\"Collection of spectra with aligned metadata and axis information.\n\n    Parameters\n    ----------\n    x :\n        Array of shape (n_samples, n_wavenumbers) containing spectral intensities.\n    wavenumbers :\n        Array of shape (n_wavenumbers,) with the spectral axis values.\n    metadata :\n        DataFrame with one row per sample storing labels and acquisition info.\n    modality :\n        Spectroscopy modality identifier: ``\"raman\"``, ``\"ftir\"``, or ``\"nir\"``.\n    \"\"\"\n\n    x: np.ndarray\n    wavenumbers: np.ndarray\n    metadata: pd.DataFrame\n    modality: Modality\n\n    def __post_init__(self) -&gt; None:\n        self.validate()\n\n    def __len__(self) -&gt; int:\n        \"\"\"Number of spectra in the set.\"\"\"\n\n        return self.x.shape[0]\n\n    def __getitem__(self, index: IndexType) -&gt; \"FoodSpectrumSet\":\n        \"\"\"Return a subset by position index or slice.\"\"\"\n\n        indices = self._normalize_index(index)\n        return FoodSpectrumSet(\n            x=self.x[indices],\n            wavenumbers=self.wavenumbers.copy(),\n            metadata=self.metadata.iloc[indices].reset_index(drop=True),\n            modality=self.modality,\n        )\n\n    def subset(\n        self,\n        by: Optional[Dict[str, Any]] = None,\n        indices: Optional[Sequence[int]] = None,\n    ) -&gt; \"FoodSpectrumSet\":\n        \"\"\"Subset the dataset by metadata filters and/or explicit indices.\n\n        Parameters\n        ----------\n        by :\n            Mapping of metadata column names to desired values. For sequence-like\n            values, membership (``isin``) is applied; otherwise equality is used.\n        indices :\n            Explicit indices to retain. If provided together with ``by``, the\n            intersection is taken in the order of ``indices``.\n\n        Returns\n        -------\n        FoodSpectrumSet\n            A new dataset containing the selected spectra.\n        \"\"\"\n\n        if by is None and indices is None:\n            return self.copy(deep=False)\n\n        mask = np.ones(len(self), dtype=bool)\n        if by is not None:\n            for key, value in by.items():\n                if key not in self.metadata.columns:\n                    raise ValueError(f\"Metadata column '{key}' not found.\")\n                series = self.metadata[key]\n                if isinstance(value, (list, tuple, set, np.ndarray, pd.Series)):\n                    mask &amp;= series.isin(value).to_numpy()\n                else:\n                    mask &amp;= (series == value).to_numpy()\n\n        if indices is not None:\n            indices_array = np.asarray(indices, dtype=int)\n            if indices_array.ndim != 1:\n                raise ValueError(\"indices must be a 1D sequence of integers.\")\n            if np.any(indices_array &lt; 0) or np.any(indices_array &gt;= len(self)):\n                raise ValueError(\"indices contain out-of-range values.\")\n            if by is not None:\n                indices_array = np.array(\n                    [idx for idx in indices_array if mask[idx]], dtype=int\n                )\n            selected_indices = indices_array\n        else:\n            selected_indices = np.where(mask)[0]\n\n        return FoodSpectrumSet(\n            x=self.x[selected_indices],\n            wavenumbers=self.wavenumbers.copy(),\n            metadata=self.metadata.iloc[selected_indices].reset_index(drop=True),\n            modality=self.modality,\n        )\n\n    def copy(self, deep: bool = True) -&gt; \"FoodSpectrumSet\":\n        \"\"\"Return a copy of the dataset.\n\n        Parameters\n        ----------\n        deep :\n            If True, copy underlying arrays and metadata; otherwise reuse references.\n\n        Returns\n        -------\n        FoodSpectrumSet\n            Copied dataset.\n        \"\"\"\n\n        if deep:\n            x = np.array(self.x, copy=True)\n            wavenumbers = np.array(self.wavenumbers, copy=True)\n            metadata = self.metadata.copy(deep=True)\n        else:\n            x = self.x\n            wavenumbers = self.wavenumbers\n            metadata = self.metadata\n\n        return FoodSpectrumSet(\n            x=x, wavenumbers=wavenumbers, metadata=metadata, modality=self.modality\n        )\n\n    def to_wide_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Convert the dataset to a wide DataFrame.\n\n        Returns\n        -------\n        pandas.DataFrame\n            Metadata columns followed by one column per wavenumber named\n            ``int_&lt;wavenumber&gt;``.\n        \"\"\"\n\n        intensity_columns = [f\"int_{float(wn)}\" for wn in self.wavenumbers]\n        spectra_df = pd.DataFrame(self.x, columns=intensity_columns)\n        return pd.concat(\n            [self.metadata.reset_index(drop=True).copy(), spectra_df], axis=1\n        )\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate array shapes, metadata length, and modality.\"\"\"\n\n        if self.x.ndim != 2:\n            raise ValueError(\"x must be a 2D array of shape (n_samples, n_wavenumbers).\")\n        if self.wavenumbers.ndim != 1:\n            raise ValueError(\"wavenumbers must be a 1D array.\")\n        n_samples, n_wavenumbers = self.x.shape\n        if n_wavenumbers &lt; 3:\n            raise ValueError(\"At least three wavenumber points are required.\")\n        if self.wavenumbers.shape[0] != n_wavenumbers:\n            raise ValueError(\n                \"wavenumbers length does not match number of columns in x \"\n                f\"({self.wavenumbers.shape[0]} != {n_wavenumbers}).\"\n            )\n        if len(self.metadata) != n_samples:\n            raise ValueError(\n                \"metadata length does not match number of rows in x \"\n                f\"({len(self.metadata)} != {n_samples}).\"\n            )\n        if self.modality not in {\"raman\", \"ftir\", \"nir\"}:\n            raise ValueError(\n                \"modality must be one of {'raman', 'ftir', 'nir'}; \"\n                f\"got '{self.modality}'.\"\n            )\n\n    def _normalize_index(self, index: IndexType) -&gt; np.ndarray:\n        \"\"\"Normalize indexing input to an array of indices.\"\"\"\n\n        if isinstance(index, int):\n            if index &lt; 0 or index &gt;= len(self):\n                raise IndexError(\"index out of range.\")\n            return np.array([index])\n        if isinstance(index, slice):\n            return np.arange(len(self))[index]\n        raise TypeError(\"Index must be an integer or slice.\")\n\n    def to_X_y(self, target_col: str) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Return (X, y) for a target column in metadata.\"\"\"\n\n        if target_col not in self.metadata.columns:\n            raise ValueError(f\"Target column '{target_col}' not found in metadata.\")\n        return self.x, self.metadata[target_col].to_numpy()\n\n    def train_test_split(\n        self,\n        target_col: str,\n        test_size: float = 0.3,\n        stratify: bool = True,\n        random_state: Optional[int] = None,\n    ) -&gt; tuple[\"FoodSpectrumSet\", \"FoodSpectrumSet\"]:\n        \"\"\"Split into train/test FoodSpectrumSets.\"\"\"\n\n        X, y = self.to_X_y(target_col)\n        stratify_arg = y if stratify else None\n        X_train, X_test, y_train, y_test, meta_train, meta_test = train_test_split(\n            X,\n            y,\n            self.metadata,\n            test_size=test_size,\n            random_state=random_state,\n            stratify=stratify_arg,\n        )\n        train_ds = FoodSpectrumSet(\n            x=X_train,\n            wavenumbers=self.wavenumbers.copy(),\n            metadata=meta_train.reset_index(drop=True),\n            modality=self.modality,\n        )\n        test_ds = FoodSpectrumSet(\n            x=X_test,\n            wavenumbers=self.wavenumbers.copy(),\n            metadata=meta_test.reset_index(drop=True),\n            modality=self.modality,\n        )\n        return train_ds, test_ds\n</code></pre>"},{"location":"api_reference/#foodspec.core.dataset.FoodSpectrumSet.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index)\n</code></pre> <p>Return a subset by position index or slice.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>def __getitem__(self, index: IndexType) -&gt; \"FoodSpectrumSet\":\n    \"\"\"Return a subset by position index or slice.\"\"\"\n\n    indices = self._normalize_index(index)\n    return FoodSpectrumSet(\n        x=self.x[indices],\n        wavenumbers=self.wavenumbers.copy(),\n        metadata=self.metadata.iloc[indices].reset_index(drop=True),\n        modality=self.modality,\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.core.dataset.FoodSpectrumSet.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Number of spectra in the set.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Number of spectra in the set.\"\"\"\n\n    return self.x.shape[0]\n</code></pre>"},{"location":"api_reference/#foodspec.core.dataset.FoodSpectrumSet.copy","title":"copy","text":"<pre><code>copy(deep=True)\n</code></pre> <p>Return a copy of the dataset.</p>"},{"location":"api_reference/#foodspec.core.dataset.FoodSpectrumSet.copy--parameters","title":"Parameters","text":"<p>deep :     If True, copy underlying arrays and metadata; otherwise reuse references.</p>"},{"location":"api_reference/#foodspec.core.dataset.FoodSpectrumSet.copy--returns","title":"Returns","text":"<p>FoodSpectrumSet     Copied dataset.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>def copy(self, deep: bool = True) -&gt; \"FoodSpectrumSet\":\n    \"\"\"Return a copy of the dataset.\n\n    Parameters\n    ----------\n    deep :\n        If True, copy underlying arrays and metadata; otherwise reuse references.\n\n    Returns\n    -------\n    FoodSpectrumSet\n        Copied dataset.\n    \"\"\"\n\n    if deep:\n        x = np.array(self.x, copy=True)\n        wavenumbers = np.array(self.wavenumbers, copy=True)\n        metadata = self.metadata.copy(deep=True)\n    else:\n        x = self.x\n        wavenumbers = self.wavenumbers\n        metadata = self.metadata\n\n    return FoodSpectrumSet(\n        x=x, wavenumbers=wavenumbers, metadata=metadata, modality=self.modality\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.core.dataset.FoodSpectrumSet.subset","title":"subset","text":"<pre><code>subset(by=None, indices=None)\n</code></pre> <p>Subset the dataset by metadata filters and/or explicit indices.</p>"},{"location":"api_reference/#foodspec.core.dataset.FoodSpectrumSet.subset--parameters","title":"Parameters","text":"<p>by :     Mapping of metadata column names to desired values. For sequence-like     values, membership (<code>isin</code>) is applied; otherwise equality is used. indices :     Explicit indices to retain. If provided together with <code>by</code>, the     intersection is taken in the order of <code>indices</code>.</p>"},{"location":"api_reference/#foodspec.core.dataset.FoodSpectrumSet.subset--returns","title":"Returns","text":"<p>FoodSpectrumSet     A new dataset containing the selected spectra.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>def subset(\n    self,\n    by: Optional[Dict[str, Any]] = None,\n    indices: Optional[Sequence[int]] = None,\n) -&gt; \"FoodSpectrumSet\":\n    \"\"\"Subset the dataset by metadata filters and/or explicit indices.\n\n    Parameters\n    ----------\n    by :\n        Mapping of metadata column names to desired values. For sequence-like\n        values, membership (``isin``) is applied; otherwise equality is used.\n    indices :\n        Explicit indices to retain. If provided together with ``by``, the\n        intersection is taken in the order of ``indices``.\n\n    Returns\n    -------\n    FoodSpectrumSet\n        A new dataset containing the selected spectra.\n    \"\"\"\n\n    if by is None and indices is None:\n        return self.copy(deep=False)\n\n    mask = np.ones(len(self), dtype=bool)\n    if by is not None:\n        for key, value in by.items():\n            if key not in self.metadata.columns:\n                raise ValueError(f\"Metadata column '{key}' not found.\")\n            series = self.metadata[key]\n            if isinstance(value, (list, tuple, set, np.ndarray, pd.Series)):\n                mask &amp;= series.isin(value).to_numpy()\n            else:\n                mask &amp;= (series == value).to_numpy()\n\n    if indices is not None:\n        indices_array = np.asarray(indices, dtype=int)\n        if indices_array.ndim != 1:\n            raise ValueError(\"indices must be a 1D sequence of integers.\")\n        if np.any(indices_array &lt; 0) or np.any(indices_array &gt;= len(self)):\n            raise ValueError(\"indices contain out-of-range values.\")\n        if by is not None:\n            indices_array = np.array(\n                [idx for idx in indices_array if mask[idx]], dtype=int\n            )\n        selected_indices = indices_array\n    else:\n        selected_indices = np.where(mask)[0]\n\n    return FoodSpectrumSet(\n        x=self.x[selected_indices],\n        wavenumbers=self.wavenumbers.copy(),\n        metadata=self.metadata.iloc[selected_indices].reset_index(drop=True),\n        modality=self.modality,\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.core.dataset.FoodSpectrumSet.to_X_y","title":"to_X_y","text":"<pre><code>to_X_y(target_col)\n</code></pre> <p>Return (X, y) for a target column in metadata.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>def to_X_y(self, target_col: str) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return (X, y) for a target column in metadata.\"\"\"\n\n    if target_col not in self.metadata.columns:\n        raise ValueError(f\"Target column '{target_col}' not found in metadata.\")\n    return self.x, self.metadata[target_col].to_numpy()\n</code></pre>"},{"location":"api_reference/#foodspec.core.dataset.FoodSpectrumSet.to_wide_dataframe","title":"to_wide_dataframe","text":"<pre><code>to_wide_dataframe()\n</code></pre> <p>Convert the dataset to a wide DataFrame.</p>"},{"location":"api_reference/#foodspec.core.dataset.FoodSpectrumSet.to_wide_dataframe--returns","title":"Returns","text":"<p>pandas.DataFrame     Metadata columns followed by one column per wavenumber named     <code>int_&lt;wavenumber&gt;</code>.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>def to_wide_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Convert the dataset to a wide DataFrame.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Metadata columns followed by one column per wavenumber named\n        ``int_&lt;wavenumber&gt;``.\n    \"\"\"\n\n    intensity_columns = [f\"int_{float(wn)}\" for wn in self.wavenumbers]\n    spectra_df = pd.DataFrame(self.x, columns=intensity_columns)\n    return pd.concat(\n        [self.metadata.reset_index(drop=True).copy(), spectra_df], axis=1\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.core.dataset.FoodSpectrumSet.train_test_split","title":"train_test_split","text":"<pre><code>train_test_split(\n    target_col,\n    test_size=0.3,\n    stratify=True,\n    random_state=None,\n)\n</code></pre> <p>Split into train/test FoodSpectrumSets.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>def train_test_split(\n    self,\n    target_col: str,\n    test_size: float = 0.3,\n    stratify: bool = True,\n    random_state: Optional[int] = None,\n) -&gt; tuple[\"FoodSpectrumSet\", \"FoodSpectrumSet\"]:\n    \"\"\"Split into train/test FoodSpectrumSets.\"\"\"\n\n    X, y = self.to_X_y(target_col)\n    stratify_arg = y if stratify else None\n    X_train, X_test, y_train, y_test, meta_train, meta_test = train_test_split(\n        X,\n        y,\n        self.metadata,\n        test_size=test_size,\n        random_state=random_state,\n        stratify=stratify_arg,\n    )\n    train_ds = FoodSpectrumSet(\n        x=X_train,\n        wavenumbers=self.wavenumbers.copy(),\n        metadata=meta_train.reset_index(drop=True),\n        modality=self.modality,\n    )\n    test_ds = FoodSpectrumSet(\n        x=X_test,\n        wavenumbers=self.wavenumbers.copy(),\n        metadata=meta_test.reset_index(drop=True),\n        modality=self.modality,\n    )\n    return train_ds, test_ds\n</code></pre>"},{"location":"api_reference/#foodspec.core.dataset.FoodSpectrumSet.validate","title":"validate","text":"<pre><code>validate()\n</code></pre> <p>Validate array shapes, metadata length, and modality.</p> Source code in <code>src/foodspec/core/dataset.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate array shapes, metadata length, and modality.\"\"\"\n\n    if self.x.ndim != 2:\n        raise ValueError(\"x must be a 2D array of shape (n_samples, n_wavenumbers).\")\n    if self.wavenumbers.ndim != 1:\n        raise ValueError(\"wavenumbers must be a 1D array.\")\n    n_samples, n_wavenumbers = self.x.shape\n    if n_wavenumbers &lt; 3:\n        raise ValueError(\"At least three wavenumber points are required.\")\n    if self.wavenumbers.shape[0] != n_wavenumbers:\n        raise ValueError(\n            \"wavenumbers length does not match number of columns in x \"\n            f\"({self.wavenumbers.shape[0]} != {n_wavenumbers}).\"\n        )\n    if len(self.metadata) != n_samples:\n        raise ValueError(\n            \"metadata length does not match number of rows in x \"\n            f\"({len(self.metadata)} != {n_samples}).\"\n        )\n    if self.modality not in {\"raman\", \"ftir\", \"nir\"}:\n        raise ValueError(\n            \"modality must be one of {'raman', 'ftir', 'nir'}; \"\n            f\"got '{self.modality}'.\"\n        )\n</code></pre>"},{"location":"api_reference/#foodspec.core.hyperspectral.HyperSpectralCube.from_pixel_labels","title":"from_pixel_labels","text":"<pre><code>from_pixel_labels(labels)\n</code></pre> <p>Reshape flat labels (n_pixels,) to (height, width) label image.</p> Source code in <code>src/foodspec/core/hyperspectral.py</code> <pre><code>def from_pixel_labels(self, labels: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Reshape flat labels (n_pixels,) to (height, width) label image.\"\"\"\n    labels = np.asarray(labels)\n    h, w = self.image_shape\n    if labels.shape[0] != h * w:\n        raise ValueError(\"labels length must match number of pixels (height*width).\")\n    return labels.reshape(h, w)\n</code></pre>"},{"location":"api_reference/#foodspec.core.hyperspectral.HyperSpectralCube.from_spectrum_set","title":"from_spectrum_set  <code>classmethod</code>","text":"<pre><code>from_spectrum_set(spectra, image_shape)\n</code></pre> <p>Create cube from flattened spectra using image_shape (h, w).</p> Source code in <code>src/foodspec/core/hyperspectral.py</code> <pre><code>@classmethod\ndef from_spectrum_set(cls, spectra: FoodSpectrumSet, image_shape: Tuple[int, int]) -&gt; \"HyperSpectralCube\":\n    \"\"\"Create cube from flattened spectra using image_shape (h, w).\"\"\"\n    h, w = image_shape\n    n_pixels = h * w\n    if len(spectra) != n_pixels:\n        raise ValueError(\"Number of spectra does not match image_shape pixels.\")\n    cube = spectra.x.reshape(h, w, -1)\n    return cls(\n        cube=cube,\n        wavenumbers=spectra.wavenumbers,\n        metadata=spectra.metadata.copy(),\n        image_shape=image_shape,\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.core.hyperspectral.HyperSpectralCube.get_pixel_spectrum","title":"get_pixel_spectrum","text":"<pre><code>get_pixel_spectrum(row, col)\n</code></pre> <p>Return spectrum at a given pixel coordinate.</p> Source code in <code>src/foodspec/core/hyperspectral.py</code> <pre><code>def get_pixel_spectrum(self, row: int, col: int) -&gt; np.ndarray:\n    \"\"\"Return spectrum at a given pixel coordinate.\"\"\"\n    if row &lt; 0 or row &gt;= self.image_shape[0] or col &lt; 0 or col &gt;= self.image_shape[1]:\n        raise IndexError(\"Pixel indices out of range.\")\n    return self.cube[row, col, :]\n</code></pre>"},{"location":"api_reference/#foodspec.core.hyperspectral.HyperSpectralCube.mean_spectrum","title":"mean_spectrum","text":"<pre><code>mean_spectrum()\n</code></pre> <p>Return mean spectrum over all pixels.</p> Source code in <code>src/foodspec/core/hyperspectral.py</code> <pre><code>def mean_spectrum(self) -&gt; np.ndarray:\n    \"\"\"Return mean spectrum over all pixels.\"\"\"\n    return self.cube.reshape(-1, self.cube.shape[-1]).mean(axis=0)\n</code></pre>"},{"location":"api_reference/#foodspec.core.hyperspectral.HyperSpectralCube.to_pixel_spectra","title":"to_pixel_spectra","text":"<pre><code>to_pixel_spectra(modality='raman')\n</code></pre> <p>Flatten to pixel spectra with row/col metadata.</p> Source code in <code>src/foodspec/core/hyperspectral.py</code> <pre><code>def to_pixel_spectra(self, modality: str = \"raman\") -&gt; FoodSpectrumSet:\n    \"\"\"Flatten to pixel spectra with row/col metadata.\"\"\"\n    return self.to_spectrum_set(modality=modality)\n</code></pre>"},{"location":"api_reference/#foodspec.core.hyperspectral.HyperSpectralCube.to_spectrum_set","title":"to_spectrum_set","text":"<pre><code>to_spectrum_set(modality='raman')\n</code></pre> <p>Flatten cube to FoodSpectrumSet, adding row/col coordinates.</p> Source code in <code>src/foodspec/core/hyperspectral.py</code> <pre><code>def to_spectrum_set(self, modality: str = \"raman\") -&gt; FoodSpectrumSet:\n    \"\"\"Flatten cube to FoodSpectrumSet, adding row/col coordinates.\"\"\"\n    h, w, n_points = self.cube.shape\n    flat = self.cube.reshape(h * w, n_points)\n    coords = pd.DataFrame({\"row\": np.repeat(np.arange(h), w), \"col\": np.tile(np.arange(w), h)})\n    meta = self.metadata.copy().reset_index(drop=True)\n    meta = pd.concat([coords, meta], axis=1)\n    return FoodSpectrumSet(x=flat, wavenumbers=self.wavenumbers, metadata=meta, modality=modality)\n</code></pre>"},{"location":"api_reference/#preprocessing","title":"Preprocessing","text":"<p>Baseline correction transformers.</p> <p>Smoothing transformers.</p> <p>Normalization transformers.</p> <p>Includes vector, area, internal-peak, standard normal variate (SNV), and multiplicative scatter correction (MSC) methods commonly used in spectroscopy.</p> <p>Cropping utilities for spectral ranges.</p> <p>Simplified FTIR-specific corrections.</p> <p>Raman-specific preprocessing helpers.</p> <p>Derivative transformers using Savitzky-Golay.</p>"},{"location":"api_reference/#foodspec.preprocess.baseline.ALSBaseline","title":"ALSBaseline","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Asymmetric Least Squares baseline correction (Eilers, 2005).</p> Source code in <code>src/foodspec/preprocess/baseline.py</code> <pre><code>class ALSBaseline(BaseEstimator, TransformerMixin):\n    \"\"\"Asymmetric Least Squares baseline correction (Eilers, 2005).\"\"\"\n\n    def __init__(self, lambda_: float = 1e5, p: float = 0.001, max_iter: int = 10):\n        self.lambda_ = lambda_\n        self.p = p\n        self.max_iter = max_iter\n\n    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; \"ALSBaseline\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        n_samples, n_wavenumbers = X.shape\n        if self.lambda_ &lt;= 0:\n            raise ValueError(\"lambda_ must be positive.\")\n        if not (0 &lt; self.p &lt; 1):\n            raise ValueError(\"p must be in (0, 1).\")\n        if self.max_iter &lt;= 0:\n            raise ValueError(\"max_iter must be positive.\")\n\n        D = _second_derivative_matrix(n_wavenumbers)\n        baselines = np.zeros_like(X)\n        for i, y in enumerate(X):\n            w = np.ones(n_wavenumbers)\n            for _ in range(self.max_iter):\n                W = diags(w, 0, shape=(n_wavenumbers, n_wavenumbers))\n                Z = W + self.lambda_ * (D.T @ D)\n                z = spsolve(Z, w * y)\n                w = self.p * (y &gt; z) + (1 - self.p) * (y &lt; z)\n            baseline = z\n            corrected_candidate = y - baseline\n            edge_mean = corrected_candidate[: min(20, n_wavenumbers)].mean()\n            if not np.isfinite(baseline).all() or abs(edge_mean) &gt; 1.0:\n                baseline = _poly_baseline(y, degree=2)\n            baselines[i, :] = baseline\n        return X - baselines\n</code></pre>"},{"location":"api_reference/#foodspec.preprocess.baseline.PolynomialBaseline","title":"PolynomialBaseline","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Baseline correction by polynomial fitting.</p> Source code in <code>src/foodspec/preprocess/baseline.py</code> <pre><code>class PolynomialBaseline(BaseEstimator, TransformerMixin):\n    \"\"\"Baseline correction by polynomial fitting.\"\"\"\n\n    def __init__(self, degree: int = 3):\n        self.degree = degree\n\n    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; \"PolynomialBaseline\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        if self.degree &lt; 0:\n            raise ValueError(\"degree must be non-negative.\")\n\n        n_samples, n_wavenumbers = X.shape\n        x_axis = np.linspace(0, 1, n_wavenumbers)\n        corrected = np.zeros_like(X)\n        for i, y in enumerate(X):\n            coefs = np.polyfit(x_axis, y, deg=self.degree)\n            baseline = np.polyval(coefs, x_axis)\n            corrected[i, :] = y - baseline\n        return corrected\n</code></pre>"},{"location":"api_reference/#foodspec.preprocess.baseline.RubberbandBaseline","title":"RubberbandBaseline","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Baseline correction using convex hull (rubberband) approach.</p> Source code in <code>src/foodspec/preprocess/baseline.py</code> <pre><code>class RubberbandBaseline(BaseEstimator, TransformerMixin):\n    \"\"\"Baseline correction using convex hull (rubberband) approach.\"\"\"\n\n    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; \"RubberbandBaseline\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n\n        n_samples, n_wavenumbers = X.shape\n        wavenumbers = np.arange(n_wavenumbers)\n        corrected = np.zeros_like(X)\n\n        for i, y in enumerate(X):\n            lower = _lower_hull_indices(wavenumbers, y)\n            baseline = np.interp(wavenumbers, wavenumbers[lower], y[lower])\n            corrected[i, :] = y - baseline\n\n        return corrected\n</code></pre>"},{"location":"api_reference/#foodspec.preprocess.smoothing.MovingAverageSmoother","title":"MovingAverageSmoother","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Simple moving average smoother.</p> Source code in <code>src/foodspec/preprocess/smoothing.py</code> <pre><code>class MovingAverageSmoother(BaseEstimator, TransformerMixin):\n    \"\"\"Simple moving average smoother.\"\"\"\n\n    def __init__(self, window_size: int = 5):\n        self.window_size = window_size\n\n    def fit(self, X: np.ndarray, y=None) -&gt; \"MovingAverageSmoother\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        if self.window_size &lt;= 0:\n            raise ValueError(\"window_size must be positive.\")\n        if self.window_size &gt; X.shape[1]:\n            raise ValueError(\"window_size cannot exceed number of wavenumbers.\")\n\n        def _smooth_row(row: np.ndarray) -&gt; np.ndarray:\n            out = np.empty_like(row)\n            last_val = row[-1]\n            for i in range(row.shape[0]):\n                window = row[i : i + self.window_size]\n                if window.shape[0] &lt; self.window_size:\n                    window = np.concatenate([window, np.full(self.window_size - window.shape[0], last_val)])\n                out[i] = window.mean()\n            return out\n\n        return np.apply_along_axis(_smooth_row, 1, X)\n</code></pre>"},{"location":"api_reference/#foodspec.preprocess.smoothing.SavitzkyGolaySmoother","title":"SavitzkyGolaySmoother","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Savitzky-Golay smoothing.</p> Source code in <code>src/foodspec/preprocess/smoothing.py</code> <pre><code>class SavitzkyGolaySmoother(BaseEstimator, TransformerMixin):\n    \"\"\"Savitzky-Golay smoothing.\"\"\"\n\n    def __init__(self, window_length: int = 7, polyorder: int = 3):\n        self.window_length = window_length\n        self.polyorder = polyorder\n\n    def fit(self, X: np.ndarray, y=None) -&gt; \"SavitzkyGolaySmoother\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        if self.window_length &lt;= 0 or self.window_length % 2 == 0:\n            raise ValueError(\"window_length must be a positive odd integer.\")\n        if self.polyorder &gt;= self.window_length:\n            raise ValueError(\"polyorder must be less than window_length.\")\n\n        if self.window_length &gt; X.shape[1]:\n            raise ValueError(\"window_length cannot exceed number of wavenumbers.\")\n\n        return savgol_filter(\n            X, window_length=self.window_length, polyorder=self.polyorder, axis=1\n        )\n</code></pre>"},{"location":"api_reference/#foodspec.preprocess.normalization.AreaNormalizer","title":"AreaNormalizer","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Normalize spectra to unit area.</p> Source code in <code>src/foodspec/preprocess/normalization.py</code> <pre><code>class AreaNormalizer(BaseEstimator, TransformerMixin):\n    \"\"\"Normalize spectra to unit area.\"\"\"\n\n    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; \"AreaNormalizer\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n\n        area = np.trapezoid(X, axis=1, dx=1.0).reshape(-1, 1)\n        area = np.maximum(np.abs(area), np.finfo(float).eps)\n        return X / area\n</code></pre>"},{"location":"api_reference/#foodspec.preprocess.normalization.InternalPeakNormalizer","title":"InternalPeakNormalizer","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Normalize spectra using an internal reference peak window.</p> Source code in <code>src/foodspec/preprocess/normalization.py</code> <pre><code>class InternalPeakNormalizer(BaseEstimator, TransformerMixin):\n    \"\"\"Normalize spectra using an internal reference peak window.\"\"\"\n\n    def __init__(self, target_wavenumber: float, window: float = 10.0):\n        self.target_wavenumber = target_wavenumber\n        self.window = window\n\n    def fit(\n        self,\n        X: np.ndarray,\n        y: Optional[np.ndarray] = None,\n        wavenumbers: Optional[np.ndarray] = None,\n    ) -&gt; \"InternalPeakNormalizer\":\n        return self\n\n    def transform(\n        self, X: np.ndarray, wavenumbers: Optional[np.ndarray] = None\n    ) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        if wavenumbers is None:\n            raise ValueError(\"wavenumbers array is required for InternalPeakNormalizer.\")\n\n        wavenumbers = np.asarray(wavenumbers, dtype=float)\n        if wavenumbers.ndim != 1:\n            raise ValueError(\"wavenumbers must be 1D.\")\n        if wavenumbers.shape[0] != X.shape[1]:\n            raise ValueError(\"wavenumbers length must match number of columns in X.\")\n        if self.window &lt;= 0:\n            raise ValueError(\"window must be positive.\")\n\n        half = self.window / 2.0\n        mask = (wavenumbers &gt;= self.target_wavenumber - half) &amp; (\n            wavenumbers &lt;= self.target_wavenumber + half\n        )\n        if not np.any(mask):\n            raise ValueError(\"No points found within the specified window.\")\n\n        ref = np.mean(X[:, mask], axis=1, keepdims=True)\n        ref = np.maximum(np.abs(ref), np.finfo(float).eps)\n        return X / ref\n</code></pre>"},{"location":"api_reference/#foodspec.preprocess.normalization.MSCNormalizer","title":"MSCNormalizer","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Multiplicative scatter correction (MSC) using a reference mean spectrum.</p> Source code in <code>src/foodspec/preprocess/normalization.py</code> <pre><code>class MSCNormalizer(BaseEstimator, TransformerMixin):\n    \"\"\"Multiplicative scatter correction (MSC) using a reference mean spectrum.\"\"\"\n\n    def __init__(self):\n        self.reference_: Optional[np.ndarray] = None\n\n    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; \"MSCNormalizer\":\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        self.reference_ = X.mean(axis=0)\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        if self.reference_ is None:\n            raise RuntimeError(\"MSCNormalizer has not been fitted. Call fit() first.\")\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        ref = self.reference_\n        ref_mean = ref.mean()\n        ref_centered = ref - ref_mean\n        denom = np.dot(ref_centered, ref_centered)\n        denom = np.maximum(denom, np.finfo(float).eps)\n\n        corrected = np.empty_like(X, dtype=float)\n        for i, x in enumerate(X):\n            x_mean = x.mean()\n            b = np.dot(ref_centered, x - x_mean) / denom\n            b = np.maximum(b, np.finfo(float).eps)\n            a = x_mean - b * ref_mean\n            corrected[i, :] = (x - a) / b\n        return corrected\n</code></pre>"},{"location":"api_reference/#foodspec.preprocess.normalization.SNVNormalizer","title":"SNVNormalizer","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Standard normal variate (SNV) normalization per spectrum.</p> Source code in <code>src/foodspec/preprocess/normalization.py</code> <pre><code>class SNVNormalizer(BaseEstimator, TransformerMixin):\n    \"\"\"Standard normal variate (SNV) normalization per spectrum.\"\"\"\n\n    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; \"SNVNormalizer\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        mean = X.mean(axis=1, keepdims=True)\n        std = X.std(axis=1, keepdims=True)\n        std = np.maximum(std, np.finfo(float).eps)\n        return (X - mean) / std\n</code></pre>"},{"location":"api_reference/#foodspec.preprocess.normalization.VectorNormalizer","title":"VectorNormalizer","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Vector normalization across spectral axis.</p> Source code in <code>src/foodspec/preprocess/normalization.py</code> <pre><code>class VectorNormalizer(BaseEstimator, TransformerMixin):\n    \"\"\"Vector normalization across spectral axis.\"\"\"\n\n    def __init__(self, norm: Literal[\"l1\", \"l2\", \"max\"] = \"l2\"):\n        self.norm = norm\n\n    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; \"VectorNormalizer\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        if self.norm not in {\"l1\", \"l2\", \"max\"}:\n            raise ValueError(\"norm must be one of {'l1', 'l2', 'max'}.\")\n\n        if self.norm == \"l1\":\n            denom = np.sum(np.abs(X), axis=1, keepdims=True)\n        elif self.norm == \"l2\":\n            denom = np.linalg.norm(X, ord=2, axis=1, keepdims=True)\n        else:\n            denom = np.max(np.abs(X), axis=1, keepdims=True)\n\n        denom = np.maximum(denom, np.finfo(float).eps)\n        return X / denom\n</code></pre>"},{"location":"api_reference/#foodspec.preprocess.cropping.RangeCropper","title":"RangeCropper","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Crop spectra to a specified wavenumber range.</p> Source code in <code>src/foodspec/preprocess/cropping.py</code> <pre><code>class RangeCropper(BaseEstimator):\n    \"\"\"Crop spectra to a specified wavenumber range.\"\"\"\n\n    def __init__(self, min_wn: float, max_wn: float):\n        if min_wn &gt;= max_wn:\n            raise ValueError(\"min_wn must be less than max_wn.\")\n        self.min_wn = min_wn\n        self.max_wn = max_wn\n\n    def fit(self, X: np.ndarray, y=None, wavenumbers: np.ndarray | None = None):\n        return self\n\n    def transform(\n        self, X: np.ndarray, wavenumbers: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        X = np.asarray(X, dtype=float)\n        wavenumbers = np.asarray(wavenumbers, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        if wavenumbers.ndim != 1 or wavenumbers.shape[0] != X.shape[1]:\n            raise ValueError(\"wavenumbers must be 1D and match columns of X.\")\n\n        mask = (wavenumbers &gt;= self.min_wn) &amp; (wavenumbers &lt;= self.max_wn)\n        if not np.any(mask):\n            raise ValueError(\"No wavenumbers within the specified range.\")\n        return X[:, mask], wavenumbers[mask]\n</code></pre>"},{"location":"api_reference/#foodspec.preprocess.cropping.crop_spectrum_set","title":"crop_spectrum_set","text":"<pre><code>crop_spectrum_set(spectra, min_wn, max_wn)\n</code></pre> <p>Crop a FoodSpectrumSet to a wavenumber range.</p> Source code in <code>src/foodspec/preprocess/cropping.py</code> <pre><code>def crop_spectrum_set(spectra: FoodSpectrumSet, min_wn: float, max_wn: float) -&gt; FoodSpectrumSet:\n    \"\"\"Crop a FoodSpectrumSet to a wavenumber range.\"\"\"\n\n    cropper = RangeCropper(min_wn=min_wn, max_wn=max_wn)\n    x_cropped, wn_cropped = cropper.transform(spectra.x, spectra.wavenumbers)\n    return FoodSpectrumSet(\n        x=x_cropped,\n        wavenumbers=wn_cropped,\n        metadata=spectra.metadata.copy(),\n        modality=spectra.modality,\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.preprocess.ftir.AtmosphericCorrector","title":"AtmosphericCorrector","text":"<p>               Bases: <code>WavenumberAwareMixin</code>, <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Atmospheric correction using synthetic or user-provided water/CO2 bases.</p> <p>This is a simplified approach (not vendor-grade). You may supply explicit water/co2 basis arrays (shape n_points x n_bases); otherwise, broad Gaussian bases are generated at typical water/CO2 positions.</p> Source code in <code>src/foodspec/preprocess/ftir.py</code> <pre><code>class AtmosphericCorrector(WavenumberAwareMixin, BaseEstimator, TransformerMixin):\n    \"\"\"Atmospheric correction using synthetic or user-provided water/CO2 bases.\n\n    This is a simplified approach (not vendor-grade). You may supply explicit\n    water/co2 basis arrays (shape n_points x n_bases); otherwise, broad Gaussian\n    bases are generated at typical water/CO2 positions.\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha_water: float = 1.0,\n        alpha_co2: float = 1.0,\n        water_center: float = 1900.0,\n        co2_center: float = 2350.0,\n        width: float = 30.0,\n        water_basis: Optional[np.ndarray] = None,\n        co2_basis: Optional[np.ndarray] = None,\n        normalize_bases: bool = True,\n    ):\n        self.alpha_water = alpha_water\n        self.alpha_co2 = alpha_co2\n        self.water_center = water_center\n        self.co2_center = co2_center\n        self.width = width\n        self.water_basis = water_basis\n        self.co2_basis = co2_basis\n        self.normalize_bases = normalize_bases\n\n    def fit(self, X, y=None, wavenumbers: Optional[np.ndarray] = None):\n        if wavenumbers is not None:\n            self.set_wavenumbers(wavenumbers)\n        self._assert_wavenumbers_set()\n        bases = self._build_bases(self.wavenumbers_)\n        if self.normalize_bases:\n            norms = np.linalg.norm(bases, axis=0, keepdims=True)\n            norms = np.maximum(norms, np.finfo(float).eps)\n            bases = bases / norms\n        self._bases = bases\n        return self\n\n    def transform(self, X):\n        self._assert_wavenumbers_set()\n        X = np.asarray(X, dtype=float)\n        bases = self._bases\n        BtB = bases.T @ bases\n        pseudo = np.linalg.pinv(BtB) @ bases.T\n        corrected = []\n        for spectrum in X:\n            coeffs = pseudo @ spectrum\n            resid = spectrum - bases @ coeffs\n            corrected.append(resid)\n        return np.vstack(corrected)\n\n    def _build_bases(self, wn: np.ndarray) -&gt; np.ndarray:\n        if self.water_basis is not None or self.co2_basis is not None:\n            parts = []\n            if self.water_basis is not None:\n                parts.append(np.asarray(self.water_basis, dtype=float))\n            if self.co2_basis is not None:\n                parts.append(np.asarray(self.co2_basis, dtype=float))\n            return np.column_stack(parts)\n        water = self.alpha_water * np.exp(-0.5 * ((wn - self.water_center) / self.width) ** 2)\n        co2 = self.alpha_co2 * np.exp(-0.5 * ((wn - self.co2_center) / self.width) ** 2)\n        return np.vstack([water, co2]).T\n</code></pre>"},{"location":"api_reference/#foodspec.preprocess.ftir.SimpleATRCorrector","title":"SimpleATRCorrector","text":"<p>               Bases: <code>WavenumberAwareMixin</code>, <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Approximate ATR correction using heuristic scaling.</p> Source code in <code>src/foodspec/preprocess/ftir.py</code> <pre><code>class SimpleATRCorrector(WavenumberAwareMixin, BaseEstimator, TransformerMixin):\n    \"\"\"Approximate ATR correction using heuristic scaling.\"\"\"\n\n    def __init__(\n        self,\n        refractive_index_sample: float = 1.5,\n        refractive_index_crystal: float = 2.4,\n        angle_of_incidence: float = 45.0,\n        wavenumber_scale: str = \"linear\",\n    ):\n        self.refractive_index_sample = refractive_index_sample\n        self.refractive_index_crystal = refractive_index_crystal\n        self.angle_of_incidence = angle_of_incidence\n        self.wavenumber_scale = wavenumber_scale\n\n    def fit(self, X, y=None, wavenumbers: Optional[np.ndarray] = None):\n        if wavenumbers is not None:\n            self.set_wavenumbers(wavenumbers)\n        self._assert_wavenumbers_set()\n        self._scale = self._compute_scale(self.wavenumbers_)\n        return self\n\n    def transform(self, X):\n        self._assert_wavenumbers_set()\n        X = np.asarray(X, dtype=float)\n        return X * self._scale\n\n    def _compute_scale(self, wn: np.ndarray) -&gt; np.ndarray:\n        ratio = self.refractive_index_sample / self.refractive_index_crystal\n        angle_factor = 1.0 + 0.01 * (self.angle_of_incidence - 45.0)\n        scale = 1.0 / (1.0 + angle_factor * ratio * (wn / wn.max()))\n        return scale\n</code></pre>"},{"location":"api_reference/#foodspec.preprocess.raman.CosmicRayRemover","title":"CosmicRayRemover","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Basic cosmic ray spike removal via thresholding.</p> <p>Detects spikes as points exceeding local median by <code>sigma_thresh</code> times the local MAD and replaces them by linear interpolation of neighbors.</p> Source code in <code>src/foodspec/preprocess/raman.py</code> <pre><code>class CosmicRayRemover(BaseEstimator, TransformerMixin):\n    \"\"\"Basic cosmic ray spike removal via thresholding.\n\n    Detects spikes as points exceeding local median by `sigma_thresh` times\n    the local MAD and replaces them by linear interpolation of neighbors.\n    \"\"\"\n\n    def __init__(self, window: int = 5, sigma_thresh: float = 8.0):\n        self.window = window\n        self.sigma_thresh = sigma_thresh\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D with shape (n_samples, n_points).\")\n        window = max(3, int(self.window))\n        corrected = []\n        for spectrum in X:\n            corrected.append(self._despike(spectrum, window))\n        return np.vstack(corrected)\n\n    def _despike(self, y: np.ndarray, window: int) -&gt; np.ndarray:\n        half = window // 2\n        y_clean = y.copy()\n        for i in range(len(y)):\n            start = max(0, i - half)\n            end = min(len(y), i + half + 1)\n            local = y[start:end]\n            median = np.median(local)\n            mad = np.median(np.abs(local - median)) + 1e-8\n            if abs(y[i] - median) &gt; self.sigma_thresh * mad:\n                # interpolate neighbors if available\n                left = y_clean[i - 1] if i &gt; 0 else median\n                right = y_clean[i + 1] if i + 1 &lt; len(y) else median\n                y_clean[i] = 0.5 * (left + right)\n        return y_clean\n</code></pre>"},{"location":"api_reference/#foodspec.preprocess.derivatives.DerivativeTransformer","title":"DerivativeTransformer","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Savitzky-Golay derivative transformer.</p> Source code in <code>src/foodspec/preprocess/derivatives.py</code> <pre><code>class DerivativeTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Savitzky-Golay derivative transformer.\"\"\"\n\n    def __init__(\n        self,\n        order: Literal[1, 2] = 1,\n        window_length: int = 7,\n        polyorder: int = 3,\n    ):\n        self.order = order\n        self.window_length = window_length\n        self.polyorder = polyorder\n\n    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; \"DerivativeTransformer\":\n        return self\n\n    def transform(self, X: np.ndarray) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D array of shape (n_samples, n_wavenumbers).\")\n        if self.order not in {1, 2}:\n            raise ValueError(\"order must be 1 or 2.\")\n        if self.window_length &lt;= 0 or self.window_length % 2 == 0:\n            raise ValueError(\"window_length must be a positive odd integer.\")\n        if self.polyorder &gt;= self.window_length:\n            raise ValueError(\"polyorder must be less than window_length.\")\n        if self.window_length &gt; X.shape[1]:\n            raise ValueError(\"window_length cannot exceed number of wavenumbers.\")\n\n        return savgol_filter(\n            X,\n            window_length=self.window_length,\n            polyorder=self.polyorder,\n            deriv=self.order,\n            axis=1,\n        )\n</code></pre>"},{"location":"api_reference/#features-and-ratios","title":"Features and ratios","text":"<p>Peak detection and feature extraction utilities.</p> <p>Band integration utilities.</p> <p>Ratio feature utilities.</p> <p>Spectral similarity utilities.</p>"},{"location":"api_reference/#foodspec.features.peaks.PeakFeatureExtractor","title":"PeakFeatureExtractor","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Extract peak height and area features around expected peaks.</p> Source code in <code>src/foodspec/features/peaks.py</code> <pre><code>class PeakFeatureExtractor(BaseEstimator, TransformerMixin):\n    \"\"\"Extract peak height and area features around expected peaks.\"\"\"\n\n    def __init__(\n        self,\n        expected_peaks: Sequence[float],\n        tolerance: float = 5.0,\n        features: Sequence[str] = (\"height\", \"area\"),\n    ):\n        self.expected_peaks = list(expected_peaks)\n        self.tolerance = tolerance\n        self.features = tuple(features)\n        self.feature_names_: list[str] = []\n\n    def fit(\n        self, X: np.ndarray, y: Optional[np.ndarray] = None, wavenumbers: Optional[np.ndarray] = None\n    ) -&gt; \"PeakFeatureExtractor\":\n        self._build_feature_names()\n        return self\n\n    def transform(\n        self, X: np.ndarray, wavenumbers: Optional[np.ndarray] = None\n    ) -&gt; np.ndarray:\n        X = np.asarray(X, dtype=float)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D with shape (n_samples, n_wavenumbers).\")\n        if wavenumbers is None:\n            raise ValueError(\"wavenumbers is required to extract peak features.\")\n        wavenumbers = np.asarray(wavenumbers, dtype=float)\n        if wavenumbers.shape[0] != X.shape[1]:\n            raise ValueError(\"wavenumbers length must match X columns.\")\n        if self.tolerance &lt;= 0:\n            raise ValueError(\"tolerance must be positive.\")\n\n        self._build_feature_names()\n        feats = np.zeros((X.shape[0], len(self.feature_names_)), dtype=float)\n        for i, spectrum in enumerate(X):\n            col = 0\n            for peak_center in self.expected_peaks:\n                mask = (wavenumbers &gt;= peak_center - self.tolerance) &amp; (\n                    wavenumbers &lt;= peak_center + self.tolerance\n                )\n                if not np.any(mask):\n                    peak_idx = np.argmax(np.full_like(wavenumbers, -np.inf))\n                    area = np.nan\n                    height = np.nan\n                else:\n                    local_w = wavenumbers[mask]\n                    local_y = spectrum[mask]\n                    local_max_idx = np.argmax(local_y)\n                    height = local_y[local_max_idx]\n                    peak_idx = np.where(mask)[0][local_max_idx]\n                    area = np.trapezoid(local_y, x=local_w)\n\n                if \"height\" in self.features:\n                    feats[i, col] = height\n                    col += 1\n                if \"area\" in self.features:\n                    feats[i, col] = area\n                    col += 1\n\n        return feats\n\n    def get_feature_names_out(self, input_features=None):\n        self._build_feature_names()\n        return np.array(self.feature_names_, dtype=str)\n\n    def _build_feature_names(self) -&gt; None:\n        names: list[str] = []\n        for peak in self.expected_peaks:\n            if \"height\" in self.features:\n                names.append(f\"peak_{peak}_height\")\n            if \"area\" in self.features:\n                names.append(f\"peak_{peak}_area\")\n        self.feature_names_ = names\n</code></pre>"},{"location":"api_reference/#foodspec.features.peaks.detect_peaks","title":"detect_peaks","text":"<pre><code>detect_peaks(x, wavenumbers, prominence=0.0, width=None)\n</code></pre> <p>Detect peaks and return their properties.</p>"},{"location":"api_reference/#foodspec.features.peaks.detect_peaks--parameters","title":"Parameters","text":"<p>x :     1D intensity array. wavenumbers :     1D axis array aligned with <code>x</code>. prominence :     Minimum prominence passed to <code>scipy.signal.find_peaks</code>. width :     Optional width parameter for <code>find_peaks</code>.</p>"},{"location":"api_reference/#foodspec.features.peaks.detect_peaks--returns","title":"Returns","text":"<p>pandas.DataFrame     Columns: <code>peak_index</code>, <code>peak_wavenumber</code>, <code>peak_intensity</code>,     <code>prominence</code>, <code>width</code>.</p> Source code in <code>src/foodspec/features/peaks.py</code> <pre><code>def detect_peaks(\n    x: np.ndarray,\n    wavenumbers: np.ndarray,\n    prominence: float = 0.0,\n    width: Optional[float] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Detect peaks and return their properties.\n\n    Parameters\n    ----------\n    x :\n        1D intensity array.\n    wavenumbers :\n        1D axis array aligned with ``x``.\n    prominence :\n        Minimum prominence passed to ``scipy.signal.find_peaks``.\n    width :\n        Optional width parameter for ``find_peaks``.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Columns: ``peak_index``, ``peak_wavenumber``, ``peak_intensity``,\n        ``prominence``, ``width``.\n    \"\"\"\n\n    x = np.asarray(x, dtype=float)\n    wavenumbers = np.asarray(wavenumbers, dtype=float)\n    if x.ndim != 1 or wavenumbers.ndim != 1:\n        raise ValueError(\"x and wavenumbers must be 1D.\")\n    if x.shape[0] != wavenumbers.shape[0]:\n        raise ValueError(\"x and wavenumbers must have the same length.\")\n\n    peak_indices, props = find_peaks(x, prominence=prominence, width=width)\n    prominences = props.get(\"prominences\", np.full_like(peak_indices, np.nan, dtype=float))\n    widths = props.get(\"widths\", np.full_like(peak_indices, np.nan, dtype=float))\n    return pd.DataFrame(\n        {\n            \"peak_index\": peak_indices,\n            \"peak_wavenumber\": wavenumbers[peak_indices],\n            \"peak_intensity\": x[peak_indices],\n            \"prominence\": prominences,\n            \"width\": widths,\n        }\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.features.bands.integrate_bands","title":"integrate_bands","text":"<pre><code>integrate_bands(X, wavenumbers, bands)\n</code></pre> <p>Integrate intensity over specified bands.</p>"},{"location":"api_reference/#foodspec.features.bands.integrate_bands--parameters","title":"Parameters","text":"<p>X :     Array of shape (n_samples, n_wavenumbers). wavenumbers :     1D array of wavenumbers aligned with <code>X</code>. bands :     Sequence of (label, min_wn, max_wn) tuples.</p>"},{"location":"api_reference/#foodspec.features.bands.integrate_bands--returns","title":"Returns","text":"<p>pandas.DataFrame     Columns named by band labels; one row per sample.</p> Source code in <code>src/foodspec/features/bands.py</code> <pre><code>def integrate_bands(\n    X: np.ndarray,\n    wavenumbers: np.ndarray,\n    bands: Sequence[Tuple[str, float, float]],\n) -&gt; pd.DataFrame:\n    \"\"\"Integrate intensity over specified bands.\n\n    Parameters\n    ----------\n    X :\n        Array of shape (n_samples, n_wavenumbers).\n    wavenumbers :\n        1D array of wavenumbers aligned with ``X``.\n    bands :\n        Sequence of (label, min_wn, max_wn) tuples.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Columns named by band labels; one row per sample.\n    \"\"\"\n\n    X = np.asarray(X, dtype=float)\n    wavenumbers = np.asarray(wavenumbers, dtype=float)\n    if X.ndim != 2:\n        raise ValueError(\"X must be 2D.\")\n    if wavenumbers.ndim != 1 or wavenumbers.shape[0] != X.shape[1]:\n        raise ValueError(\"wavenumbers must be 1D and match number of columns in X.\")\n\n    data = {}\n    for label, min_wn, max_wn in bands:\n        if min_wn &gt;= max_wn:\n            raise ValueError(f\"Band {label} has invalid range.\")\n        mask = (wavenumbers &gt;= min_wn) &amp; (wavenumbers &lt;= max_wn)\n        if not np.any(mask):\n            data[label] = np.full(X.shape[0], np.nan)\n            continue\n        data[label] = np.trapezoid(X[:, mask], x=wavenumbers[mask], axis=1)\n\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"api_reference/#foodspec.features.ratios.RatioFeatureGenerator","title":"RatioFeatureGenerator","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Generate ratio features for use in pipelines.</p> Source code in <code>src/foodspec/features/ratios.py</code> <pre><code>class RatioFeatureGenerator(BaseEstimator, TransformerMixin):\n    \"\"\"Generate ratio features for use in pipelines.\"\"\"\n\n    def __init__(self, ratio_def: Dict[str, Tuple[str, str]]):\n        self.ratio_def = ratio_def\n\n    def fit(self, X: pd.DataFrame, y: Optional[np.ndarray] = None) -&gt; \"RatioFeatureGenerator\":\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"RatioFeatureGenerator expects a pandas DataFrame.\")\n        return compute_ratios(X, self.ratio_def)\n</code></pre>"},{"location":"api_reference/#foodspec.features.ratios.compute_ratios","title":"compute_ratios","text":"<pre><code>compute_ratios(df, ratio_def)\n</code></pre> <p>Compute ratios of specified columns.</p>"},{"location":"api_reference/#foodspec.features.ratios.compute_ratios--parameters","title":"Parameters","text":"<p>df :     DataFrame containing numerator and denominator columns. ratio_def :     Mapping from new column name to (numerator_col, denominator_col).</p>"},{"location":"api_reference/#foodspec.features.ratios.compute_ratios--returns","title":"Returns","text":"<p>pandas.DataFrame     Original DataFrame with additional ratio columns.</p> Source code in <code>src/foodspec/features/ratios.py</code> <pre><code>def compute_ratios(\n    df: pd.DataFrame, ratio_def: Dict[str, Tuple[str, str]]\n) -&gt; pd.DataFrame:\n    \"\"\"Compute ratios of specified columns.\n\n    Parameters\n    ----------\n    df :\n        DataFrame containing numerator and denominator columns.\n    ratio_def :\n        Mapping from new column name to (numerator_col, denominator_col).\n\n    Returns\n    -------\n    pandas.DataFrame\n        Original DataFrame with additional ratio columns.\n    \"\"\"\n\n    result = df.copy()\n    for name, (num_col, denom_col) in ratio_def.items():\n        if num_col not in result.columns or denom_col not in result.columns:\n            raise ValueError(f\"Columns {num_col} and {denom_col} must exist in DataFrame.\")\n        denom = result[denom_col].to_numpy()\n        num = result[num_col].to_numpy()\n        ratio = np.divide(num, denom, out=np.full_like(num, np.nan, dtype=float), where=denom != 0)\n        result[name] = ratio\n    return result\n</code></pre>"},{"location":"api_reference/#foodspec.features.fingerprint.correlation_similarity_matrix","title":"correlation_similarity_matrix","text":"<pre><code>correlation_similarity_matrix(X_ref, X_query)\n</code></pre> <p>Compute Pearson correlation similarity matrix.</p> Source code in <code>src/foodspec/features/fingerprint.py</code> <pre><code>def correlation_similarity_matrix(X_ref: np.ndarray, X_query: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute Pearson correlation similarity matrix.\"\"\"\n\n    X_ref = np.asarray(X_ref, dtype=float)\n    X_query = np.asarray(X_query, dtype=float)\n    X_ref_centered = X_ref - X_ref.mean(axis=1, keepdims=True)\n    X_query_centered = X_query - X_query.mean(axis=1, keepdims=True)\n    ref_norm = np.linalg.norm(X_ref_centered, axis=1, keepdims=True)\n    query_norm = np.linalg.norm(X_query_centered, axis=1, keepdims=True)\n    ref_norm = np.maximum(ref_norm, np.finfo(float).eps)\n    query_norm = np.maximum(query_norm, np.finfo(float).eps)\n    sims = (X_ref_centered @ X_query_centered.T) / (ref_norm * query_norm.T)\n    return sims\n</code></pre>"},{"location":"api_reference/#foodspec.features.fingerprint.cosine_similarity_matrix","title":"cosine_similarity_matrix","text":"<pre><code>cosine_similarity_matrix(X_ref, X_query)\n</code></pre> <p>Compute cosine similarity matrix between reference and query spectra.</p> Source code in <code>src/foodspec/features/fingerprint.py</code> <pre><code>def cosine_similarity_matrix(X_ref: np.ndarray, X_query: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute cosine similarity matrix between reference and query spectra.\"\"\"\n\n    X_ref = np.asarray(X_ref, dtype=float)\n    X_query = np.asarray(X_query, dtype=float)\n    ref_norm = np.linalg.norm(X_ref, axis=1, keepdims=True)\n    query_norm = np.linalg.norm(X_query, axis=1, keepdims=True)\n    ref_norm = np.maximum(ref_norm, np.finfo(float).eps)\n    query_norm = np.maximum(query_norm, np.finfo(float).eps)\n    sims = (X_ref @ X_query.T) / (ref_norm * query_norm.T)\n    return sims\n</code></pre>"},{"location":"api_reference/#chemometrics-and-models","title":"Chemometrics and models","text":"<p>Principal Component Analysis utilities.</p> <p>Model factories for chemometrics workflows.</p> <p>Validation utilities for chemometrics models.</p> <p>Mixture analysis utilities (NNLS and simplified MCR-ALS).</p> <p>Optional deep learning models for spectral classification.</p> <p>Provides a minimal 1D CNN classifier with a scikit-learn-like API. This module relies on TensorFlow/Keras or PyTorch; it is only intended for advanced users and examples. Dependencies are optional and must be installed separately.</p>"},{"location":"api_reference/#foodspec.chemometrics.pca.PCAResult","title":"PCAResult  <code>dataclass</code>","text":"<p>Container for PCA outputs.</p> Source code in <code>src/foodspec/chemometrics/pca.py</code> <pre><code>@dataclass\nclass PCAResult:\n    \"\"\"Container for PCA outputs.\"\"\"\n\n    scores: np.ndarray\n    loadings: np.ndarray\n    explained_variance: np.ndarray\n    explained_variance_ratio: np.ndarray\n    mean_: np.ndarray\n</code></pre>"},{"location":"api_reference/#foodspec.chemometrics.pca.run_pca","title":"run_pca","text":"<pre><code>run_pca(X, n_components=2)\n</code></pre> <p>Run PCA on data matrix.</p>"},{"location":"api_reference/#foodspec.chemometrics.pca.run_pca--parameters","title":"Parameters","text":"<p>X :     Array of shape (n_samples, n_features). n_components :     Number of components to compute.</p>"},{"location":"api_reference/#foodspec.chemometrics.pca.run_pca--returns","title":"Returns","text":"<p>tuple     Fitted PCA estimator and PCAResult container.</p> Source code in <code>src/foodspec/chemometrics/pca.py</code> <pre><code>def run_pca(X: np.ndarray, n_components: int = 2) -&gt; Tuple[PCA, PCAResult]:\n    \"\"\"Run PCA on data matrix.\n\n    Parameters\n    ----------\n    X :\n        Array of shape (n_samples, n_features).\n    n_components :\n        Number of components to compute.\n\n    Returns\n    -------\n    tuple\n        Fitted PCA estimator and PCAResult container.\n    \"\"\"\n\n    if n_components &lt;= 0:\n        raise ValueError(\"n_components must be positive.\")\n\n    pca = PCA(n_components=n_components)\n    scores = pca.fit_transform(X)\n    result = PCAResult(\n        scores=scores,\n        loadings=pca.components_.T,\n        explained_variance=pca.explained_variance_,\n        explained_variance_ratio=pca.explained_variance_ratio_,\n        mean_=pca.mean_,\n    )\n    return pca, result\n</code></pre>"},{"location":"api_reference/#foodspec.chemometrics.models.make_classifier","title":"make_classifier","text":"<pre><code>make_classifier(model_name, **kwargs)\n</code></pre> <p>Factory for common classifiers.</p>"},{"location":"api_reference/#foodspec.chemometrics.models.make_classifier--parameters","title":"Parameters","text":"<p>model_name :     One of: <code>logreg</code>, <code>svm_linear</code>, <code>svm_rbf</code>, <code>rf</code>, <code>xgb</code>, <code>lgbm</code>, <code>knn</code>. kwargs :     Additional parameters forwarded to the model constructor.</p>"},{"location":"api_reference/#foodspec.chemometrics.models.make_classifier--returns","title":"Returns","text":"<p>BaseEstimator     Instantiated classifier.</p> Source code in <code>src/foodspec/chemometrics/models.py</code> <pre><code>def make_classifier(model_name: str, **kwargs: Any) -&gt; BaseEstimator:\n    \"\"\"Factory for common classifiers.\n\n    Parameters\n    ----------\n    model_name :\n        One of: ``logreg``, ``svm_linear``, ``svm_rbf``, ``rf``, ``xgb``, ``lgbm``, ``knn``.\n    kwargs :\n        Additional parameters forwarded to the model constructor.\n\n    Returns\n    -------\n    BaseEstimator\n        Instantiated classifier.\n    \"\"\"\n\n    name = model_name.lower()\n    if name == \"logreg\":\n        return LogisticRegression(max_iter=1000, **kwargs)\n    if name == \"svm_linear\":\n        return SVC(kernel=\"linear\", probability=True, **kwargs)\n    if name == \"svm_rbf\":\n        return SVC(kernel=\"rbf\", probability=True, **kwargs)\n    if name == \"rf\":\n        return RandomForestClassifier(**kwargs)\n    if name == \"knn\":\n        return KNeighborsClassifier(**kwargs)\n    if name == \"xgb\":\n        try:\n            from xgboost import XGBClassifier  # type: ignore\n        except ModuleNotFoundError as exc:\n            raise ImportError(\"xgboost is required for model_name='xgb'.\") from exc\n        return XGBClassifier(**kwargs)\n    if name == \"lgbm\":\n        try:\n            from lightgbm import LGBMClassifier  # type: ignore\n        except ModuleNotFoundError as exc:\n            raise ImportError(\"lightgbm is required for model_name='lgbm'.\") from exc\n        return LGBMClassifier(**kwargs)\n\n    raise ValueError(\n        \"model_name must be one of {'logreg','svm_linear','svm_rbf','rf','xgb','lgbm','knn'}\"\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.chemometrics.models.make_pls_da","title":"make_pls_da","text":"<pre><code>make_pls_da(n_components=10)\n</code></pre> <p>Create a PLS-DA (PLS + Logistic Regression) pipeline.</p> Source code in <code>src/foodspec/chemometrics/models.py</code> <pre><code>def make_pls_da(n_components: int = 10) -&gt; Pipeline:\n    \"\"\"Create a PLS-DA (PLS + Logistic Regression) pipeline.\"\"\"\n\n    return Pipeline(\n        [\n            (\"scaler\", StandardScaler()),\n            (\"pls_proj\", _PLSProjector(n_components=n_components)),\n            (\"clf\", LogisticRegression(max_iter=1000)),\n        ]\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.chemometrics.models.make_pls_regression","title":"make_pls_regression","text":"<pre><code>make_pls_regression(n_components=10)\n</code></pre> <p>Create a PLS regression pipeline with scaling.</p> Source code in <code>src/foodspec/chemometrics/models.py</code> <pre><code>def make_pls_regression(n_components: int = 10) -&gt; Pipeline:\n    \"\"\"Create a PLS regression pipeline with scaling.\"\"\"\n\n    return Pipeline(\n        [\n            (\"scaler\", StandardScaler()),\n            (\"pls\", PLSRegression(n_components=n_components)),\n        ]\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.chemometrics.validation.compute_classification_metrics","title":"compute_classification_metrics","text":"<pre><code>compute_classification_metrics(\n    y_true, y_pred, y_proba=None\n)\n</code></pre> <p>Compute common classification metrics.</p> Source code in <code>src/foodspec/chemometrics/validation.py</code> <pre><code>def compute_classification_metrics(\n    y_true: np.ndarray, y_pred: np.ndarray, y_proba: Optional[np.ndarray] = None\n) -&gt; pd.DataFrame:\n    \"\"\"Compute common classification metrics.\"\"\"\n\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    labels = np.unique(np.concatenate([y_true, y_pred]))\n    average = \"binary\" if len(labels) == 2 else \"weighted\"\n    pos_label = labels[0] if len(labels) == 2 else None\n\n    results: dict[str, Any] = {\n        \"accuracy\": metrics.accuracy_score(y_true, y_pred),\n        \"precision\": metrics.precision_score(\n            y_true, y_pred, zero_division=0, average=average, pos_label=pos_label\n        ),\n        \"recall\": metrics.recall_score(\n            y_true, y_pred, zero_division=0, average=average, pos_label=pos_label\n        ),\n        \"f1\": metrics.f1_score(\n            y_true, y_pred, zero_division=0, average=average, pos_label=pos_label\n        ),\n    }\n    if y_proba is not None and len(labels) == 2:\n        y_proba = np.asarray(y_proba)\n        if y_proba.ndim == 2 and y_proba.shape[1] &gt; 1:\n            pos_scores = y_proba[:, 1]\n        else:\n            pos_scores = y_proba\n        results[\"roc_auc\"] = metrics.roc_auc_score(y_true, pos_scores)\n        results[\"average_precision\"] = metrics.average_precision_score(y_true, pos_scores)\n\n    return pd.DataFrame([results])\n</code></pre>"},{"location":"api_reference/#foodspec.chemometrics.validation.compute_regression_metrics","title":"compute_regression_metrics","text":"<pre><code>compute_regression_metrics(y_true, y_pred)\n</code></pre> <p>Compute regression metrics.</p> Source code in <code>src/foodspec/chemometrics/validation.py</code> <pre><code>def compute_regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -&gt; pd.Series:\n    \"\"\"Compute regression metrics.\"\"\"\n\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    rmse = np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n    mae = metrics.mean_absolute_error(y_true, y_pred)\n    r2 = metrics.r2_score(y_true, y_pred)\n    return pd.Series({\"rmse\": rmse, \"mae\": mae, \"r2\": r2})\n</code></pre>"},{"location":"api_reference/#foodspec.chemometrics.validation.cross_validate_pipeline","title":"cross_validate_pipeline","text":"<pre><code>cross_validate_pipeline(\n    pipeline, X, y, cv_splits=5, scoring=\"accuracy\"\n)\n</code></pre> <p>Cross-validate a pipeline and return fold scores plus summary.</p> Source code in <code>src/foodspec/chemometrics/validation.py</code> <pre><code>def cross_validate_pipeline(\n    pipeline,\n    X: np.ndarray,\n    y: np.ndarray,\n    cv_splits: int = 5,\n    scoring: str = \"accuracy\",\n) -&gt; pd.DataFrame:\n    \"\"\"Cross-validate a pipeline and return fold scores plus summary.\"\"\"\n\n    cv_results = cross_validate(\n        pipeline,\n        X,\n        y,\n        cv=cv_splits,\n        scoring=scoring,\n        return_train_score=False,\n    )\n    scores = cv_results[\"test_score\"]\n    rows = [{\"fold\": i + 1, \"score\": s} for i, s in enumerate(scores)]\n    rows.append({\"fold\": \"mean\", \"score\": np.mean(scores)})\n    rows.append({\"fold\": \"std\", \"score\": np.std(scores)})\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"api_reference/#foodspec.chemometrics.validation.permutation_test_score_wrapper","title":"permutation_test_score_wrapper","text":"<pre><code>permutation_test_score_wrapper(\n    estimator,\n    X,\n    y,\n    scoring=\"accuracy\",\n    n_permutations=100,\n    random_state=None,\n)\n</code></pre> <p>Wrapper around sklearn's permutation_test_score.</p> Source code in <code>src/foodspec/chemometrics/validation.py</code> <pre><code>def permutation_test_score_wrapper(\n    estimator,\n    X: np.ndarray,\n    y: np.ndarray,\n    scoring: str = \"accuracy\",\n    n_permutations: int = 100,\n    random_state: Optional[int] = None,\n):\n    \"\"\"Wrapper around sklearn's permutation_test_score.\"\"\"\n\n    score, perm_scores, pvalue = permutation_test_score(\n        estimator,\n        X,\n        y,\n        scoring=scoring,\n        n_permutations=n_permutations,\n        random_state=random_state,\n    )\n    return score, perm_scores, pvalue\n</code></pre>"},{"location":"api_reference/#foodspec.chemometrics.mixture.mcr_als","title":"mcr_als","text":"<pre><code>mcr_als(\n    X,\n    n_components,\n    max_iter=100,\n    tol=1e-06,\n    random_state=None,\n)\n</code></pre> <p>Perform a simplified MCR-ALS decomposition with non-negativity clipping.</p>"},{"location":"api_reference/#foodspec.chemometrics.mixture.mcr_als--parameters","title":"Parameters","text":"<p>X:     Data matrix of shape (n_samples, n_points). n_components:     Number of components to estimate. max_iter:     Maximum number of ALS iterations. tol:     Convergence tolerance on reconstruction error. random_state:     Optional seed for reproducible initialization.</p>"},{"location":"api_reference/#foodspec.chemometrics.mixture.mcr_als--returns","title":"Returns","text":"<p>C:     Concentration profiles (n_samples, n_components). S:     Spectral profiles (n_points, n_components).</p> Source code in <code>src/foodspec/chemometrics/mixture.py</code> <pre><code>def mcr_als(\n    X: np.ndarray,\n    n_components: int,\n    max_iter: int = 100,\n    tol: float = 1e-6,\n    random_state: Optional[int] = None,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Perform a simplified MCR-ALS decomposition with non-negativity clipping.\n\n    Parameters\n    ----------\n    X:\n        Data matrix of shape (n_samples, n_points).\n    n_components:\n        Number of components to estimate.\n    max_iter:\n        Maximum number of ALS iterations.\n    tol:\n        Convergence tolerance on reconstruction error.\n    random_state:\n        Optional seed for reproducible initialization.\n\n    Returns\n    -------\n    C:\n        Concentration profiles (n_samples, n_components).\n    S:\n        Spectral profiles (n_points, n_components).\n    \"\"\"\n\n    rng = np.random.default_rng(random_state)\n    X = np.asarray(X, dtype=float)\n    n_samples, n_points = X.shape\n    S = np.abs(rng.standard_normal(size=(n_points, n_components)))\n    C = np.abs(rng.standard_normal(size=(n_samples, n_components)))\n\n    prev_err = np.inf\n    for _ in range(max_iter):\n        # Update C\n        S_pinv = np.linalg.pinv(S)\n        C = np.maximum(0, X @ S_pinv)\n        # Update S\n        C_pinv = np.linalg.pinv(C)\n        S = np.maximum(0, (C_pinv @ X).T)\n\n        recon = C @ S.T\n        err = np.linalg.norm(X - recon)\n        if abs(prev_err - err) &lt; tol:\n            break\n        prev_err = err\n    return C, S\n</code></pre>"},{"location":"api_reference/#foodspec.chemometrics.mixture.nnls_mixture","title":"nnls_mixture","text":"<pre><code>nnls_mixture(spectrum, pure_spectra)\n</code></pre> <p>Fit a non-negative least squares mixture.</p>"},{"location":"api_reference/#foodspec.chemometrics.mixture.nnls_mixture--parameters","title":"Parameters","text":"<p>spectrum:     Array of shape (n_points,) representing the mixture spectrum. pure_spectra:     Array of shape (n_points, n_components) containing pure component spectra as columns.</p>"},{"location":"api_reference/#foodspec.chemometrics.mixture.nnls_mixture--returns","title":"Returns","text":"<p>coefficients:     Non-negative coefficients for each component (shape (n_components,)). residual_norm:     Euclidean norm of the residual.</p> Source code in <code>src/foodspec/chemometrics/mixture.py</code> <pre><code>def nnls_mixture(spectrum: np.ndarray, pure_spectra: np.ndarray) -&gt; Tuple[np.ndarray, float]:\n    \"\"\"\n    Fit a non-negative least squares mixture.\n\n    Parameters\n    ----------\n    spectrum:\n        Array of shape (n_points,) representing the mixture spectrum.\n    pure_spectra:\n        Array of shape (n_points, n_components) containing pure component spectra as columns.\n\n    Returns\n    -------\n    coefficients:\n        Non-negative coefficients for each component (shape (n_components,)).\n    residual_norm:\n        Euclidean norm of the residual.\n    \"\"\"\n\n    spectrum = np.asarray(spectrum, dtype=float).ravel()\n    pure_spectra = np.asarray(pure_spectra, dtype=float)\n    if pure_spectra.shape[0] != spectrum.shape[0]:\n        raise ValueError(\"pure_spectra rows must match spectrum length.\")\n\n    if scipy_nnls is not None:\n        coeffs, res = scipy_nnls(pure_spectra, spectrum)\n    else:  # simple non-negative least squares fallback\n        coeffs, *_ = np.linalg.lstsq(pure_spectra, spectrum, rcond=None)\n        coeffs = np.clip(coeffs, 0, None)\n        res = np.linalg.norm(spectrum - pure_spectra @ coeffs)\n    return coeffs, float(res)\n</code></pre>"},{"location":"api_reference/#foodspec.chemometrics.deep.Conv1DSpectrumClassifier","title":"Conv1DSpectrumClassifier","text":"<p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Opinionated 1D CNN classifier for spectra (optional dependency: Keras).</p>"},{"location":"api_reference/#foodspec.chemometrics.deep.Conv1DSpectrumClassifier--parameters","title":"Parameters","text":"<p>epochs : int     Number of training epochs. batch_size : int     Batch size for training. validation_split : float     Fraction of training data used for validation. random_state : int | None     Optional random seed.</p>"},{"location":"api_reference/#foodspec.chemometrics.deep.Conv1DSpectrumClassifier--notes","title":"Notes","text":"<ul> <li>Uses a fixed shallow 1D CNN architecture.</li> <li>Requires TensorFlow/Keras (<code>pip install tensorflow</code>).</li> <li>Not used in core workflows; for advanced experimentation only.</li> </ul> Source code in <code>src/foodspec/chemometrics/deep.py</code> <pre><code>class Conv1DSpectrumClassifier(BaseEstimator, ClassifierMixin):\n    \"\"\"Opinionated 1D CNN classifier for spectra (optional dependency: Keras).\n\n    Parameters\n    ----------\n    epochs : int\n        Number of training epochs.\n    batch_size : int\n        Batch size for training.\n    validation_split : float\n        Fraction of training data used for validation.\n    random_state : int | None\n        Optional random seed.\n\n    Notes\n    -----\n    - Uses a fixed shallow 1D CNN architecture.\n    - Requires TensorFlow/Keras (`pip install tensorflow`).\n    - Not used in core workflows; for advanced experimentation only.\n    \"\"\"\n\n    def __init__(\n        self,\n        epochs: int = 20,\n        batch_size: int = 32,\n        validation_split: float = 0.1,\n        random_state: Optional[int] = None,\n    ):\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.validation_split = validation_split\n        self.random_state = random_state\n        self.model_ = None\n        self.classes_: Optional[np.ndarray] = None\n        if importlib.util.find_spec(\"tensorflow\") is None:\n            raise ImportError(\n                \"Conv1DSpectrumClassifier requires TensorFlow. \"\n                \"Please install the deep extra: pip install 'foodspec[deep]'.\"\n            )\n\n    def fit(self, X: np.ndarray, y: np.ndarray):\n        try:\n            import tensorflow as tf  # type: ignore\n        except ImportError as exc:  # pragma: no cover - optional dep\n            raise ImportError(\n                \"Conv1DSpectrumClassifier requires TensorFlow. \"\n                \"Please install the deep extra: pip install 'foodspec[deep]'.\"\n            ) from exc\n\n        X = np.asarray(X, dtype=np.float32)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D (n_samples, n_wavenumbers).\")\n        y = np.asarray(y)\n        self.classes_, y_idx = np.unique(y, return_inverse=True)\n\n        if self.random_state is not None:\n            try:\n                tf.keras.utils.set_random_seed(self.random_state)\n            except Exception:\n                pass\n\n        n_points = X.shape[1]\n        model = tf.keras.Sequential(\n            [\n                tf.keras.layers.Input(shape=(n_points, 1)),\n                tf.keras.layers.Conv1D(32, 5, activation=\"relu\", padding=\"same\"),\n                tf.keras.layers.MaxPool1D(pool_size=2),\n                tf.keras.layers.Conv1D(64, 5, activation=\"relu\", padding=\"same\"),\n                tf.keras.layers.MaxPool1D(pool_size=2),\n                tf.keras.layers.Flatten(),\n                tf.keras.layers.Dense(64, activation=\"relu\"),\n                tf.keras.layers.Dense(len(self.classes_), activation=\"softmax\"),\n            ]\n        )\n        model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n        model.fit(\n            X[..., None],\n            y_idx,\n            epochs=self.epochs,\n            batch_size=self.batch_size,\n            validation_split=self.validation_split,\n            verbose=0,\n        )\n        self.model_ = model\n        return self\n\n    def predict_proba(self, X: np.ndarray) -&gt; np.ndarray:\n        if self.model_ is None or self.classes_ is None:\n            raise RuntimeError(\"Model is not fitted. Call fit() first.\")\n        X = np.asarray(X, dtype=np.float32)\n        if X.ndim != 2:\n            raise ValueError(\"X must be 2D (n_samples, n_wavenumbers).\")\n        probs = self.model_.predict(X[..., None], verbose=0)\n        return probs\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        probs = self.predict_proba(X)\n        idx = np.argmax(probs, axis=1)\n        return self.classes_[idx]\n</code></pre>"},{"location":"api_reference/#apps-and-workflows","title":"Apps and workflows","text":"<p>Edible oil authentication workflow.</p> <p>Heating degradation analysis (stub).</p> <p>Quality control / novelty detection utilities.</p> <p>Dairy authentication template.</p> <p>Meat authentication template.</p> <p>Microbial detection template.</p> <p>Protocol-level benchmarks on public datasets.</p> <p>Reproduce the core analyses from the MethodsX protocol.</p> <p>Command-line interface for foodspec.</p>"},{"location":"api_reference/#foodspec.apps.oils.OilAuthResult","title":"OilAuthResult  <code>dataclass</code>","text":"<p>Results of the oil authentication workflow.</p> Source code in <code>src/foodspec/apps/oils.py</code> <pre><code>@dataclass\nclass OilAuthResult:\n    \"\"\"Results of the oil authentication workflow.\"\"\"\n\n    pipeline: Pipeline\n    cv_metrics: pd.DataFrame\n    confusion_matrix: np.ndarray\n    class_labels: List[str]\n    feature_importances: Optional[pd.Series]\n</code></pre>"},{"location":"api_reference/#foodspec.apps.oils.default_oil_feature_pipeline","title":"default_oil_feature_pipeline","text":"<pre><code>default_oil_feature_pipeline(wavenumbers)\n</code></pre> <p>Feature pipeline for oil peaks and ratios.</p> Source code in <code>src/foodspec/apps/oils.py</code> <pre><code>def default_oil_feature_pipeline(wavenumbers: np.ndarray) -&gt; Pipeline:\n    \"\"\"Feature pipeline for oil peaks and ratios.\"\"\"\n\n    # Typical oil bands (approx): 1655 (C=C), 1742 (C=O), 1450 (CH2 bend)\n    expected_peaks = [1655.0, 1742.0, 1450.0]\n    ratio_def = {\n        \"ratio_1655_1742\": (\"peak_1655.0_height\", \"peak_1742.0_height\"),\n        \"ratio_1450_1655\": (\"peak_1450.0_height\", \"peak_1655.0_height\"),\n    }\n\n    return Pipeline(\n        steps=[\n            (\"peaks\", _PeakFeatureTransformer(wavenumbers=wavenumbers, expected_peaks=expected_peaks)),\n            (\"ratios\", _RatioFeatureTransformer(ratio_def=ratio_def)),\n            (\"to_array\", _DataFrameToArray()),\n        ]\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.apps.oils.default_oil_preprocessing_pipeline","title":"default_oil_preprocessing_pipeline","text":"<pre><code>default_oil_preprocessing_pipeline(wavenumbers)\n</code></pre> <p>Baseline, smoothing, normalization, and fingerprint cropping.</p> Source code in <code>src/foodspec/apps/oils.py</code> <pre><code>def default_oil_preprocessing_pipeline(wavenumbers: np.ndarray) -&gt; Pipeline:\n    \"\"\"Baseline, smoothing, normalization, and fingerprint cropping.\"\"\"\n\n    return Pipeline(\n        steps=[\n            (\"als\", ALSBaseline(lambda_=1e5, p=0.01, max_iter=10)),\n            (\"savgol\", SavitzkyGolaySmoother(window_length=9, polyorder=3)),\n            (\"norm\", VectorNormalizer(norm=\"l2\")),\n            (\"crop\", _RangeCropperTransformer(wavenumbers=wavenumbers, min_wn=600, max_wn=1800)),\n        ]\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.apps.oils.run_oil_authentication_workflow","title":"run_oil_authentication_workflow","text":"<pre><code>run_oil_authentication_workflow(\n    spectra,\n    label_column=\"oil_type\",\n    classifier_name=\"rf\",\n    cv_splits=5,\n)\n</code></pre> <p>Run oil authentication pipeline with cross-validation.</p> Source code in <code>src/foodspec/apps/oils.py</code> <pre><code>def run_oil_authentication_workflow(\n    spectra: FoodSpectrumSet,\n    label_column: str = \"oil_type\",\n    classifier_name: str = \"rf\",\n    cv_splits: int = 5,\n) -&gt; OilAuthResult:\n    \"\"\"Run oil authentication pipeline with cross-validation.\"\"\"\n\n    validate_spectrum_set(spectra)\n    if label_column not in spectra.metadata.columns:\n        raise ValueError(f\"Label column '{label_column}' not found in metadata.\")\n\n    X = spectra.x\n    y = spectra.metadata[label_column].to_numpy()\n    classes = np.unique(y)\n\n    preproc = default_oil_preprocessing_pipeline(spectra.wavenumbers)\n    cropped_axis = preproc.named_steps[\"crop\"].wavenumbers_\n    feat_pipe = default_oil_feature_pipeline(cropped_axis)\n    clf = make_classifier(classifier_name)\n\n    pipeline = Pipeline(\n        steps=[\n            (\"preprocess\", preproc),\n            (\"features\", feat_pipe),\n            (\"clf\", clf),\n        ]\n    )\n\n    skf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=0)\n    fold_rows = []\n    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), start=1):\n        pipeline.fit(X[train_idx], y[train_idx])\n        preds = pipeline.predict(X[test_idx])\n        y_true_fold = y[test_idx]\n        metrics_df = compute_classification_metrics(y_true_fold, preds)\n        metrics_row = metrics_df.iloc[0].to_dict()\n        metrics_row[\"fold\"] = fold\n        fold_rows.append(metrics_row)\n\n    metrics_df = pd.DataFrame(fold_rows)\n    summary = metrics_df.drop(columns=[\"fold\"]).agg([\"mean\", \"std\"])\n    cv_metrics = pd.concat([metrics_df, summary.reset_index().rename(columns={\"index\": \"fold\"})])\n\n    # Fit on full dataset\n    pipeline.fit(X, y)\n    preds_full = pipeline.predict(X)\n    cm = confusion_matrix(y, preds_full, labels=classes)\n\n    feature_importances = None\n    clf_est = pipeline.named_steps[\"clf\"]\n    feature_names = pipeline.named_steps[\"features\"].named_steps[\"to_array\"].columns_\n    if hasattr(clf_est, \"feature_importances_\"):\n        feature_importances = pd.Series(\n            clf_est.feature_importances_, index=feature_names, name=\"importance\"\n        )\n\n    return OilAuthResult(\n        pipeline=pipeline,\n        cv_metrics=cv_metrics,\n        confusion_matrix=cm,\n        class_labels=classes.tolist(),\n        feature_importances=feature_importances,\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.apps.heating.run_heating_degradation_analysis","title":"run_heating_degradation_analysis","text":"<pre><code>run_heating_degradation_analysis(\n    spectra, time_column=\"heating_time\"\n)\n</code></pre> <p>Run heating degradation analysis.</p> <p>Applies baseline/smoothing/normalization/cropping, extracts a simple peak ratio versus heating time, fits trend regressions, and computes a basic ANOVA if groups are present.</p> Source code in <code>src/foodspec/apps/heating.py</code> <pre><code>def run_heating_degradation_analysis(\n    spectra: FoodSpectrumSet,\n    time_column: str = \"heating_time\",\n) -&gt; HeatingAnalysisResult:\n    \"\"\"Run heating degradation analysis.\n\n    Applies baseline/smoothing/normalization/cropping, extracts a simple\n    peak ratio versus heating time, fits trend regressions, and computes\n    a basic ANOVA if groups are present.\n    \"\"\"\n\n    validate_spectrum_set(spectra)\n    if time_column not in spectra.metadata.columns:\n        raise ValueError(f\"Metadata column '{time_column}' not found.\")\n\n    preproc = _default_heating_preprocess(spectra.wavenumbers)\n    X_proc = preproc.transform(spectra.x)\n    wn_proc = preproc.named_steps[\"crop\"].wavenumbers_\n\n    extractor = PeakFeatureExtractor(expected_peaks=[1655.0, 1742.0], tolerance=8.0)\n    extractor.fit(X_proc, wavenumbers=wn_proc)\n    peak_feats = extractor.transform(X_proc, wavenumbers=wn_proc)\n    peak_df = pd.DataFrame(\n        peak_feats, columns=extractor.get_feature_names_out(), index=spectra.metadata.index\n    )\n    ratios = RatioFeatureGenerator({\"ratio_1655_1742\": (\"peak_1655.0_height\", \"peak_1742.0_height\")})\n    ratio_df = ratios.transform(peak_df)\n\n    trend_models: Dict[str, Any] = {}\n    time_values = spectra.metadata[time_column].to_numpy().reshape(-1, 1)\n    for col in ratio_df.columns:\n        model = LinearRegression()\n        model.fit(time_values, ratio_df[col].to_numpy())\n        trend_models[col] = model\n\n    # Optional group-wise models if oil_type present\n    if \"oil_type\" in spectra.metadata.columns:\n        grouped_models: Dict[str, Dict[str, Any]] = {}\n        for col in ratio_df.columns:\n            grouped_models[col] = {}\n            for group, idxs in spectra.metadata.groupby(\"oil_type\").groups.items():\n                Xg = time_values[list(idxs)]\n                yg = ratio_df[col].iloc[list(idxs)].to_numpy()\n                if len(yg) &gt;= 2:\n                    m = LinearRegression()\n                    m.fit(Xg, yg)\n                    grouped_models[col][group] = m\n        trend_models[\"by_oil_type\"] = grouped_models\n\n    anova_results = None\n    if \"oil_type\" in spectra.metadata.columns and spectra.metadata[\"oil_type\"].nunique() &gt;= 2:\n        rows = []\n        for col in ratio_df.columns:\n            groups = []\n            for _, idxs in spectra.metadata.groupby(\"oil_type\").groups.items():\n                vals = ratio_df[col].iloc[list(idxs)].to_numpy()\n                if len(vals) &gt; 0:\n                    groups.append(vals)\n            if len(groups) &gt;= 2:\n                F, p = stats.f_oneway(*groups)\n                rows.append({\"factor\": \"oil_type\", \"metric\": col, \"F\": F, \"pvalue\": p})\n        if rows:\n            anova_results = pd.DataFrame(rows)\n\n    return HeatingAnalysisResult(\n        preprocessed_spectra=X_proc,\n        wavenumbers=wn_proc,\n        time_variable=spectra.metadata[time_column],\n        key_ratios=ratio_df,\n        trend_models=trend_models,\n        anova_results=anova_results,\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.apps.qc.apply_qc_model","title":"apply_qc_model","text":"<pre><code>apply_qc_model(\n    spectra,\n    model,\n    threshold=None,\n    higher_score_is_more_normal=True,\n    metadata=None,\n)\n</code></pre> <p>Score spectra with a novelty model and produce QC labels.</p> Source code in <code>src/foodspec/apps/qc.py</code> <pre><code>def apply_qc_model(\n    spectra: FoodSpectrumSet,\n    model: Any,\n    threshold: Optional[float] = None,\n    higher_score_is_more_normal: bool = True,\n    metadata: Optional[pd.DataFrame] = None,\n) -&gt; QCResult:\n    \"\"\"Score spectra with a novelty model and produce QC labels.\"\"\"\n\n    X = spectra.x\n    if hasattr(model, \"decision_function\"):\n        scores = model.decision_function(X)\n    elif hasattr(model, \"score_samples\"):\n        scores = model.score_samples(X)\n    else:\n        raise ValueError(\"Model must implement decision_function or score_samples.\")\n\n    scores_ser = pd.Series(scores, index=spectra.metadata.index)\n\n    if threshold is None:\n        if isinstance(model, OneClassSVM):\n            threshold = float(np.quantile(scores, 0.2))\n        else:\n            threshold = float(np.median(scores))\n    if higher_score_is_more_normal:\n        labels_arr = np.where(scores &gt;= threshold, \"authentic\", \"suspect\")\n    else:\n        labels_arr = np.where(scores &lt;= threshold, \"authentic\", \"suspect\")\n    labels_ser = pd.Series(labels_arr, index=spectra.metadata.index)\n\n    meta = metadata.copy() if metadata is not None else spectra.metadata.copy()\n\n    return QCResult(\n        scores=scores_ser,\n        labels_pred=labels_ser,\n        threshold=threshold,\n        model=model,\n        metadata=meta,\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.apps.qc.train_qc_model","title":"train_qc_model","text":"<pre><code>train_qc_model(\n    spectra,\n    train_mask=None,\n    model_type=\"oneclass_svm\",\n    **kwargs\n)\n</code></pre> <p>Train a novelty detection model on authentic spectra.</p> Source code in <code>src/foodspec/apps/qc.py</code> <pre><code>def train_qc_model(\n    spectra: FoodSpectrumSet,\n    train_mask: Optional[pd.Series] = None,\n    model_type: str = \"oneclass_svm\",\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Train a novelty detection model on authentic spectra.\"\"\"\n\n    X = spectra.x\n    if train_mask is not None:\n        train_idx = train_mask.to_numpy()\n        X = X[train_idx]\n\n    name = model_type.lower()\n    if name == \"oneclass_svm\":\n        gamma = kwargs.pop(\"gamma\", \"scale\")\n        nu = kwargs.pop(\"nu\", 0.05)\n        model = OneClassSVM(kernel=\"rbf\", gamma=gamma, nu=nu, **kwargs)\n    elif name == \"isolation_forest\":\n        model = IsolationForest(contamination=0.05, random_state=0, **kwargs)\n    else:\n        raise ValueError(\"model_type must be 'oneclass_svm' or 'isolation_forest'.\")\n\n    model.fit(X)\n    return model\n</code></pre>"},{"location":"api_reference/#foodspec.apps.dairy.DairyAnalysisResult","title":"DairyAnalysisResult  <code>dataclass</code>","text":"<p>Result container for dairy authentication.</p> Source code in <code>src/foodspec/apps/dairy.py</code> <pre><code>@dataclass\nclass DairyAnalysisResult:\n    \"\"\"Result container for dairy authentication.\"\"\"\n\n    preprocessed_spectra: np.ndarray\n    wavenumbers: np.ndarray\n    cv_metrics: pd.DataFrame\n    confusion_matrix: np.ndarray\n    class_labels: list[str]\n</code></pre>"},{"location":"api_reference/#foodspec.apps.dairy.run_dairy_authentication_workflow","title":"run_dairy_authentication_workflow","text":"<pre><code>run_dairy_authentication_workflow(\n    spectra,\n    label_column=\"label\",\n    classifier_name=\"rf\",\n    cv_splits=5,\n)\n</code></pre> <p>Apply a generic authentication workflow to dairy spectra.</p> Source code in <code>src/foodspec/apps/dairy.py</code> <pre><code>def run_dairy_authentication_workflow(\n    spectra: FoodSpectrumSet,\n    label_column: str = \"label\",\n    classifier_name: str = \"rf\",\n    cv_splits: int = 5,\n) -&gt; DairyAnalysisResult:\n    \"\"\"Apply a generic authentication workflow to dairy spectra.\"\"\"\n    result = run_oil_authentication_workflow(\n        spectra=spectra, label_column=label_column, classifier_name=classifier_name, cv_splits=cv_splits\n    )\n    preprocess = result.pipeline.named_steps.get(\"preprocess\")\n    if preprocess is not None:\n        x_proc = preprocess.transform(spectra.x)\n        wn_proc = preprocess.named_steps[\"crop\"].wavenumbers_\n    else:\n        x_proc = spectra.x\n        wn_proc = spectra.wavenumbers\n\n    return DairyAnalysisResult(\n        preprocessed_spectra=x_proc,\n        wavenumbers=wn_proc,\n        cv_metrics=result.cv_metrics,\n        confusion_matrix=result.confusion_matrix,\n        class_labels=result.class_labels,\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.apps.meat.MeatAnalysisResult","title":"MeatAnalysisResult  <code>dataclass</code>","text":"<p>Result container for meat authentication.</p> Source code in <code>src/foodspec/apps/meat.py</code> <pre><code>@dataclass\nclass MeatAnalysisResult:\n    \"\"\"Result container for meat authentication.\"\"\"\n\n    preprocessed_spectra: np.ndarray\n    wavenumbers: np.ndarray\n    cv_metrics: pd.DataFrame\n    confusion_matrix: np.ndarray\n    class_labels: list[str]\n</code></pre>"},{"location":"api_reference/#foodspec.apps.meat.run_meat_authentication_workflow","title":"run_meat_authentication_workflow","text":"<pre><code>run_meat_authentication_workflow(\n    spectra,\n    label_column=\"label\",\n    classifier_name=\"rf\",\n    cv_splits=5,\n)\n</code></pre> <p>Apply a generic authentication workflow to meat spectra.</p> Source code in <code>src/foodspec/apps/meat.py</code> <pre><code>def run_meat_authentication_workflow(\n    spectra: FoodSpectrumSet,\n    label_column: str = \"label\",\n    classifier_name: str = \"rf\",\n    cv_splits: int = 5,\n) -&gt; MeatAnalysisResult:\n    \"\"\"Apply a generic authentication workflow to meat spectra.\"\"\"\n    result = run_oil_authentication_workflow(\n        spectra=spectra, label_column=label_column, classifier_name=classifier_name, cv_splits=cv_splits\n    )\n    preprocess = result.pipeline.named_steps.get(\"preprocess\")\n    if preprocess is not None:\n        x_proc = preprocess.transform(spectra.x)\n        wn_proc = preprocess.named_steps[\"crop\"].wavenumbers_\n    else:\n        x_proc = spectra.x\n        wn_proc = spectra.wavenumbers\n\n    return MeatAnalysisResult(\n        preprocessed_spectra=x_proc,\n        wavenumbers=wn_proc,\n        cv_metrics=result.cv_metrics,\n        confusion_matrix=result.confusion_matrix,\n        class_labels=result.class_labels,\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.apps.microbial.MicrobialAnalysisResult","title":"MicrobialAnalysisResult  <code>dataclass</code>","text":"<p>Result container for microbial detection.</p> Source code in <code>src/foodspec/apps/microbial.py</code> <pre><code>@dataclass\nclass MicrobialAnalysisResult:\n    \"\"\"Result container for microbial detection.\"\"\"\n\n    preprocessed_spectra: np.ndarray\n    wavenumbers: np.ndarray\n    cv_metrics: pd.DataFrame\n    confusion_matrix: np.ndarray\n    class_labels: list[str]\n</code></pre>"},{"location":"api_reference/#foodspec.apps.microbial.run_microbial_detection_workflow","title":"run_microbial_detection_workflow","text":"<pre><code>run_microbial_detection_workflow(\n    spectra,\n    label_column=\"label\",\n    classifier_name=\"rf\",\n    cv_splits=5,\n)\n</code></pre> <p>Apply a generic authentication workflow to microbial spectra.</p> Source code in <code>src/foodspec/apps/microbial.py</code> <pre><code>def run_microbial_detection_workflow(\n    spectra: FoodSpectrumSet,\n    label_column: str = \"label\",\n    classifier_name: str = \"rf\",\n    cv_splits: int = 5,\n) -&gt; MicrobialAnalysisResult:\n    \"\"\"Apply a generic authentication workflow to microbial spectra.\"\"\"\n    result = run_oil_authentication_workflow(\n        spectra=spectra, label_column=label_column, classifier_name=classifier_name, cv_splits=cv_splits\n    )\n    preprocess = result.pipeline.named_steps.get(\"preprocess\")\n    if preprocess is not None:\n        x_proc = preprocess.transform(spectra.x)\n        wn_proc = preprocess.named_steps[\"crop\"].wavenumbers_\n    else:\n        x_proc = spectra.x\n        wn_proc = spectra.wavenumbers\n\n    return MicrobialAnalysisResult(\n        preprocessed_spectra=x_proc,\n        wavenumbers=wn_proc,\n        cv_metrics=result.cv_metrics,\n        confusion_matrix=result.confusion_matrix,\n        class_labels=result.class_labels,\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.apps.protocol_validation.run_protocol_benchmarks","title":"run_protocol_benchmarks","text":"<pre><code>run_protocol_benchmarks(output_dir, random_state=42)\n</code></pre> <p>Run core protocol benchmarks on public datasets and save reports.</p> <p>Returns a dict summarizing metrics.</p> Source code in <code>src/foodspec/apps/protocol_validation.py</code> <pre><code>def run_protocol_benchmarks(\n    output_dir: PathLike,\n    random_state: int = 42,\n) -&gt; dict:\n    \"\"\"\n    Run core protocol benchmarks on public datasets and save reports.\n\n    Returns a dict summarizing metrics.\n    \"\"\"\n\n    out_base = Path(output_dir)\n    run_dir = create_run_dir(out_base, \"protocol\")\n    summary: Dict[str, Dict] = {}\n\n    # Classification benchmark\n    try:\n        metrics_cls, cm_df = _classification_benchmark(random_state=random_state)\n        summary[\"classification\"] = metrics_cls\n        write_json(run_dir / \"classification_metrics.json\", metrics_cls)\n        cm_df.to_csv(run_dir / \"classification_confusion_matrix.csv\")\n    except FileNotFoundError as exc:\n        summary[\"classification_error\"] = str(exc)\n\n    # Mixture benchmark\n    try:\n        metrics_mix = _mixture_benchmark(random_state=random_state)\n        summary[\"mixture\"] = metrics_mix\n        write_json(run_dir / \"mixture_metrics.json\", metrics_mix)\n    except FileNotFoundError as exc:\n        summary[\"mixture_error\"] = str(exc)\n\n    # Report\n    report_sections = {\n        \"Overview\": \"Protocol benchmarks on public datasets (oil classification, mixture regression).\",\n        \"Metrics\": summarize_metrics_for_markdown(summary),\n    }\n    write_markdown_report(run_dir / \"report.md\", title=\"Protocol Benchmarks\", sections=report_sections)\n    summary[\"run_dir\"] = str(run_dir)\n    return summary\n</code></pre>"},{"location":"api_reference/#foodspec.apps.methodsx_reproduction.run_methodsx_reproduction","title":"run_methodsx_reproduction","text":"<pre><code>run_methodsx_reproduction(output_dir, random_state=42)\n</code></pre> <p>Run the core analyses used in the MethodsX protocol article and save artifacts.</p>"},{"location":"api_reference/#foodspec.apps.methodsx_reproduction.run_methodsx_reproduction--steps","title":"Steps","text":"<p>1) Oil-type classification on a public dataset (Raman/FTIR). 2) PCA visualization of the same dataset. 3) Mixture analysis on EVOO\u2013sunflower data via regression/NNLS-like fit.</p>"},{"location":"api_reference/#foodspec.apps.methodsx_reproduction.run_methodsx_reproduction--returns","title":"Returns","text":"<p>dict     High-level metrics including accuracy/F1 for classification and     R\u00b2/RMSE for mixture regression. A <code>run_dir</code> key points to the     folder containing artifacts.</p> Source code in <code>src/foodspec/apps/methodsx_reproduction.py</code> <pre><code>def run_methodsx_reproduction(output_dir: PathLike, random_state: int = 42) -&gt; Dict:\n    \"\"\"\n    Run the core analyses used in the MethodsX protocol article and save artifacts.\n\n    Steps\n    -----\n    1) Oil-type classification on a public dataset (Raman/FTIR).\n    2) PCA visualization of the same dataset.\n    3) Mixture analysis on EVOO\u2013sunflower data via regression/NNLS-like fit.\n\n    Returns\n    -------\n    dict\n        High-level metrics including accuracy/F1 for classification and\n        R\u00b2/RMSE for mixture regression. A ``run_dir`` key points to the\n        folder containing artifacts.\n    \"\"\"\n\n    run_dir = create_run_dir(output_dir, \"methodsx\")\n    metrics: Dict[str, float] = {}\n\n    # Oil classification (classic ML only)\n    oil_ds = load_public_mendeley_oils()\n    oil_result = run_oil_authentication_workflow(oil_ds, label_column=\"oil_type\", cv_splits=3)\n    if \"accuracy\" in oil_result.cv_metrics.columns:\n        metrics[\"oil_accuracy\"] = float(oil_result.cv_metrics[\"accuracy\"].mean())\n    if \"f1\" in oil_result.cv_metrics.columns:\n        metrics[\"oil_f1\"] = float(oil_result.cv_metrics[\"f1\"].mean())\n\n    # Confusion matrix plot\n    fig_cm, ax_cm = plt.subplots()\n    plot_confusion_matrix(oil_result.confusion_matrix, class_names=oil_result.class_labels, ax=ax_cm)\n    save_figure(run_dir, \"oil_confusion_matrix\", fig_cm)\n    plt.close(fig_cm)\n\n    # PCA visualization\n    _, pca_res = run_pca(oil_ds.x, n_components=2)\n    fig_pca, ax_pca = plt.subplots()\n    plot_pca_scores(pca_res.scores, labels=oil_ds.metadata.get(\"oil_type\"), ax=ax_pca)\n    ax_pca.set_title(\"PCA scores (public oil dataset)\")\n    save_figure(run_dir, \"oil_pca_scores\", fig_pca)\n    plt.close(fig_pca)\n\n    # Mixture regression (EVOO\u2013sunflower)\n    mix_ds = load_public_evoo_sunflower_raman()\n    y = mix_ds.metadata[\"mixture_fraction_evoo\"].to_numpy()\n    mask = ~np.isnan(y)\n    X = mix_ds.x[mask]\n    y = y[mask]\n    # If values look like percentages, scale to 0-1\n    if np.nanmax(y) &gt; 1:\n        y = y / 100.0\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=random_state, stratify=None\n    )\n    reg = Pipeline([(\"scaler\", StandardScaler()), (\"linreg\", LinearRegression())])\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_test)\n    metrics[\"mixture_r2\"] = float(r2_score(y_test, y_pred))\n    metrics[\"mixture_rmse\"] = float(np.sqrt(mean_squared_error(y_test, y_pred)))\n\n    # Save metrics and report\n    metrics[\"run_dir\"] = str(run_dir)\n    write_json(run_dir / \"metrics.json\", metrics)\n    sections = {\n        \"Overview\": (\n            \"This run executes the core MethodsX protocol reproduction: oil-type \"\n            \"classification, PCA visualization, and EVOO\u2013sunflower mixture regression.\"\n        ),\n        \"Key metrics\": summarize_metrics_for_markdown(metrics),\n        \"Artifacts\": (\n            \"- oil_confusion_matrix.png\\n\"\n            \"- oil_pca_scores.png\\n\"\n            \"- metrics.json\"\n        ),\n        \"Assumptions\": (\n            \"Public datasets must be downloaded locally for the loaders to succeed.\"\n        ),\n    }\n    write_markdown_report(run_dir / \"report.md\", title=\"MethodsX Protocol Reproduction\", sections=sections)\n    return metrics\n</code></pre>"},{"location":"api_reference/#foodspec.cli.about","title":"about","text":"<pre><code>about()\n</code></pre> <p>Print version and environment information for foodspec.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"about\")\ndef about() -&gt; None:\n    \"\"\"Print version and environment information for foodspec.\"\"\"\n\n    extras = _detect_optional_extras()\n    typer.echo(f\"foodspec version: {__version__}\")\n    typer.echo(f\"Python version: {sys.version.split()[0]}\")\n    typer.echo(f\"Optional extras detected: {', '.join(extras) if extras else 'none'}\")\n    typer.echo(\"Documentation: https://github.com/your-org/foodspec#documentation\")\n    typer.echo(\n        \"Description: foodspec is a headless, research-grade toolkit for Raman/FTIR in food science.\"\n    )\n</code></pre>"},{"location":"api_reference/#foodspec.cli.csv_to_library","title":"csv_to_library","text":"<pre><code>csv_to_library(\n    csv_path=typer.Argument(\n        ..., help=\"Input CSV file with spectra.\"\n    ),\n    output_hdf5=typer.Argument(\n        ...,\n        help=\"Output HDF5 library path (will be created or overwritten).\",\n    ),\n    format=typer.Option(\n        \"wide\",\n        \"--format\",\n        help=\"CSV layout: 'wide' (one column per spectrum) or 'long' (tidy format).\",\n        case_sensitive=False,\n    ),\n    modality=typer.Option(\n        \"raman\",\n        \"--modality\",\n        help=\"Spectroscopy modality tag (e.g. 'raman', 'ftir').\",\n    ),\n    wavenumber_column=typer.Option(\n        \"wavenumber\",\n        \"--wavenumber-column\",\n        help=\"Name of the wavenumber column.\",\n    ),\n    sample_id_column=typer.Option(\n        \"sample_id\",\n        \"--sample-id-column\",\n        help=\"For 'long' format: sample identifier column.\",\n    ),\n    intensity_column=typer.Option(\n        \"intensity\",\n        \"--intensity-column\",\n        help=\"For 'long' format: intensity column.\",\n    ),\n    label_column=typer.Option(\n        \"\",\n        \"--label-column\",\n        help=\"Optional label column name (e.g. oil_type).\",\n    ),\n)\n</code></pre> <p>Convert a CSV file of spectra into an HDF5 library usable by foodspec workflows.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"csv-to-library\")\ndef csv_to_library(\n    csv_path: str = typer.Argument(..., help=\"Input CSV file with spectra.\"),\n    output_hdf5: str = typer.Argument(\n        ..., help=\"Output HDF5 library path (will be created or overwritten).\"\n    ),\n    format: str = typer.Option(\n        \"wide\",\n        \"--format\",\n        help=\"CSV layout: 'wide' (one column per spectrum) or 'long' (tidy format).\",\n        case_sensitive=False,\n    ),\n    modality: str = typer.Option(\n        \"raman\",\n        \"--modality\",\n        help=\"Spectroscopy modality tag (e.g. 'raman', 'ftir').\",\n    ),\n    wavenumber_column: str = typer.Option(\n        \"wavenumber\",\n        \"--wavenumber-column\",\n        help=\"Name of the wavenumber column.\",\n    ),\n    sample_id_column: str = typer.Option(\n        \"sample_id\",\n        \"--sample-id-column\",\n        help=\"For 'long' format: sample identifier column.\",\n    ),\n    intensity_column: str = typer.Option(\n        \"intensity\",\n        \"--intensity-column\",\n        help=\"For 'long' format: intensity column.\",\n    ),\n    label_column: str = typer.Option(\n        \"\",\n        \"--label-column\",\n        help=\"Optional label column name (e.g. oil_type).\",\n    ),\n):\n    \"\"\"\n    Convert a CSV file of spectra into an HDF5 library usable by foodspec workflows.\n    \"\"\"\n\n    label_column = label_column or None\n    logger.info(\"Loading CSV spectra from %s\", csv_path)\n    ds = load_csv_spectra(\n        csv_path=csv_path,\n        format=format,\n        wavenumber_column=wavenumber_column,\n        sample_id_column=sample_id_column,\n        intensity_column=intensity_column,\n        label_column=label_column,\n        modality=modality,\n    )\n\n    output_path = Path(output_hdf5)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    logger.info(\"Saving HDF5 library to %s\", output_hdf5)\n    create_library(path=output_hdf5, spectra=ds)\n    logger.info(\"Done. Library contains %s spectra.\", len(ds))\n</code></pre>"},{"location":"api_reference/#foodspec.cli.domains_command","title":"domains_command","text":"<pre><code>domains_command(\n    input_hdf5=typer.Argument(\n        ..., help=\"Preprocessed spectra HDF5.\"\n    ),\n    domain=typer.Option(\n        ...,\n        \"--type\",\n        help=\"Domain type: dairy, meat, microbial.\",\n    ),\n    label_column=typer.Option(\n        \"label\", help=\"Metadata column with class labels.\"\n    ),\n    classifier_name=typer.Option(\n        \"rf\", help=\"Classifier name.\"\n    ),\n    cv_splits=typer.Option(5, help=\"Number of CV splits.\"),\n    output_dir=typer.Option(\n        \"./out\", help=\"Base output directory.\"\n    ),\n    save_model_path=typer.Option(\n        None,\n        \"--save-model\",\n        help=\"Optional base path to save the trained model (without extension).\",\n    ),\n    model_version=typer.Option(\n        __version__, help=\"Model version tag.\"\n    ),\n)\n</code></pre> <p>Run domain-specific authentication templates and write report.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"domains\")\ndef domains_command(\n    input_hdf5: str = typer.Argument(..., help=\"Preprocessed spectra HDF5.\"),\n    domain: str = typer.Option(..., \"--type\", help=\"Domain type: dairy, meat, microbial.\"),\n    label_column: str = typer.Option(\"label\", help=\"Metadata column with class labels.\"),\n    classifier_name: str = typer.Option(\"rf\", help=\"Classifier name.\"),\n    cv_splits: int = typer.Option(5, help=\"Number of CV splits.\"),\n    output_dir: str = typer.Option(\"./out\", help=\"Base output directory.\"),\n    save_model_path: Optional[str] = typer.Option(\n        None, \"--save-model\", help=\"Optional base path to save the trained model (without extension).\"\n    ),\n    model_version: str = typer.Option(__version__, help=\"Model version tag.\"),\n):\n    \"\"\"Run domain-specific authentication templates and write report.\"\"\"\n\n    ds = load_library(input_hdf5)\n    domain_lower = domain.lower()\n    if domain_lower == \"dairy\":\n        result = run_dairy_authentication_workflow(\n            ds, label_column=label_column, classifier_name=classifier_name, cv_splits=cv_splits\n        )\n    elif domain_lower == \"meat\":\n        result = run_meat_authentication_workflow(\n            ds, label_column=label_column, classifier_name=classifier_name, cv_splits=cv_splits\n        )\n    elif domain_lower == \"microbial\":\n        result = run_microbial_detection_workflow(\n            ds, label_column=label_column, classifier_name=classifier_name, cv_splits=cv_splits\n        )\n    else:\n        raise typer.BadParameter(\"domain must be one of: dairy, meat, microbial.\")\n\n    report_dir = _write_domain_report(result, Path(output_dir), domain=domain_lower, classifier_name=classifier_name)\n\n    if save_model_path is not None:\n        name = f\"{domain_lower}_{classifier_name.lower()}\"\n        registry_save_model(\n            result.pipeline,\n            save_model_path,\n            name=name,\n            version=model_version,\n            foodspec_version=__version__,\n            extra={\n                \"command\": \"domains\",\n                \"domain\": domain_lower,\n                \"classifier_name\": classifier_name,\n                \"label_column\": label_column,\n                \"cv_splits\": cv_splits,\n                \"class_labels\": list(result.class_labels),\n            },\n        )\n        typer.echo(f\"Model saved: {save_model_path}.joblib / {save_model_path}.json\")\n\n    typer.echo(f\"{domain} report: {report_dir}\")\n</code></pre>"},{"location":"api_reference/#foodspec.cli.heating_command","title":"heating_command","text":"<pre><code>heating_command(\n    input_hdf5=typer.Argument(\n        ..., help=\"Preprocessed spectra HDF5.\"\n    ),\n    time_column=typer.Option(\n        \"heating_time\",\n        help=\"Metadata column for heating time.\",\n    ),\n    output_dir=typer.Option(\n        \"./out\", help=\"Base output directory.\"\n    ),\n)\n</code></pre> <p>Run heating degradation workflow and write report folder.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"heating\")\ndef heating_command(\n    input_hdf5: str = typer.Argument(..., help=\"Preprocessed spectra HDF5.\"),\n    time_column: str = typer.Option(\"heating_time\", help=\"Metadata column for heating time.\"),\n    output_dir: str = typer.Option(\"./out\", help=\"Base output directory.\"),\n):\n    \"\"\"Run heating degradation workflow and write report folder.\"\"\"\n\n    run_meta = log_run_metadata(logger, {\"command\": \"heating\"})\n    ds = load_library(input_hdf5)\n    result = run_heating_degradation_analysis(ds, time_column=time_column)\n    report_dir = _write_heating_report(result, Path(output_dir), time_column=time_column)\n    typer.echo(f\"Heating report: {report_dir}\")\n</code></pre>"},{"location":"api_reference/#foodspec.cli.hyperspectral_command","title":"hyperspectral_command","text":"<pre><code>hyperspectral_command(\n    input_hdf5=typer.Argument(\n        ..., help=\"Flattened pixel spectra HDF5.\"\n    ),\n    height=typer.Option(\n        ..., help=\"Image height in pixels.\"\n    ),\n    width=typer.Option(..., help=\"Image width in pixels.\"),\n    target_wavenumber=typer.Option(\n        1655.0, help=\"Target wavenumber for intensity map.\"\n    ),\n    window=typer.Option(5.0, help=\"Integration window.\"),\n    output_dir=typer.Option(\n        \"./out\", help=\"Base output directory.\"\n    ),\n)\n</code></pre> <p>Create hyperspectral intensity map from flattened spectra.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"hyperspectral\")\ndef hyperspectral_command(\n    input_hdf5: str = typer.Argument(..., help=\"Flattened pixel spectra HDF5.\"),\n    height: int = typer.Option(..., help=\"Image height in pixels.\"),\n    width: int = typer.Option(..., help=\"Image width in pixels.\"),\n    target_wavenumber: float = typer.Option(1655.0, help=\"Target wavenumber for intensity map.\"),\n    window: float = typer.Option(5.0, help=\"Integration window.\"),\n    output_dir: str = typer.Option(\"./out\", help=\"Base output directory.\"),\n):\n    \"\"\"Create hyperspectral intensity map from flattened spectra.\"\"\"\n\n    ds = load_library(input_hdf5)\n    cube = HyperSpectralCube.from_spectrum_set(ds, image_shape=(height, width))\n    report_dir = _write_hyperspectral_report(\n        cube=cube, target_wavenumber=target_wavenumber, window=window, output_dir=Path(output_dir)\n    )\n    typer.echo(f\"Hyperspectral report: {report_dir}\")\n</code></pre>"},{"location":"api_reference/#foodspec.cli.mixture_command","title":"mixture_command","text":"<pre><code>mixture_command(\n    input_hdf5=typer.Argument(\n        ..., help=\"Preprocessed spectra HDF5.\"\n    ),\n    pure_hdf5=typer.Option(\n        ..., help=\"HDF5 with pure component spectra.\"\n    ),\n    spectrum_index=typer.Option(\n        0,\n        help=\"Index of spectrum in input file to decompose.\",\n    ),\n    output_dir=typer.Option(\n        \"./out\", help=\"Base output directory.\"\n    ),\n)\n</code></pre> <p>Perform NNLS mixture analysis on a single spectrum and write report.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"mixture\")\ndef mixture_command(\n    input_hdf5: str = typer.Argument(..., help=\"Preprocessed spectra HDF5.\"),\n    pure_hdf5: str = typer.Option(..., help=\"HDF5 with pure component spectra.\"),\n    spectrum_index: int = typer.Option(0, help=\"Index of spectrum in input file to decompose.\"),\n    output_dir: str = typer.Option(\"./out\", help=\"Base output directory.\"),\n):\n    \"\"\"Perform NNLS mixture analysis on a single spectrum and write report.\"\"\"\n\n    spectra = load_library(input_hdf5)\n    pure = load_library(pure_hdf5)\n    if pure.wavenumbers.shape != spectra.wavenumbers.shape or not np.allclose(pure.wavenumbers, spectra.wavenumbers):\n        raise typer.BadParameter(\"Pure and input wavenumbers must match.\")\n    if spectrum_index &lt; 0 or spectrum_index &gt;= len(spectra):\n        raise typer.BadParameter(\"spectrum_index out of range.\")\n\n    spectrum = spectra.x[spectrum_index]\n    pure_mat = pure.x.T  # n_points x n_components\n    coeffs, res = nnls_mixture(spectrum, pure_mat)\n    reconstructed = pure_mat @ coeffs\n\n    fig, ax = plt.subplots()\n    ax.plot(spectra.wavenumbers, spectrum, label=\"original\")\n    ax.plot(spectra.wavenumbers, reconstructed, label=\"reconstructed\", linestyle=\"--\")\n    ax.set_xlabel(\"Wavenumber\")\n    ax.set_ylabel(\"Intensity\")\n    ax.legend()\n    report_dir = _write_mixture_report(\n        spectrum_index=spectrum_index,\n        coeffs=coeffs,\n        residual=res,\n        pure_labels=pure.metadata[\"sample_id\"] if \"sample_id\" in pure.metadata.columns else None,\n        output_dir=Path(output_dir),\n    )\n    save_figure(report_dir, \"mixture_fit\", fig)\n    plt.close(fig)\n    typer.echo(f\"Mixture report: {report_dir}\")\n</code></pre>"},{"location":"api_reference/#foodspec.cli.model_info_command","title":"model_info_command","text":"<pre><code>model_info_command(\n    path=typer.Argument(\n        ...,\n        help=\"Base path of saved model (without extension).\",\n    )\n)\n</code></pre> <p>Inspect saved model metadata.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"model-info\")\ndef model_info_command(\n    path: str = typer.Argument(..., help=\"Base path of saved model (without extension).\"),\n):\n    \"\"\"Inspect saved model metadata.\"\"\"\n\n    model_base = Path(path)\n    joblib_path = model_base.with_suffix(\".joblib\")\n    json_path = model_base.with_suffix(\".json\")\n    if not joblib_path.exists() or not json_path.exists():\n        typer.echo(\"Model files not found (expected .joblib and .json).\", err=True)\n        raise typer.Exit(code=1)\n    try:\n        _, meta = registry_load_model(path)\n    except Exception as exc:  # pragma: no cover - defensive\n        typer.echo(f\"Failed to load model metadata: {exc}\", err=True)\n        raise typer.Exit(code=1)\n    typer.echo(f\"Name: {meta.name}\")\n    typer.echo(f\"Version: {meta.version}\")\n    typer.echo(f\"Foodspec version: {meta.foodspec_version}\")\n    typer.echo(f\"Created at: {meta.created_at}\")\n    typer.echo(\"Extra:\")\n    typer.echo(json.dumps(meta.extra, indent=2))\n</code></pre>"},{"location":"api_reference/#foodspec.cli.oil_auth","title":"oil_auth","text":"<pre><code>oil_auth(\n    input_hdf5=typer.Argument(\n        ..., help=\"Input HDF5 file with spectra.\"\n    ),\n    label_column=typer.Option(\n        \"oil_type\", help=\"Metadata column for class labels.\"\n    ),\n    cv_splits=typer.Option(\n        5, help=\"CV splits for classifier.\"\n    ),\n    output_report=typer.Option(\n        \"oil_auth_report.html\",\n        help=\"Output HTML report path.\",\n    ),\n    save_model_path=typer.Option(\n        None,\n        \"--save-model\",\n        help=\"Optional base path to save the trained model (without extension).\",\n    ),\n    model_version=typer.Option(\n        __version__, help=\"Model version tag.\"\n    ),\n    config=typer.Option(\n        None,\n        \"--config\",\n        help=\"Optional YAML/JSON config file.\",\n    ),\n)\n</code></pre> <p>Run oil authentication workflow and save HTML report.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"oil-auth\")\ndef oil_auth(\n    input_hdf5: str = typer.Argument(..., help=\"Input HDF5 file with spectra.\"),\n    label_column: str = typer.Option(\"oil_type\", help=\"Metadata column for class labels.\"),\n    cv_splits: int = typer.Option(5, help=\"CV splits for classifier.\"),\n    output_report: str = typer.Option(\"oil_auth_report.html\", help=\"Output HTML report path.\"),\n    save_model_path: Optional[str] = typer.Option(\n        None, \"--save-model\", help=\"Optional base path to save the trained model (without extension).\"\n    ),\n    model_version: str = typer.Option(__version__, help=\"Model version tag.\"),\n    config: Optional[str] = typer.Option(None, \"--config\", help=\"Optional YAML/JSON config file.\"),\n):\n    \"\"\"Run oil authentication workflow and save HTML report.\"\"\"\n\n    run_meta = log_run_metadata(logger, {\"command\": \"oil-auth\"})\n    base_cfg = {\"input_hdf5\": input_hdf5, \"label_column\": label_column, \"output_report\": output_report, \"cv_splits\": cv_splits}\n    cfg = load_config(config) if config else base_cfg\n    cfg = merge_cli_overrides(cfg, base_cfg)\n\n    ds = load_library(cfg[\"input_hdf5\"])\n    result = run_oil_authentication_workflow(\n        spectra=ds,\n        label_column=cfg.get(\"label_column\", label_column),\n        cv_splits=cfg.get(\"cv_splits\", cv_splits),\n    )\n    render_html_report_oil_auth(result, cfg.get(\"output_report\", output_report))\n    classifier_name = result.pipeline.named_steps.get(\"clf\").__class__.__name__ if result.pipeline else \"unknown\"\n    report_dir = _write_oil_report(\n        result,\n        ds,\n        label_column=label_column,\n        output_report=Path(output_report),\n        classifier_name=classifier_name,\n        run_metadata=run_meta,\n    )\n    typer.echo(f\"Report folder: {report_dir}\")\n    if save_model_path is not None:\n        name = f\"oil_{classifier_name.lower()}\"\n        registry_save_model(\n            result.pipeline,\n            save_model_path,\n            name=name,\n            version=model_version,\n            foodspec_version=__version__,\n            extra={\n                \"command\": \"oil-auth\",\n                \"label_column\": label_column,\n                \"classifier_name\": classifier_name,\n                \"class_labels\": list(result.class_labels),\n            },\n        )\n        typer.echo(f\"Model saved: {save_model_path}.joblib / {save_model_path}.json\")\n    typer.echo(f\"HTML report written to {output_report}\")\n</code></pre>"},{"location":"api_reference/#foodspec.cli.preprocess","title":"preprocess","text":"<pre><code>preprocess(\n    input_folder=typer.Argument(\n        ..., help=\"Folder containing spectra text files.\"\n    ),\n    metadata_csv=typer.Option(\n        None, help=\"Optional metadata CSV with sample_id.\"\n    ),\n    output_hdf5=typer.Argument(\n        ..., help=\"Output HDF5 path.\"\n    ),\n    modality=typer.Option(\n        \"raman\", help=\"Spectroscopy modality.\"\n    ),\n    min_wn=typer.Option(\n        600.0, help=\"Minimum wavenumber for cropping.\"\n    ),\n    max_wn=typer.Option(\n        1800.0, help=\"Maximum wavenumber for cropping.\"\n    ),\n)\n</code></pre> <p>Load spectra, apply default preprocessing, and save to HDF5.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"preprocess\")\ndef preprocess(\n    input_folder: str = typer.Argument(..., help=\"Folder containing spectra text files.\"),\n    metadata_csv: Optional[str] = typer.Option(None, help=\"Optional metadata CSV with sample_id.\"),\n    output_hdf5: str = typer.Argument(..., help=\"Output HDF5 path.\"),\n    modality: str = typer.Option(\"raman\", help=\"Spectroscopy modality.\"),\n    min_wn: float = typer.Option(600.0, help=\"Minimum wavenumber for cropping.\"),\n    max_wn: float = typer.Option(1800.0, help=\"Maximum wavenumber for cropping.\"),\n):\n    \"\"\"Load spectra, apply default preprocessing, and save to HDF5.\"\"\"\n\n    ds = load_folder(\n        folder=input_folder,\n        metadata_csv=metadata_csv,\n        modality=modality,\n    )\n    pipe = _default_preprocess_pipeline(ds.wavenumbers, min_wn=min_wn, max_wn=max_wn)\n    x_proc = pipe.fit_transform(ds.x)\n    cropper = pipe.named_steps[\"crop\"]\n    ds_out = FoodSpectrumSet(\n        x=x_proc,\n        wavenumbers=cropper.wavenumbers_,\n        metadata=ds.metadata.copy(),\n        modality=ds.modality,\n    )\n    to_hdf5(ds_out, output_hdf5)\n    typer.echo(f\"Preprocessed spectra saved to {output_hdf5}\")\n</code></pre>"},{"location":"api_reference/#foodspec.cli.protocol_benchmarks","title":"protocol_benchmarks","text":"<pre><code>protocol_benchmarks(\n    output_dir=typer.Option(\n        \"./protocol_benchmarks\",\n        help=\"Directory to write benchmark metrics.\",\n    ),\n    random_state=typer.Option(42, help=\"Random seed.\"),\n    config=typer.Option(\n        None, \"--config\", help=\"Optional YAML/JSON config.\"\n    ),\n)\n</code></pre> <p>Run protocol benchmarks on public datasets and save reports.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"protocol-benchmarks\")\ndef protocol_benchmarks(\n    output_dir: str = typer.Option(\"./protocol_benchmarks\", help=\"Directory to write benchmark metrics.\"),\n    random_state: int = typer.Option(42, help=\"Random seed.\"),\n    config: Optional[str] = typer.Option(None, \"--config\", help=\"Optional YAML/JSON config.\"),\n):\n    \"\"\"Run protocol benchmarks on public datasets and save reports.\"\"\"\n\n    base_cfg = {\"output_dir\": output_dir, \"random_state\": random_state}\n    cfg = load_config(config) if config else base_cfg\n    cfg = merge_cli_overrides(cfg, base_cfg)\n\n    out_path = Path(cfg[\"output_dir\"])\n    run_meta = log_run_metadata(logger, {\"command\": \"protocol-benchmarks\"})\n    summary = run_protocol_benchmarks(out_path, random_state=cfg.get(\"random_state\", random_state))\n    # write run metadata alongside metrics\n    meta_path = out_path / \"run_metadata.json\"\n    meta_path.parent.mkdir(parents=True, exist_ok=True)\n    meta_path.write_text(json.dumps(run_meta, indent=2), encoding=\"utf-8\")\n    typer.echo(\"Protocol benchmarks summary:\")\n    typer.echo(json.dumps(summary, indent=2))\n</code></pre>"},{"location":"api_reference/#foodspec.cli.qc_command","title":"qc_command","text":"<pre><code>qc_command(\n    input_hdf5=typer.Argument(\n        ..., help=\"Preprocessed spectra HDF5.\"\n    ),\n    model_type=typer.Option(\n        \"oneclass_svm\",\n        help=\"QC model type: oneclass_svm or isolation_forest.\",\n    ),\n    label_column=typer.Option(\n        None, help=\"Optional label column for inspection.\"\n    ),\n    output_dir=typer.Option(\n        \"./out\", help=\"Base output directory.\"\n    ),\n)\n</code></pre> <p>Run QC/novelty detection and write report.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"qc\")\ndef qc_command(\n    input_hdf5: str = typer.Argument(..., help=\"Preprocessed spectra HDF5.\"),\n    model_type: str = typer.Option(\"oneclass_svm\", help=\"QC model type: oneclass_svm or isolation_forest.\"),\n    label_column: Optional[str] = typer.Option(None, help=\"Optional label column for inspection.\"),\n    output_dir: str = typer.Option(\"./out\", help=\"Base output directory.\"),\n):\n    \"\"\"Run QC/novelty detection and write report.\"\"\"\n\n    ds = load_library(input_hdf5)\n    model = train_qc_model(ds, train_mask=None, model_type=model_type)\n    qc_result = apply_qc_model(ds, model=model, metadata=ds.metadata)\n    report_dir = _write_qc_report(qc_result, Path(output_dir), model_type=model_type, threshold=qc_result.threshold)\n    typer.echo(f\"QC report: {report_dir}\")\n</code></pre>"},{"location":"api_reference/#foodspec.cli.reproduce_methodsx","title":"reproduce_methodsx","text":"<pre><code>reproduce_methodsx(\n    output_dir=typer.Option(\n        \"./methodsx_runs\",\n        help=\"Directory to write MethodsX reproduction artifacts.\",\n    ),\n    random_state=typer.Option(42, help=\"Random seed.\"),\n)\n</code></pre> <p>Run MethodsX protocol reproduction analyses and write artifacts.</p> Source code in <code>src/foodspec/cli.py</code> <pre><code>@app.command(\"reproduce-methodsx\")\ndef reproduce_methodsx(\n    output_dir: str = typer.Option(\"./methodsx_runs\", help=\"Directory to write MethodsX reproduction artifacts.\"),\n    random_state: int = typer.Option(42, help=\"Random seed.\"),\n):\n    \"\"\"Run MethodsX protocol reproduction analyses and write artifacts.\"\"\"\n\n    run_meta = log_run_metadata(logger, {\"command\": \"reproduce-methodsx\"})\n    summary = run_methodsx_reproduction(output_dir=output_dir, random_state=random_state)\n    # Save run metadata\n    run_dir = Path(summary.get(\"run_dir\", output_dir))\n    run_dir.mkdir(parents=True, exist_ok=True)\n    (run_dir / \"run_metadata.json\").write_text(json.dumps(run_meta, indent=2), encoding=\"utf-8\")\n    typer.echo(\"MethodsX reproduction summary:\")\n    typer.echo(json.dumps(summary, indent=2))\n</code></pre>"},{"location":"api_reference/#navigation-tips","title":"Navigation tips","text":"<ul> <li>Concept-based lookup: see <code>keyword_index.md</code>.</li> <li>CLI usage: see <code>cli.md</code>.</li> </ul>"},{"location":"chemometrics_guide/","title":"Chemometrics &amp; models","text":"<p>This guide provides intuition for the main methods used in FoodSpec to extract information from spectra.</p>"},{"location":"chemometrics_guide/#pca-principal-component-analysis","title":"PCA (Principal Component Analysis)","text":"<ul> <li>Purpose: reduce dimensionality, visualize clustering, and identify spectral regions driving variance.  </li> <li>Outputs: scores (sample coordinates in PC space) and loadings (weights per wavenumber).  </li> <li>Explained variance: fraction of total variance captured by PCs; use scree plots to pick PCs.  </li> <li>Interpretation: clusters in score plots suggest similarity; loadings show which bands contribute most.</li> </ul>"},{"location":"chemometrics_guide/#pls-pls-da","title":"PLS / PLS-DA","text":"<ul> <li>PLS (regression) links spectra (X) to a continuous response (y), maximizing covariance.  </li> <li>PLS-DA combines PLS projection with a classifier (e.g., logistic regression).  </li> <li>Good for correlated predictors and modest sample sizes; watch for overfitting\u2014use cross-validation.</li> </ul>"},{"location":"chemometrics_guide/#classifiers-intuitive","title":"Classifiers (intuitive)","text":"<ul> <li>Logistic regression: linear boundary; fast baseline.  </li> <li>SVM (linear/RBF): maximizes margin; RBF handles nonlinear separations.  </li> <li>Random forest: ensemble of trees; captures nonlinearities and can rank feature importance.  </li> <li>kNN: instance-based; sensitive to scaling and class imbalance. Choose based on data size, linearity, and need for interpretability.</li> </ul>"},{"location":"chemometrics_guide/#mixture-models-nnls-mcr-als","title":"Mixture models (NNLS, MCR-ALS)","text":"<ul> <li>NNLS: for one mixture spectrum (\\mathbf{x}) and pure spectra matrix (\\mathbf{S}), solve (\\mathbf{x} \\approx \\mathbf{S}\\mathbf{c}) with (c_i \\ge 0). Coefficients (\\mathbf{c}) are estimated fractions.  </li> <li>MCR-ALS: for multiple mixtures matrix (\\mathbf{X}), factorize (\\mathbf{X} \\approx \\mathbf{C}\\mathbf{S}^\\top) iteratively with non-negativity. Retrieves concentrations (\\mathbf{C}) and pure-like spectra (\\mathbf{S}).</li> </ul>"},{"location":"chemometrics_guide/#validation-helpers","title":"Validation helpers","text":"<ul> <li>Cross-validation (stratified for classification) to estimate generalization.</li> <li>Metrics: accuracy, F1, confusion matrices for classification; R\u00b2/RMSE for regression/mixture.</li> <li>Permutation tests (if used) to assess significance by label shuffling.</li> </ul>"},{"location":"chemometrics_guide/#practical-guidelines","title":"Practical guidelines","text":"<ul> <li>Always keep preprocessing identical between train/test.</li> <li>Stratify when classes are imbalanced; report per-class metrics.</li> <li>Inspect residuals for regression/mixture tasks to detect bias.</li> <li>Prefer simpler models if performance is similar\u2014easier to explain and reproduce.</li> </ul>"},{"location":"citing/","title":"Citing foodspec","text":"<p>If you use foodspec in your research, we kindly ask you to cite:</p> <ol> <li>The foodspec software (this package).  </li> <li>The MethodsX protocol article that formally describes the workflow    (once it is published).</li> </ol>"},{"location":"citing/#software-citation","title":"Software citation","text":"<p>Citation metadata for the foodspec software is provided in the <code>CITATION.cff</code> file at the root of the repository. Many reference managers and tools can import this file directly.</p> <p>A provisional human-readable citation is:</p> <p>Chandrasekar Subramani Narayan, foodspec: A Python toolkit for Raman and FTIR spectroscopy in food science, software version 0.2.0, 2025.</p> <p>Please update this with the exact release details that you are using.</p>"},{"location":"citing/#methodsx-protocol-article","title":"MethodsX protocol article","text":"<p>Once the MethodsX article describing the computational protocol is accepted, this page will be updated with the formal citation (authors, title, journal, volume, pages, DOI).</p> <p>Until then, you can refer to the software citation and describe the method as \u201cthe foodspec protocol for Raman/FTIR analysis in food science\u201d.</p>"},{"location":"cli/","title":"CLI reference","text":"<p>Questions this page answers - What commands does foodspec provide? - What arguments/options do they take? - What outputs should I expect? - How do I troubleshoot common CLI issues?</p>"},{"location":"cli/#about","title":"about","text":"<ul> <li>Synopsis: <code>foodspec about</code></li> <li>Purpose: print foodspec/Python versions, optional extras status.</li> </ul>"},{"location":"cli/#preprocess","title":"preprocess","text":"<ul> <li>Synopsis: <code>foodspec preprocess INPUT_FOLDER OUTPUT_HDF5 [--metadata-csv PATH] [--modality MOD] [--min-wn VAL --max-wn VAL]</code></li> <li>Required: <code>INPUT_FOLDER</code>, <code>OUTPUT_HDF5</code></li> <li>Common options: modality (<code>raman</code>/<code>ftir</code>), wavenumber crop, metadata CSV.</li> <li>Example:   <code>bash   foodspec preprocess data/raw_txt libraries/preproc.h5 --metadata-csv data/meta.csv --modality raman --min-wn 600 --max-wn 1800</code></li> </ul>"},{"location":"cli/#csv-to-library","title":"csv-to-library","text":"<ul> <li>Synopsis: <code>foodspec csv-to-library CSV_PATH OUTPUT_HDF5 [--format wide|long] [--wavenumber-column COL] [--sample-id-column COL] [--intensity-column COL] [--label-column COL] [--modality MOD]</code></li> <li>Purpose: convert wide/long CSV to HDF5 library.</li> <li>Example:   <code>bash   foodspec csv-to-library data/oils.csv libraries/oils.h5 --format wide --wavenumber-column wavenumber --label-column oil_type --modality raman</code></li> </ul>"},{"location":"cli/#oil-auth","title":"oil-auth","text":"<ul> <li>Synopsis: <code>foodspec oil-auth INPUT_HDF5 [--label-column COL] [--classifier-name NAME] [--cv-splits N] [--output-dir DIR]</code></li> <li>Purpose: oil authentication classification workflow.</li> <li>Example:   <code>bash   foodspec oil-auth libraries/oils.h5 --label-column oil_type --classifier-name rf --cv-splits 5 --output-dir runs/oils</code></li> <li>Outputs: metrics JSON/CSV, confusion_matrix.png, report.md.</li> </ul>"},{"location":"cli/#heating","title":"heating","text":"<ul> <li>Synopsis: <code>foodspec heating INPUT_HDF5 [--time-column COL] [--output-dir DIR]</code></li> <li>Purpose: heating degradation ratios vs time with trend/ANOVA.</li> <li>Example:   <code>bash   foodspec heating libraries/heating.h5 --time-column heating_time --output-dir runs/heating</code></li> </ul>"},{"location":"cli/#qc","title":"qc","text":"<ul> <li>Synopsis: <code>foodspec qc INPUT_HDF5 [--model-type oneclass_svm|isolation_forest] [--label-column COL] [--output-dir DIR]</code></li> <li>Purpose: novelty/quality-control scoring.</li> <li>Example:   <code>bash   foodspec qc libraries/oils.h5 --model-type oneclass_svm --output-dir runs/qc</code></li> </ul>"},{"location":"cli/#domains","title":"domains","text":"<ul> <li>Synopsis: <code>foodspec domains INPUT_HDF5 --type {dairy,meat,microbial} [--label-column COL] [--classifier-name NAME] [--cv-splits N] [--output-dir DIR]</code></li> <li>Purpose: domain templates reusing oil-style workflow.</li> </ul>"},{"location":"cli/#mixture","title":"mixture","text":"<ul> <li>Synopsis: <code>foodspec mixture INPUT_HDF5 --pure-hdf5 PURE_PATH [--mode nnls|mcr_als] [--spectrum-index IDX] [--output-dir DIR]</code></li> <li>Purpose: NNLS or MCR-ALS mixture analysis.</li> </ul>"},{"location":"cli/#hyperspectral","title":"hyperspectral","text":"<ul> <li>Synopsis: <code>foodspec hyperspectral INPUT_HDF5 --height H --width W --target-wavenumber WN [--window VAL] [--output-dir DIR]</code></li> <li>Purpose: build hyperspectral cube and plot intensity map.</li> </ul>"},{"location":"cli/#protocol-benchmarks","title":"protocol-benchmarks","text":"<ul> <li>Synopsis: <code>foodspec protocol-benchmarks --output-dir DIR</code></li> <li>Purpose: run reference benchmarks on public datasets; emits metrics/report.</li> </ul>"},{"location":"cli/#reproduce-methodsx","title":"reproduce-methodsx","text":"<ul> <li>Synopsis: <code>foodspec reproduce-methodsx --output-dir DIR</code></li> <li>Purpose: run MethodsX reproduction (classification + mixture + PCA) and save artifacts.</li> </ul>"},{"location":"cli/#model-info","title":"model-info","text":"<ul> <li>Synopsis: <code>foodspec model-info MODEL_BASEPATH</code></li> <li>Purpose: print saved model metadata from model registry.</li> </ul>"},{"location":"cli/#troubleshooting-cli","title":"Troubleshooting CLI","text":"<ul> <li>Missing HDF5 or bad path: check file existence and permissions.</li> <li>Invalid CSV format: ensure correct <code>--format</code> and column names; wavenumber must be numeric/monotonic.</li> <li>Wrong label column: confirm metadata column name; use <code>--label-column</code>.</li> <li>Small class sizes: reduce <code>--cv-splits</code> if classes have very few samples.</li> <li>For detailed errors, inspect report folder (<code>summary.json</code>, <code>metrics.json</code>, <code>report.md</code>) and logs printed to stdout.</li> </ul> <p>See also - csv_to_library.md - oil_auth_tutorial.md - keyword_index.md - ftir_raman_preprocessing.md</p>"},{"location":"config_logging/","title":"Configuration &amp; logging","text":""},{"location":"config_logging/#config-files","title":"Config files","text":"<ul> <li>Supported formats: YAML (<code>.yml/.yaml</code>) and JSON (<code>.json</code>).</li> <li>Load with <code>load_config(path)</code>; CLI commands accept <code>--config</code> to apply a base config.</li> <li>CLI overrides: values passed on the command line overwrite config entries (via <code>merge_cli_overrides</code>).</li> <li>Typical keys: <code>input_hdf5</code>, <code>output_dir</code>, <code>label_column</code>, <code>classifier_name</code>, <code>cv_splits</code>.</li> </ul>"},{"location":"config_logging/#logging","title":"Logging","text":"<ul> <li>foodspec uses lightweight logging to record environment info (versions, platform, timestamp).</li> <li>Run metadata may be written alongside reports (e.g., <code>run_metadata.json</code>).</li> <li>If you want more verbosity, configure Python logging before calling foodspec APIs.</li> <li>When debugging, check the timestamped run directory for summary.json/metrics.json and logs emitted to stdout/stderr by CLI commands.</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>FoodSpec is a research project led by Chandrasekar Subramani Narayan. External collaborators are welcome; please align with the style and testing expectations to keep the protocol robust.</p>"},{"location":"contributing/#how-to-propose-changes","title":"How to propose changes","text":"<ul> <li>Open an issue to discuss new features or bugs before a major change.</li> <li>Fork and submit PRs for code/docs improvements; keep changes focused and documented.</li> </ul>"},{"location":"contributing/#development-setup","title":"Development setup","text":"<pre><code>git clone https://github.com/chandrasekarnarayana/foodspec.git\ncd foodspec\npip install -e \".[dev]\"\n</code></pre> <p>Run formatting/linting/tests before PRs:</p> <pre><code>black .\nruff check .\npytest\n</code></pre>"},{"location":"contributing/#style-and-scope","title":"Style and scope","text":"<ul> <li>Follow existing type hints and sklearn-like patterns.</li> <li>Keep tests deterministic (no network) and add coverage for new public APIs.</li> <li>Update docs when adding user-facing features or CLI flags.</li> </ul> <p>Contact: chandrasekarnarayana@gmail.com for coordination or questions.</p>"},{"location":"csv_to_library/","title":"CSV \u2192 HDF5 pipeline","text":"<p>foodspec converts CSVs to reusable spectral libraries (HDF5) for all workflows.</p>"},{"location":"csv_to_library/#supported-inputs","title":"Supported inputs","text":"Format Description Required columns Typical use CSV (wide) One column per spectrum, one row per wavenumber <code>wavenumber</code>, sample columns Fast conversion to HDF5 CSV (long/tidy) One row per (sample_id, wavenumber, intensity) <code>sample_id</code>, <code>wavenumber</code>, <code>intensity</code> Public datasets, tidy data Folder of TXT/CSV One file per spectrum, aligned axes filename, wavenumber/intensity columns Instrument exports HDF5 library Serialized FoodSpectrumSet x, wavenumbers, metadata, modality Primary format for workflows"},{"location":"csv_to_library/#csv-layouts","title":"CSV layouts","text":""},{"location":"csv_to_library/#wide-format","title":"Wide format","text":"<pre><code>wavenumber,s1,s2,s3\n500,10.1,12.2,9.9\n502,10.3,12.4,10.0\n</code></pre> <ul> <li>wavenumber is the axis; each other column is a spectrum.</li> </ul>"},{"location":"csv_to_library/#longtidy-format","title":"Long/tidy format","text":"<pre><code>sample_id,wavenumber,intensity,oil_type\ns001,500,10.1,olive\ns001,502,10.3,olive\ns002,500,12.2,sunflower\n</code></pre> <ul> <li>one row per (sample, wavenumber); extra columns become metadata (e.g., oil_type).</li> </ul>"},{"location":"csv_to_library/#cli-csv-to-library","title":"CLI: csv-to-library","text":"<pre><code>foodspec csv-to-library \\\n  data/oils.csv \\\n  libraries/oils.h5 \\\n  --format wide \\\n  --wavenumber-column wavenumber \\\n  --modality raman \\\n  --label-column oil_type\n</code></pre> <ul> <li>Use <code>--format long</code> with <code>--sample-id-column</code> / <code>--intensity-column</code> for tidy data.</li> <li>Parent directories are created automatically; output is an HDF5 library.</li> </ul>"},{"location":"csv_to_library/#python-api","title":"Python API","text":"<pre><code>from foodspec.io.csv_import import load_csv_spectra\nfrom foodspec.io import create_library\n\nfs = load_csv_spectra(\"data/oils.csv\", format=\"wide\", modality=\"raman\")\ncreate_library(\"libraries/oils.h5\", spectra=fs.x, wavenumbers=fs.wavenumbers, metadata=fs.metadata, modality=fs.modality)\n</code></pre>"},{"location":"csv_to_library/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Missing column errors: check spelling of wavenumber/sample_id/intensity.</li> <li>Non-monotonic axis: sort wavenumbers before conversion.</li> <li>Mixed labels: ensure consistent sample IDs and metadata rows.</li> </ul>"},{"location":"design_overview/","title":"Design overview","text":"<p>Status: Archived This page summarizes an earlier architectural view. The current design reference is design/01_overview.md.</p>"},{"location":"domains_overview/","title":"Domains overview","text":"<p>Questions this page answers - How do domain templates relate to core workflows? - Which template should I use for oils, meat, or microbial data? - How do I run them via CLI/Python?</p>"},{"location":"domains_overview/#what-are-domain-templates","title":"What are domain templates?","text":"<p>Domain templates are thin wrappers around the core workflows (oil-auth, heating, QC) pre-configured for specific food types (oils, meat, microbial, etc.). They reuse the same preprocessing, feature extraction, and chemometrics to keep results consistent and reproducible.</p>"},{"location":"domains_overview/#when-to-use-templates-vs-raw-workflows","title":"When to use templates vs raw workflows","text":"<ul> <li>Use a template when your task matches a provided domain (e.g., meat authentication with a label column) and you want sensible defaults with minimal setup.</li> <li>Use raw workflows (oil-auth/heating/qc) when you need full control over preprocessing, labels, or model choices beyond the templates.</li> </ul>"},{"location":"domains_overview/#domain-quick-reference","title":"Domain quick reference","text":"Domain Typical task Recommended workflow Example CLI command Oils Multi-class authentication Oil-auth workflow <code>foodspec oil-auth libraries/oils.h5 --label-column oil_type</code> Meat Multi-class meat type QC/auth Domains (meat template) <code>foodspec domains libraries/meat.h5 --type meat --label-column meat_type</code> Microbial Species/strain identification Domains (microbial template) <code>foodspec domains libraries/microbial.h5 --type microbial --label-column species</code> <p>See also - <code>meat_tutorial.md</code> - <code>microbial_tutorial.md</code> - <code>oil_auth_tutorial.md</code> - <code>metrics_interpretation.md</code></p>"},{"location":"ftir_raman_preprocessing/","title":"Raman/FTIR preprocessing guide","text":"<p>Questions this page answers - Why preprocess Raman/FTIR spectra? - Which baseline, smoothing, normalization, and helpers should I use? - How do I configure them in Python and CLI?</p>"},{"location":"ftir_raman_preprocessing/#why-preprocessing-matters","title":"Why preprocessing matters","text":"<ul> <li>Baseline: removes fluorescence (Raman) or sloping background (FTIR).</li> <li>Smoothing: reduces noise while preserving peaks.</li> <li>Scatter/normalization: corrects intensity scaling, pathlength, and scatter differences.</li> <li>Derivatives: enhance subtle features and reduce baseline.</li> <li>FTIR/Raman helpers: handle ATR effects, atmospheric bands, and cosmic rays.</li> </ul>"},{"location":"ftir_raman_preprocessing/#baseline-correction","title":"Baseline correction","text":"<ul> <li>ALSBaseline: general-purpose baseline removal; tune <code>lambda_</code>, <code>p</code>.</li> <li>RubberbandBaseline: convex-hull baseline; useful for concave backgrounds.</li> <li>PolynomialBaseline: low-degree fit for globally smooth baselines.</li> </ul>"},{"location":"ftir_raman_preprocessing/#smoothing","title":"Smoothing","text":"<ul> <li>Savitzky\u2013Golay: preserves peak shape; choose odd window length, polyorder &lt; window.</li> <li>MovingAverageSmoother: simple denoising; may broaden peaks.</li> </ul>"},{"location":"ftir_raman_preprocessing/#scatter-normalization","title":"Scatter &amp; normalization","text":"<ul> <li>Vector/Area/Max normalizers: scale spectra to unit norm/area; remove overall intensity differences.</li> <li>SNVNormalizer: subtract mean, divide by std per spectrum; removes additive/multiplicative scatter.</li> <li>MSCNormalizer: regress onto reference (mean spectrum) and correct slope/intercept; good for scatter variation.</li> <li>InternalPeakNormalizer: normalize to a stable internal band (mean intensity in a window).</li> </ul>"},{"location":"ftir_raman_preprocessing/#derivatives","title":"Derivatives","text":"<ul> <li>DerivativeTransformer: Savitzky\u2013Golay derivatives (1st/2nd) to emphasize subtle bands and suppress baseline.</li> </ul>"},{"location":"ftir_raman_preprocessing/#ftirraman-specific-helpers","title":"FTIR/Raman-specific helpers","text":"<ul> <li>AtmosphericCorrector: subtracts water/CO\u2082 components (FTIR).</li> <li>SimpleATRCorrector: heuristic ATR depth correction (FTIR).</li> <li>CosmicRayRemover: detects/replaces spikes (Raman).</li> </ul>"},{"location":"ftir_raman_preprocessing/#example-pipeline-python","title":"Example pipeline (Python)","text":"<pre><code>from sklearn.pipeline import Pipeline\nfrom foodspec.preprocess.baseline import ALSBaseline\nfrom foodspec.preprocess.smoothing import SavitzkyGolaySmoother\nfrom foodspec.preprocess.normalization import VectorNormalizer\nfrom foodspec.preprocess.cropping import RangeCropper\n\npipe = Pipeline([\n    (\"als\", ALSBaseline(lambda_=1e5, p=0.01, max_iter=10)),\n    (\"savgol\", SavitzkyGolaySmoother(window_length=9, polyorder=3)),\n    (\"norm\", VectorNormalizer(norm=\"l2\")),\n    (\"crop\", RangeCropper(min_wn=600, max_wn=1800)),\n])\nX_proc = pipe.fit_transform(fs.x)\n</code></pre>"},{"location":"ftir_raman_preprocessing/#cli-usage","title":"CLI usage","text":"<p>Most workflows (oil-auth, heating) include defaults. For raw folders:</p> <pre><code>foodspec preprocess raw_folder out.h5 --modality raman --min-wn 600 --max-wn 1800\n</code></pre> <p>See also - <code>keyword_index.md</code> - <code>oil_auth_tutorial.md</code> - <code>heating_tutorial.md</code> - <code>keyword_index.md</code></p>"},{"location":"getting_started/","title":"Getting started","text":"<p>Questions this page answers - Who is foodspec for? - How do I install it (core vs deep extra)? - What data formats and metadata does it expect? - What is the typical pipeline and why? - Where do I go for full Python/CLI quickstarts?</p>"},{"location":"getting_started/#who-is-it-for","title":"Who is it for?","text":"<p>Food scientists, analytical chemists, QC engineers, and data scientists working with Raman/FTIR spectra who need reproducible preprocessing, chemometrics, and reporting.</p>"},{"location":"getting_started/#installation","title":"Installation","text":"<ul> <li>Core:   <code>bash   pip install foodspec</code></li> <li>Deep-learning extra (optional 1D CNN prototype):   <code>bash   pip install \"foodspec[deep]\"</code></li> <li>Verify:   <code>bash   foodspec about</code></li> </ul>"},{"location":"getting_started/#data-formats-and-metadata","title":"Data formats and metadata","text":"<ul> <li>Instrument exports: commercial Raman/FTIR instruments often export per-spectrum TXT/CSV files (wavenumber/intensity columns) or wide CSVs (one column per spectrum).  </li> <li>FoodSpec standard: convert to a validated <code>FoodSpectrumSet</code> (HDF5 library) with:</li> <li><code>x</code>: spectra matrix (n_samples \u00d7 n_wavenumbers)</li> <li><code>wavenumbers</code>: monotonic axis (cm\u207b\u00b9)</li> <li><code>metadata</code>: one row per sample (e.g., <code>oil_type</code>, <code>meat_type</code>, <code>species</code>, <code>heating_time</code>)</li> <li><code>modality</code>: <code>raman</code>/<code>ftir</code>/<code>nir</code></li> <li>Why this protocol? Keeps spectra + metadata together, enables reproducible preprocessing/models, and matches downstream workflows (oil-auth, heating, QC).</li> </ul>"},{"location":"getting_started/#typical-pipeline-text-diagram","title":"Typical pipeline (text diagram)","text":"<p>Raw spectra (instrument CSV/TXT) \u2192 CSV\u2192HDF5 library \u2192 Preprocess (baseline, smoothing, normalization, crop) \u2192 Features/chemometrics (peaks/ratios/PCA/PLS/models) \u2192 Metrics &amp; reports (plots, JSON/Markdown).</p>"},{"location":"getting_started/#minimal-examples-stepwise","title":"Minimal examples (stepwise)","text":"<p>For full code, see the dedicated quickstarts. Highlights:</p>"},{"location":"getting_started/#python-steps","title":"Python (steps)","text":"<p>1) Load library &amp; validate. 2) Apply simple preprocessing (ALS baseline \u2192 Savitzky\u2013Golay \u2192 Vector norm). 3) Run PCA for a quick check.</p> <pre><code>from pathlib import Path\nimport matplotlib.pyplot as plt\nfrom foodspec.data import load_library\nfrom foodspec.preprocess.baseline import ALSBaseline\nfrom foodspec.preprocess.smoothing import SavitzkyGolaySmoother\nfrom foodspec.preprocess.normalization import VectorNormalizer\nfrom foodspec.chemometrics.pca import run_pca\nfrom foodspec.validation import validate_spectrum_set\n\nfs = load_library(Path(\"libraries/oils_demo.h5\"))\nvalidate_spectrum_set(fs)\n\nX = fs.x\nfor step in [ALSBaseline(lambda_=1e5, p=0.01, max_iter=10),\n             SavitzkyGolaySmoother(window_length=9, polyorder=3),\n             VectorNormalizer(norm=\"l2\")]:\n    X = step.fit_transform(X)\n\n_, pca_res = run_pca(X, n_components=2)\nplt.scatter(pca_res.scores[:, 0], pca_res.scores[:, 1]); plt.tight_layout()\nplt.savefig(\"pca_scores.png\", dpi=150)\n</code></pre>"},{"location":"getting_started/#cli-steps","title":"CLI (steps)","text":"<p>1) Convert CSV (wide example) to HDF5:</p> <pre><code>foodspec csv-to-library data/oils.csv libraries/oils.h5 \\\n  --format wide --wavenumber-column wavenumber \\\n  --label-column oil_type --modality raman\n</code></pre> <p>2) Run oil authentication:</p> <pre><code>foodspec oil-auth libraries/oils.h5 \\\n  --label-column oil_type \\\n  --output-dir runs/oils_demo\n</code></pre> <p>Outputs: metrics.json/CSV, confusion_matrix.png, report.md in a timestamped folder.</p>"},{"location":"getting_started/#quickstarts","title":"Quickstarts","text":"<ul> <li>Full CLI walkthrough: quickstart_cli.md</li> <li>Full Python walkthrough: quickstart_python.md</li> </ul>"},{"location":"getting_started/#links","title":"Links","text":"<ul> <li>Libraries &amp; formats: libraries.md, csv_to_library.md</li> <li>Workflows: oil authentication, heating, mixture, hyperspectral, QC</li> <li>User guide: CLI reference (cli.md), preprocessing (ftir_raman_preprocessing.md)</li> <li>Keyword lookup: keyword_index.md</li> </ul> <p>See also - oil_auth_tutorial.md - heating_tutorial.md - csv_to_library.md</p>"},{"location":"heating_tutorial/","title":"Heating degradation workflow","text":"<p>Questions this page answers - How do heating time/temperature affect oil spectra? - How do I run the heating workflow (Python and CLI)? - How do I interpret ratio trends, slopes, and ANOVA? - How should I report these results?</p>"},{"location":"heating_tutorial/#experiment-setup","title":"Experiment setup","text":"<ul> <li>Input: spectra with metadata column (e.g., <code>heating_time</code> or <code>temperature</code>); optional <code>oil_type</code>.</li> <li>Goal: track degradation via ratios (e.g., 1655/1742) over time/cycles.</li> </ul>"},{"location":"heating_tutorial/#running-the-workflow","title":"Running the workflow","text":"<p>CLI:</p> <pre><code>foodspec heating libraries/heating.h5 \\\n  --time-column heating_time \\\n  --output-dir runs/heating_demo\n</code></pre> <p>Outputs: ratios.csv, ratio_vs_time.png, optional anova.csv, report.md.</p> <p>Python:</p> <pre><code>from foodspec.data import load_library\nfrom foodspec.apps.heating import run_heating_degradation_analysis\nfs = load_library(\"libraries/heating.h5\")\nres = run_heating_degradation_analysis(fs, time_column=\"heating_time\")\nprint(res.key_ratios.head())\n</code></pre>"},{"location":"heating_tutorial/#interpreting-results","title":"Interpreting results","text":"<ul> <li>Ratios vs time: slope sign/magnitude indicates increasing/decreasing features (e.g., unsaturation loss).  </li> <li>Trend model: simple linear fit ( r = a + b t ); slope ( b ) is key metric.  </li> <li>ANOVA (if group labels): p-value tests differences between groups (e.g., oil types).</li> <li>Look for consistent trends across batches; inspect variance.</li> </ul>"},{"location":"heating_tutorial/#reporting","title":"Reporting","text":"<ul> <li>Main figures: ratio_vs_time plot with fitted line; note slope and any significant p-values.  </li> <li>Main text: preprocessing summary, ratio definition, slope/correlation, ANOVA if applicable.  </li> <li>Supplementary: full ratios table, ANOVA table, spectra before/after heating, run metadata/configs.</li> <li>Supporting tests (conceptual): peroxide/anisidine values, GC\u2013MS, sensory panel for corroboration.</li> </ul>"},{"location":"heating_tutorial/#optional-testing-degradation-trends","title":"Optional: testing degradation trends","text":"<p>Test whether a ratio changes significantly over time using simple linear regression.</p> <pre><code>import pandas as pd\nfrom scipy.stats import linregress\n\n# df_ratios has columns: ratio_1655_1745, heating_time\nslope, intercept, r, p, stderr = linregress(df_ratios[\"heating_time\"], df_ratios[\"ratio_1655_1745\"])\nprint(f\"slope={slope:.3f}, R\u00b2={r**2:.3f}, p={p:.3g}\")\n</code></pre> <p>Interpretation: slope indicates direction/magnitude of change; p tests if slope differs from zero; R\u00b2 shows how much of the ratio variance is explained by time. Include these in MethodsX-style reports alongside plots.</p> <p>See also - metrics_interpretation.md - reporting_guidelines.md - keyword_index.md - ftir_raman_preprocessing.md</p>"},{"location":"hyperspectral_tutorial/","title":"Hyperspectral analysis","text":""},{"location":"hyperspectral_tutorial/#what-is-a-hyperspectral-cube","title":"What is a hyperspectral cube?","text":"<ul> <li>A 3D array <code>(height, width, n_wavenumbers)</code> where each pixel has a full spectrum.</li> <li>FoodSpec wraps this as <code>HyperSpectralCube</code>, convertible to/from <code>FoodSpectrumSet</code> (flattened pixels with row/col metadata).</li> </ul>"},{"location":"hyperspectral_tutorial/#workflow","title":"Workflow","text":"<ol> <li>Load or construct per-pixel spectra (flattened) + image shape.  </li> <li>Build a cube: <code>cube = HyperSpectralCube.from_spectrum_set(fs_pixels, image_shape=(H, W))</code>.  </li> <li>Visualization:</li> <li>Intensity map at a target wavenumber: <code>plot_hyperspectral_intensity_map(cube, target_wavenumber=1655, window=5)</code>.</li> <li>Ratio map: <code>plot_ratio_map(cube, num1=1655, num2=1742)</code>.</li> <li>Cluster map: apply clustering/labels and use <code>plot_cluster_map</code>.</li> </ol>"},{"location":"hyperspectral_tutorial/#example-cli","title":"Example (CLI)","text":"<pre><code>foodspec hyperspectral \\\n  libraries/hyperspectral_pixels.h5 \\\n  --height 20 --width 20 \\\n  --target-wavenumber 1655 \\\n  --window 5 \\\n  --output-dir runs/hyper_demo\n</code></pre> <p>Outputs: intensity_map.png, summary.json, optional mean spectrum CSV.</p>"},{"location":"hyperspectral_tutorial/#example-python","title":"Example (Python)","text":"<pre><code>from foodspec.core.hyperspectral import HyperSpectralCube\nfrom foodspec.viz.hyperspectral import plot_hyperspectral_intensity_map, plot_ratio_map\ncube = HyperSpectralCube.from_spectrum_set(fs_pixels, image_shape=(20, 20))\nplot_hyperspectral_intensity_map(cube, target_wavenumber=1655, window=5)\nplot_ratio_map(cube, num1=1655, num2=1742)\n</code></pre>"},{"location":"hyperspectral_tutorial/#interpretation-and-reporting","title":"Interpretation and reporting","text":"<ul> <li>Intensity/ratio maps show spatial distribution of components or contaminants; hotspots may indicate adulteration or uneven composition.</li> <li>Cluster maps summarize segmentation (e.g., phases or contaminants).</li> <li>Main figure: key intensity/ratio map with color bar and scale.  </li> <li>Supplementary: representative pixel spectra, mean spectrum, cluster map variations, and any preprocessing notes (cropping/smoothing applied to pixels).</li> </ul> <p>See also - metrics_interpretation.md - keyword_index.md - api_reference.md</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or 3.11 (recommended).</li> <li>Typical scientific stack: NumPy, SciPy, scikit-learn, pandas, matplotlib, h5py (installed as dependencies).</li> </ul>"},{"location":"installation/#user-installation","title":"User installation","text":"<pre><code>pip install foodspec\n</code></pre> <p>Verify:</p> <pre><code>foodspec about\n</code></pre>"},{"location":"installation/#optional-extras","title":"Optional extras","text":"<ul> <li>Deep learning (1D CNN prototype): <code>bash   pip install \"foodspec[deep]\"</code>   Calling <code>Conv1DSpectrumClassifier</code> without TensorFlow installed will raise a clear ImportError suggesting this extra.</li> </ul>"},{"location":"installation/#developer-installation","title":"Developer installation","text":"<pre><code>git clone https://github.com/chandrasekarnarayana/foodspec.git\ncd foodspec\npip install -e \".[dev]\"\n</code></pre> <p>Run tests to confirm:</p> <pre><code>pytest\n</code></pre>"},{"location":"keyword_index/","title":"Keyword index &amp; glossary","text":"<p>You are here: Reference &amp; index \u2192 Keyword index &amp; glossary</p> <p>Questions this page answers - Where do I find a concept (preprocessing method, test, model, metric, workflow, CLI command)? - Which docs and API pages explain it?</p>"},{"location":"keyword_index/#spectral-preprocessing","title":"Spectral preprocessing","text":"<ul> <li>ALSBaseline (ALS baseline correction) \u2014 removes fluorescence/sloping background. See: <code>ftir_raman_preprocessing.md</code>, <code>api_reference.md#foodspec.preprocess.baseline.ALSBaseline</code>.</li> <li>RubberbandBaseline \u2014 convex-hull baseline for concave backgrounds. See: <code>ftir_raman_preprocessing.md</code>, <code>api_reference.md#foodspec.preprocess.baseline.RubberbandBaseline</code>.</li> <li>PolynomialBaseline \u2014 low-degree baseline fit. See: <code>ftir_raman_preprocessing.md</code>, <code>api_reference.md#foodspec.preprocess.baseline.PolynomialBaseline</code>.</li> <li>SavitzkyGolaySmoother (SavGol) \u2014 noise reduction preserving peaks. See: <code>ftir_raman_preprocessing.md</code>, <code>api_reference.md#foodspec.preprocess.smoothing.SavitzkyGolaySmoother</code>.</li> <li>MovingAverageSmoother \u2014 simple denoising; may broaden peaks. See: <code>ftir_raman_preprocessing.md</code>, <code>api_reference.md#foodspec.preprocess.smoothing.MovingAverageSmoother</code>.</li> <li>Vector/Area/Max normalization \u2014 scales spectra to unit norm/area. See: <code>ftir_raman_preprocessing.md</code>, <code>api_reference.md#foodspec.preprocess.normalization.VectorNormalizer</code>.</li> <li>SNVNormalizer (Standard Normal Variate) \u2014 mean/std per spectrum to reduce scatter. See: <code>ftir_raman_preprocessing.md</code>, <code>api_reference.md#foodspec.preprocess.normalization.SNVNormalizer</code>.</li> <li>MSCNormalizer (Multiplicative Scatter Correction) \u2014 corrects additive/multiplicative scatter via reference. See: <code>ftir_raman_preprocessing.md</code>, <code>api_reference.md#foodspec.preprocess.normalization.MSCNormalizer</code>.</li> <li>InternalPeakNormalizer \u2014 normalize to a stable internal band/window. See: <code>ftir_raman_preprocessing.md</code>, <code>api_reference.md#foodspec.preprocess.normalization.InternalPeakNormalizer</code>.</li> <li>DerivativeTransformer (derivatives) \u2014 Savitzky\u2013Golay derivatives (1st/2nd). See: <code>ftir_raman_preprocessing.md</code>, <code>api_reference.md#foodspec.preprocess.derivatives.DerivativeTransformer</code>.</li> <li>AtmosphericCorrector (FTIR) \u2014 remove water/CO\u2082 contributions. See: <code>ftir_raman_preprocessing.md</code>, <code>api_reference.md#foodspec.preprocess.ftir.AtmosphericCorrector</code>.</li> <li>SimpleATRCorrector (FTIR) \u2014 heuristic ATR depth correction. See: <code>ftir_raman_preprocessing.md</code>, <code>api_reference.md#foodspec.preprocess.ftir.SimpleATRCorrector</code>.</li> <li>CosmicRayRemover (Raman) \u2014 remove spike artifacts. See: <code>ftir_raman_preprocessing.md</code>, <code>api_reference.md#foodspec.preprocess.raman.CosmicRayRemover</code>.</li> <li>RangeCropper \u2014 crop to target wavenumber window. See: <code>ftir_raman_preprocessing.md</code>, <code>api_reference.md#foodspec.preprocess.cropping.RangeCropper</code>.</li> </ul>"},{"location":"keyword_index/#features-and-ratios","title":"Features and ratios","text":"<ul> <li>PeakFeatureExtractor / detect_peaks \u2014 peak heights/areas near expected bands. See: <code>oil_auth_tutorial.md</code>, <code>api_reference.md#foodspec.features.peaks.PeakFeatureExtractor</code>.</li> <li>integrate_bands \u2014 integrate intensity over defined bands. See: <code>mixture_tutorial.md</code>, <code>api_reference.md#foodspec.features.bands.integrate_bands</code>.</li> <li>RatioFeatureGenerator / compute_ratios \u2014 compute band/peak ratios (e.g., 1655/1742). See: <code>oil_auth_tutorial.md</code>, <code>api_reference.md#foodspec.features.ratios.RatioFeatureGenerator</code>.</li> <li>Fingerprint similarity (cosine/correlation) \u2014 spectral similarity matrices. See: <code>hyperspectral_tutorial.md</code>, <code>api_reference.md#foodspec.features.fingerprint</code>.</li> </ul>"},{"location":"keyword_index/#statistical-tests","title":"Statistical tests","text":"<ul> <li>t-tests (independent/paired/one-sample) \u2014 compare means. See: <code>stats_tests.md</code>.</li> <li>ANOVA / MANOVA \u2014 multi-group mean differences. See: <code>stats_tests.md</code>.</li> <li>Mann\u2013Whitney U / Kruskal\u2013Wallis / Wilcoxon / Friedman \u2014 non-parametric comparisons. See: <code>stats_tests.md</code>.</li> <li>Correlation (Pearson/Spearman) / simple regression \u2014 associations and trends. See: <code>stats_tests.md</code>.</li> </ul>"},{"location":"keyword_index/#machine-learning-models","title":"Machine learning models","text":"<ul> <li>Logistic regression / Linear SVM / PLS-DA \u2014 linear classifiers. See: <code>ml_models.md</code>, <code>api_reference.md#foodspec.chemometrics.models</code>.</li> <li>RBF SVM / k-NN / Random Forest (RF) / Gradient Boosting \u2014 nonlinear classifiers. See: <code>ml_models.md</code>, <code>api_reference.md#foodspec.chemometrics.models</code>.</li> <li>PCA / clustering \u2014 unsupervised exploration/visualization. See: <code>chemometrics_guide.md</code>, <code>api_reference.md#foodspec.chemometrics.pca</code>.</li> <li>Conv1DSpectrumClassifier (1D CNN) \u2014 optional deep model. See: <code>ml_models.md</code>, <code>api_reference.md#foodspec.chemometrics.deep.Conv1DSpectrumClassifier</code>.</li> <li>Mixture models (NNLS, MCR-ALS) \u2014 estimate component fractions. See: <code>mixture_tutorial.md</code>, <code>api_reference.md#foodspec.chemometrics.mixture</code>.</li> </ul>"},{"location":"keyword_index/#metrics-and-validation","title":"Metrics and validation","text":"<ul> <li>Accuracy, Precision, Recall, F1 (macro/micro), ROC-AUC, Confusion matrix \u2014 classification metrics. See: <code>metrics_interpretation.md</code>, <code>api_reference.md#foodspec.chemometrics.validation</code>.</li> <li>R\u00b2, RMSE, MAE, Residuals \u2014 regression/mixture metrics. See: <code>metrics_interpretation.md</code>, <code>api_reference.md#foodspec.chemometrics.validation</code>.</li> <li>Cross-validation (CV) \u2014 k-fold, stratified CV for models. See: <code>metrics_interpretation.md</code>, <code>api_reference.md#foodspec.chemometrics.validation</code>.</li> </ul>"},{"location":"keyword_index/#workflows","title":"Workflows","text":"<ul> <li>Oil authentication \u2014 classify oils/adulteration. See: <code>oil_auth_tutorial.md</code>, <code>methodsx_protocol.md</code>, <code>api_reference.md#foodspec.apps.oils</code>.</li> <li>Heating degradation \u2014 ratios vs time/temperature. See: <code>heating_tutorial.md</code>, <code>api_reference.md#foodspec.apps.heating</code>.</li> <li>Mixture analysis (NNLS/MCR-ALS) \u2014 estimate fractions. See: <code>mixture_tutorial.md</code>, <code>api_reference.md#foodspec.chemometrics.mixture</code> and <code>api_reference.md#foodspec.apps.methodsx_reproduction</code>.</li> <li>QC / Novelty detection \u2014 one-class scoring. See: <code>qc_tutorial.md</code>, <code>api_reference.md#foodspec.apps.qc</code>.</li> <li>Hyperspectral analysis \u2014 ratio/cluster maps. See: <code>hyperspectral_tutorial.md</code>, <code>api_reference.md#foodspec.core.hyperspectral.HyperSpectralCube</code>.</li> <li>Protocol benchmarks / MethodsX reproduction \u2014 standardized evaluation. See: <code>protocol_benchmarks.md</code>, <code>methodsx_protocol.md</code>, <code>api_reference.md#foodspec.apps.protocol_validation</code>, <code>api_reference.md#foodspec.apps.methodsx_reproduction</code>.</li> <li>Domain templates (meat/microbial) \u2014 adapt oil workflow to other domains. See: <code>domains_overview.md</code>, <code>meat_tutorial.md</code>, <code>microbial_tutorial.md</code>, <code>api_reference.md#foodspec.apps.meat</code>, <code>api_reference.md#foodspec.apps.microbial</code>.</li> </ul>"},{"location":"keyword_index/#cli-commands","title":"CLI commands","text":"<ul> <li>about \u2014 version/info. See: <code>cli.md</code>.</li> <li>csv-to-library / preprocess \u2014 build libraries, preprocess raw data. See: <code>cli.md</code>.</li> <li>oil-auth / heating / qc / domains \u2014 workflow commands. See: <code>cli.md</code>.</li> <li>mixture / hyperspectral \u2014 mixture and hyperspectral utilities. See: <code>cli.md</code>.</li> <li>protocol-benchmarks / reproduce-methodsx \u2014 protocol runs. See: <code>cli.md</code>.</li> <li>model-info \u2014 inspect saved model metadata. See: <code>cli.md</code>.</li> </ul> <p>See also - metrics_interpretation.md - oil_auth_tutorial.md - methodsx_protocol.md - api_reference.md</p>"},{"location":"libraries/","title":"! Libraries &amp; public datasets","text":"<p>Questions this page answers - What is stored in a FoodSpec HDF5 library? - How do I build/load libraries? - Which public datasets are used in the protocol? - How does CSV \u2192 FoodSpectrumSet \u2192 HDF5 work?</p>"},{"location":"libraries/#what-is-stored-in-an-hdf5-library","title":"What is stored in an HDF5 library?","text":"<ul> <li><code>x</code>: spectra matrix (n_samples \u00d7 n_wavenumbers)</li> <li><code>wavenumbers</code>: shared axis (cm\u207b\u00b9)</li> <li><code>metadata</code>: one row per sample (labels, acquisition info)</li> <li><code>modality</code>: Raman / FTIR / NIR tag</li> <li><code>provenance</code>: basic version/timestamp metadata</li> </ul>"},{"location":"libraries/#building-and-loading-libraries","title":"Building and loading libraries","text":"<pre><code>from pathlib import Path\nfrom foodspec.io import create_library\nfrom foodspec.data import load_library\n\ninput_folder = Path(\"data/oils_raw\")\nout_path = Path(\"libraries/oils_raman.h5\")\ncreate_library(input_folder, out_path, modality=\"raman\")\nfs = load_library(out_path)\n</code></pre>"},{"location":"libraries/#public-datasets-used-in-protocol-examples","title":"Public datasets (used in protocol examples)","text":"Dataset Modality Task Approx size DOI/URL Mendeley edible oils Raman/FTIR Classification Tens\u2013hundreds spectra Mendeley Data (edible oils) EVOO\u2013sunflower mixtures Raman Regression (fraction) Tens spectra DOI: 10.57745/DOGT0E FTIR edible oils (ATR) FTIR Classification/adulteration Tens spectra Public FTIR oil datasets <p>Loaders: <code>load_public_mendeley_oils</code>, <code>load_public_evoo_sunflower_raman</code>, <code>load_public_ftir_oils</code> (data must be pre-downloaded).</p>"},{"location":"libraries/#csv-foodspectrumset-hdf5","title":"CSV \u2192 FoodSpectrumSet \u2192 HDF5","text":"<ul> <li>Use <code>foodspec csv-to-library</code> for CSV inputs.</li> <li>Wide: one column per spectrum, <code>wavenumber</code> column for the axis.</li> <li>Long/tidy: rows = (sample_id, wavenumber, intensity), extra metadata columns allowed.</li> <li>Command:</li> </ul> <pre><code>foodspec csv-to-library data/oils.csv libraries/oils.h5 \\\n  --format wide \\\n  --wavenumber-column wavenumber \\\n  --label-column oil_type \\\n  --modality raman\n</code></pre> <ul> <li>Internally: CSV \u2192 FoodSpectrumSet (validated) \u2192 HDF5 with spectra, axis, metadata, modality, provenance.</li> </ul>"},{"location":"libraries/#public-loader-examples","title":"Public loader examples","text":"<pre><code>from foodspec.data import load_public_mendeley_oils, load_public_evoo_sunflower_raman\nfs_cls = load_public_mendeley_oils(root=\"path/to/mendeley\")\nfs_mix = load_public_evoo_sunflower_raman(root=\"path/to/evoo_sunflower\")\n</code></pre> <p>These return validated FoodSpectrumSet objects ready for workflows.</p> <p>See also - csv_to_library.md - oil_auth_tutorial.md - mixture_tutorial.md</p>"},{"location":"meat_tutorial/","title":"Meat domain template","text":"<p>Questions this page answers - How do I classify meat types/freshness with foodspec? - How do the domain templates map to the core oil-auth workflow? - What do CLI and Python runs look like?</p>"},{"location":"meat_tutorial/#use-case-and-data","title":"Use case and data","text":"<p>Assume a Raman/FTIR dataset of meat samples with label column <code>meat_type</code> (e.g., chicken, beef, pork) or freshness (<code>meat_label</code> such as fresh/spoiled). The meat template reuses the oil-auth pipeline (baseline, smoothing, normalization, cropping, peaks/ratios + classifier).</p>"},{"location":"meat_tutorial/#pipeline-outline","title":"Pipeline outline","text":"<p>1) CSV \u2192 HDF5 library (wide example):</p> <pre><code>foodspec csv-to-library data/meat.csv libraries/meat.h5 \\\n  --format wide \\\n  --wavenumber-column wavenumber \\\n  --label-column meat_type \\\n  --modality raman\n</code></pre> <p>2) Run classification via domain template (CLI):</p> <pre><code>foodspec domains \\\n  libraries/meat.h5 \\\n  --type meat \\\n  --label-column meat_type \\\n  --classifier-name rf \\\n  --output-dir runs/meat_demo\n</code></pre> <p>Outputs: CV metrics CSV, confusion_matrix.png, report.md, summary.json.</p>"},{"location":"meat_tutorial/#python-example","title":"Python example","text":"<pre><code>from foodspec.data import load_library\nfrom foodspec.apps.meat import run_meat_authentication_workflow\n\nfs = load_library(\"libraries/meat.h5\")\nres = run_meat_authentication_workflow(fs, label_column=\"meat_type\", classifier_name=\"rf\", cv_splits=5)\nprint(res.cv_metrics.head())\n</code></pre>"},{"location":"meat_tutorial/#interpretation-and-reporting","title":"Interpretation and reporting","text":"<ul> <li>Look for balanced accuracy/macro F1 across meat classes; inspect confusion matrix for specific confusions (e.g., similar cuts).</li> <li>Report main: overall accuracy/macro F1 + confusion matrix; supplementary: per-class precision/recall, feature importances/ratios.</li> <li>Cross-validation reduces bias; mention folds/stratification.</li> </ul> <p>See also - domains_overview.md - oil_auth_tutorial.md - metrics_interpretation.md - methodsx_protocol.md</p>"},{"location":"methodsx_protocol/","title":"MethodsX protocol reproduction","text":"<p>This page maps the MethodsX protocol paper directly to FoodSpec commands and outputs so that all figures and tables can be reproduced from public datasets.</p>"},{"location":"methodsx_protocol/#command","title":"Command","text":"<pre><code>foodspec reproduce-methodsx --output-dir runs/methodsx_protocol\n</code></pre> <p>Produces a timestamped run directory with metrics.json, run_metadata.json, confusion_matrix/PCA plots, and report.md.</p>"},{"location":"methodsx_protocol/#datasets-public","title":"Datasets (public)","text":"<ol> <li>Mendeley edible oils (Raman/FTIR) \u2013 multi-class classification.  </li> <li>EVOO\u2013sunflower Raman mixtures (data.gouv.fr, DOI 10.57745/DOGT0E) \u2013 fraction regression/mixture analysis.  </li> <li>Groundnut adulteration ATR-MIR (Kaggle) \u2013 optional robustness/adulteration check.  </li> </ol> <p>Convert each to HDF5 using <code>foodspec csv-to-library</code> or the public loaders; see Libraries for folder structure.</p>"},{"location":"methodsx_protocol/#mapping-figurestables-to-commands","title":"Mapping figures/tables to commands","text":"<ul> <li>PCA scores (Figure): generated from the classification dataset; output <code>oil_pca_scores.png</code>.  </li> <li>Command: <code>foodspec reproduce-methodsx</code> (internal PCA).  </li> <li>Dataset: public oils library (Raman/FTIR).</li> <li>Confusion matrix (Figure): <code>oil_confusion_matrix.png</code>.  </li> <li>Command: <code>foodspec reproduce-methodsx</code> (oil classification step).  </li> <li>Metrics: accuracy, F1 in <code>metrics.json</code> (classification section).</li> <li>Classification metrics (Table): accuracy/F1 summary in <code>metrics.json</code>.  </li> <li>Use overall values for main text; per-class precision/recall can be supplementary if available.</li> <li>Mixture analysis metrics (Table/Plot): <code>mixture_r2</code>, <code>mixture_rmse</code> in <code>metrics.json</code>; add predicted vs true fraction plot if desired.  </li> <li>Dataset: EVOO\u2013sunflower library.</li> </ul>"},{"location":"methodsx_protocol/#reproduction-checklist","title":"Reproduction checklist","text":"<ol> <li>Download public datasets and convert to HDF5 libraries (see Libraries and CSV\u2192HDF5 pages).  </li> <li>Run <code>foodspec reproduce-methodsx --output-dir runs/methodsx_protocol</code>.  </li> <li>Verify artifacts in the run directory: metrics.json, report.md, PCA/confusion matrix plots.  </li> <li>Align outputs with paper figures/tables (rename or re-caption as needed).  </li> <li>For publication:  </li> <li>Main: PCA scores figure, confusion matrix, classification metrics (accuracy/F1), mixture metrics (R\u00b2/RMSE).  </li> <li>Supplementary: additional per-class metrics, residual plots, spectra examples, run_metadata.json for reproducibility.</li> </ol> <p>See also - Keyword index: keyword_index.md</p>"},{"location":"metrics_interpretation/","title":"Metrics &amp; result interpretation","text":"<p>Questions this page answers - What do common classification/regression metrics mean? - How do I compute them on foodspec outputs? - How should I interpret and report them in food spectroscopy?</p>"},{"location":"metrics_interpretation/#classification","title":"Classification","text":"<ul> <li>Accuracy: fraction correct; sensitive to imbalance. Good for balanced datasets; supplement with F1 otherwise.  </li> <li>Precision: of predicted positives, how many are correct; important when false positives are costly.  </li> <li>Recall: of true positives, how many were found; important when missing adulterants is costly.  </li> <li>F1-score: harmonic mean of precision and recall (macro for imbalance).  </li> <li>Confusion matrix: shows which classes are confused; normalize rows to interpret per-class accuracy.  </li> <li>ROC-AUC: ranking quality; use when probabilities/scores available.</li> </ul> <p>Guidelines: early/exploratory work may see 0.7\u20130.85 accuracy/F1; for publication aim higher (\u22650.9) on well-designed data; always report class balance and CV design.</p>"},{"location":"metrics_interpretation/#small-code-example-classification","title":"Small code example (classification)","text":"<pre><code>from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n\n# y_true, y_pred from a foodspec workflow (e.g., oil_auth)\nacc = accuracy_score(y_true, y_pred)\nf1_macro = f1_score(y_true, y_pred, average=\"macro\")\ncm = confusion_matrix(y_true, y_pred, labels=class_labels)\nprint(acc, f1_macro, cm)\n</code></pre> <p>Interpretation: combine accuracy with macro/weighted F1 for imbalance; use confusion matrix to inspect per-class errors.</p>"},{"location":"metrics_interpretation/#regression-and-mixture-analysis","title":"Regression and mixture analysis","text":"<ul> <li>RMSE / MAE: typical absolute error in target units (e.g., fraction or %); smaller is better; relate to acceptable error (e.g., \u00b10.05 fraction).  </li> <li>R\u00b2: proportion of variance explained; near 1 is good; check residuals for bias.  </li> <li>Bias: mean error; indicates systematic over/underestimation.  </li> <li>Residuals: inspect predicted vs true and residual vs true plots.</li> </ul>"},{"location":"metrics_interpretation/#small-code-example-regressionmixture","title":"Small code example (regression/mixture)","text":"<pre><code>from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport numpy as np\n\n# y_true, y_pred from mixture/regression workflow\nrmse = np.sqrt(mean_squared_error(y_true, y_pred))\nmae = mean_absolute_error(y_true, y_pred)\nr2 = r2_score(y_true, y_pred)\nprint(rmse, mae, r2)\n</code></pre> <p>Interpretation: relate RMSE/MAE to allowable error; R\u00b2 near 1 is good but inspect residuals for structure.</p>"},{"location":"metrics_interpretation/#cross-validation-and-robustness","title":"Cross-validation and robustness","text":"<ul> <li>Use k-fold CV (stratified for classification) and report mean \u00b1 std across folds.  </li> <li>Consider multiple random seeds for small datasets.  </li> <li>Validate on an independent dataset/instrument when possible.</li> </ul>"},{"location":"metrics_interpretation/#reporting","title":"Reporting","text":"<ul> <li>Main: confusion matrix + macro/weighted F1 (classification); R\u00b2/RMSE (regression/mixture).  </li> <li>Supplementary: per-class precision/recall/F1, ROC curves, sensitivity analyses, residual plots.  </li> <li>Always state preprocessing, features, model type, CV design (folds, stratification), and dataset provenance.</li> </ul> <p>See also - stats_tests.md - oil_auth_tutorial.md - methodsx_protocol.md - api_reference.md</p>"},{"location":"microbial_tutorial/","title":"Microbial domain template","text":"<p>Questions this page answers - How do I classify microbial spectra with foodspec? - How does the microbial template map to the core workflows? - What do CLI and Python runs look like?</p>"},{"location":"microbial_tutorial/#use-case-and-data","title":"Use case and data","text":"<p>Assume spectral data with label column <code>species</code> or <code>strain</code> (or contamination <code>status</code>: positive/negative). The microbial template reuses the oil-auth pipeline (baseline, smoothing, normalization, cropping, peaks/ratios + classifier).</p>"},{"location":"microbial_tutorial/#pipeline-outline","title":"Pipeline outline","text":"<p>1) CSV \u2192 HDF5 library:</p> <pre><code>foodspec csv-to-library data/microbial.csv libraries/microbial.h5 \\\n  --format wide \\\n  --wavenumber-column wavenumber \\\n  --label-column species \\\n  --modality raman\n</code></pre> <p>2) Run classification via domain template (CLI):</p> <pre><code>foodspec domains \\\n  libraries/microbial.h5 \\\n  --type microbial \\\n  --label-column species \\\n  --classifier-name svm_rbf \\\n  --output-dir runs/microbial_demo\n</code></pre> <p>Outputs: CV metrics CSV, confusion_matrix.png, report.md, summary.json.</p>"},{"location":"microbial_tutorial/#python-example","title":"Python example","text":"<pre><code>from foodspec.data import load_library\nfrom foodspec.apps.microbial import run_microbial_detection_workflow\n\nfs = load_library(\"libraries/microbial.h5\")\nres = run_microbial_detection_workflow(fs, label_column=\"species\", classifier_name=\"svm_rbf\", cv_splits=5)\nprint(res.cv_metrics.head())\n</code></pre>"},{"location":"microbial_tutorial/#considerations","title":"Considerations","text":"<ul> <li>Class imbalance is common; rely on macro/weighted F1 and per-class metrics.</li> <li>Cross-validation and, ideally, external validation (independent isolates) are important for robustness.</li> </ul>"},{"location":"microbial_tutorial/#reporting","title":"Reporting","text":"<ul> <li>Main: overall accuracy/macro F1, confusion matrix; note handling of imbalance.  </li> <li>Supplementary: per-class precision/recall/F1, spectra examples, feature importances/ratios, run metadata/configs.</li> </ul> <p>See also - domains_overview.md - metrics_interpretation.md - oil_auth_tutorial.md - methodsx_protocol.md</p>"},{"location":"mixture_tutorial/","title":"Mixture analysis (NNLS / MCR-ALS)","text":""},{"location":"mixture_tutorial/#problem","title":"Problem","text":"<p>Estimate the fraction of component A vs B (e.g., EVOO\u2013sunflower) in a mixture from Raman/FTIR spectra.</p>"},{"location":"mixture_tutorial/#methods-plain-language-math","title":"Methods (plain language + math)","text":"<ul> <li>NNLS (Non-negative least squares): solve ( \\mathbf{x} \\approx \\mathbf{S} \\cdot \\mathbf{c} ) subject to ( c_i \\ge 0 ), where (\\mathbf{x}) is a mixture spectrum, (\\mathbf{S}) are pure spectra (columns), and (\\mathbf{c}) are coefficients (fractions).  </li> <li>MCR-ALS (Alternating least squares): factorize a matrix of mixtures ( \\mathbf{X} \\approx \\mathbf{C} \\mathbf{S}^\\top ) with non-negativity; iteratively update concentrations ( \\mathbf{C} ) and pure profiles ( \\mathbf{S} ).</li> </ul> <p>Assumptions: linear mixing, non-negative spectra and concentrations, shared wavenumber axis.</p>"},{"location":"mixture_tutorial/#metrics-and-expectations","title":"Metrics and expectations","text":"<ul> <li>RMSE / MAE: error in predicted fraction; small (&lt;0.05\u20130.1) is good for lab-grade mixtures; industrial contexts may tolerate higher.  </li> <li>R\u00b2: variance explained; closer to 1 indicates better fits.  </li> <li>Bias: systematic over/underestimation; check residuals vs true fraction.</li> </ul>"},{"location":"mixture_tutorial/#cli-examples","title":"CLI examples","text":"<ul> <li>NNLS (single spectrum index):</li> </ul> <pre><code>foodspec mixture \\\n  libraries/mixture.h5 \\\n  --pure-hdf5 libraries/pure_oils.h5 \\\n  --mode nnls \\\n  --spectrum-index 0 \\\n  --output-dir runs/mixture_nnls\n</code></pre> <ul> <li>MCR-ALS:</li> </ul> <pre><code>foodspec mixture \\\n  libraries/mixture.h5 \\\n  --pure-hdf5 libraries/pure_oils.h5 \\\n  --mode mcr_als \\\n  --output-dir runs/mixture_mcr\n</code></pre> <p>Outputs: coefficients/residuals (NNLS) or C/S matrices (MCR-ALS), metrics.json, plots (optional).</p>"},{"location":"mixture_tutorial/#python-example","title":"Python example","text":"<pre><code>from foodspec.chemometrics.mixture import nnls_mixture, mcr_als\ncoeffs, resid = nnls_mixture(mixture_spectrum, pure_matrix)  # pure_matrix: (n_points, n_components)\nC, S = mcr_als(X_mixture, n_components=2)  # X_mixture: (n_samples, n_points)\n</code></pre>"},{"location":"mixture_tutorial/#quick-metric-check","title":"Quick metric check","text":"<pre><code>from sklearn.metrics import r2_score, mean_squared_error\nimport numpy as np\npred = pure_matrix @ coeffs\nprint(\"RMSE:\", np.sqrt(mean_squared_error(mixture_spectrum, pred)))\n</code></pre> <p>Interpretation: small RMSE and high R\u00b2 indicate good reconstruction; inspect residuals for bias.</p>"},{"location":"mixture_tutorial/#reporting-guidance","title":"Reporting guidance","text":"<ul> <li>Main figures: predicted vs true fraction plot; residual plot.  </li> <li>Supplementary: recovered pure spectra (S), concentration profiles (C), detailed RMSE/MAE tables.  </li> <li>State assumptions (linear mixing, non-negativity) and wavenumber range used; include any calibration/validation split details.</li> </ul> <p>See also - metrics_interpretation.md - stats_tests.md - api_reference.md</p>"},{"location":"ml_models/","title":"Machine learning models in foodspec","text":"<p>Questions this page answers - Which models are available and when to use them? - How do I invoke them via foodspec (classifier_name/run_pca)? - What should I report?</p>"},{"location":"ml_models/#linear-models","title":"Linear models","text":"<ul> <li>Logistic regression: linear boundary; good baseline, interpretable.  </li> <li>Foodspec: <code>classifier_name=\"logreg\"</code> in workflows.  </li> <li>Example:     <code>python     from foodspec.chemometrics.models import make_classifier     clf = make_classifier(\"logreg\")     clf.fit(X_train, y_train)</code></li> <li>Report: accuracy/F1, confusion matrix; note regularization if tuned.</li> <li>Linear SVM: margin maximization for linear problems.  </li> <li>Foodspec: <code>classifier_name=\"svm_linear\"</code>.</li> <li>PLS-DA: PLS projection + classifier for correlated spectra.  </li> <li>Foodspec: <code>make_pls_da(n_components=10)</code>.</li> </ul>"},{"location":"ml_models/#non-linear-models","title":"Non-linear models","text":"<ul> <li>RBF SVM: nonlinear separation.  </li> <li>Foodspec: <code>classifier_name=\"svm_rbf\"</code>; key params: C, gamma.</li> <li>k-NN: instance-based; sensitive to scaling/imbalance.  </li> <li>Foodspec: <code>classifier_name=\"knn\"</code>; key param: k.</li> <li>Random Forest (RF): ensemble of trees; robust, feature importance.  </li> <li>Foodspec: <code>classifier_name=\"rf\"</code>; key params: n_estimators, max_depth.</li> <li>Gradient boosting (if configured): stronger ensemble; risk overfitting on small sets.</li> <li>Example (via workflow):   <code>python   clf = make_classifier(\"rf\", n_estimators=200, random_state=0)   clf.fit(X_train, y_train)</code></li> </ul>"},{"location":"ml_models/#unsupervised-methods","title":"Unsupervised methods","text":"<ul> <li>PCA: dimensionality reduction/visualization; use for QC/exploration.  </li> <li>Foodspec: <code>run_pca(X, n_components=2)</code>.  </li> <li>Report explained variance and score plots.</li> <li>Clustering: not core in foodspec; apply to PCA/features if needed (k-means, etc.).</li> </ul>"},{"location":"ml_models/#mixture-models","title":"Mixture models","text":"<ul> <li>NNLS / MCR-ALS: estimate component fractions in mixtures.  </li> <li>Foodspec: <code>nnls_mixture</code>, <code>mcr_als</code> in <code>chemometrics.mixture</code>.  </li> <li>Report RMSE/R\u00b2, residuals.</li> </ul>"},{"location":"ml_models/#deep-learning-optional","title":"Deep learning (optional)","text":"<ul> <li>Conv1DSpectrumClassifier: 1D CNN prototype; optional extra (<code>foodspec[deep]</code>).  </li> <li>Use only with sufficient data; risk of overfitting on small sets.  </li> <li>Report train/val split, regularization, seeds.</li> </ul>"},{"location":"ml_models/#reporting","title":"Reporting","text":"<ul> <li>State model type and key hyperparameters.</li> <li>Report metrics: accuracy, macro/weighted F1, ROC-AUC (if probabilities), confusion matrix; for regression/mixture, R\u00b2/RMSE.</li> <li>Use stratified CV for classification; report mean \u00b1 std.</li> <li>Mention preprocessing and features (baseline/smoothing/normalization, peaks/ratios).</li> </ul> <p>See also - metrics_interpretation.md - oil_auth_tutorial.md - methodsx_protocol.md - api_reference.md</p>"},{"location":"model_registry/","title":"Model registry","text":"<p>Questions this page answers - How do I save and load trained foodspec pipelines? - What metadata is stored? - How do I apply a saved model in a new session?</p>"},{"location":"model_registry/#what-metadata-is-stored","title":"What metadata is stored?","text":"<ul> <li><code>name</code>, <code>version</code>, <code>foodspec_version</code></li> <li>Timestamp</li> <li>Model artifact (<code>.joblib</code>) + metadata JSON</li> <li><code>extra</code>: label/model configuration, any custom fields (e.g., classifier name, label column)</li> </ul>"},{"location":"model_registry/#end-to-end-example-python","title":"End-to-end example (Python)","text":"<p>Train (e.g., oil-auth), save, then load and apply to new data.</p> <pre><code>from foodspec.data import load_library\nfrom foodspec.apps.oils import run_oil_authentication_workflow\nfrom foodspec.model_registry import save_model, load_model\n\n# Train on a library\nfs_train = load_library(\"libraries/oils_train.h5\")\nres = run_oil_authentication_workflow(fs_train, label_column=\"oil_type\", classifier_name=\"rf\", cv_splits=5)\n\n# Save pipeline\nsave_model(\n    model=res.pipeline,\n    path=\"models/oil_rf_v1\",\n    name=\"oil_rf\",\n    version=\"0.2.0\",\n    foodspec_version=\"0.2.0\",\n    extra={\"label_column\": \"oil_type\", \"classifier_name\": \"rf\"},\n)\n\n# Load and apply to new samples\nmodel_loaded, meta = load_model(\"models/oil_rf_v1\")\nfs_new = load_library(\"libraries/oils_new.h5\")\npreds = model_loaded.predict(fs_new.x)\nprint(meta, preds[:5])\n</code></pre>"},{"location":"model_registry/#cli-flow","title":"CLI flow","text":"<ul> <li>Save during workflow:</li> </ul> <pre><code>foodspec oil-auth libraries/oils_train.h5 \\\n  --label-column oil_type \\\n  --classifier-name rf \\\n  --save-model models/oil_rf_v1 \\\n  --output-dir runs/oils_rf\n</code></pre> <ul> <li>Inspect metadata:</li> </ul> <pre><code>foodspec model-info models/oil_rf_v1\n</code></pre> <p>To apply the saved model, load it in Python (as above) and predict on a new HDF5 library.</p>"},{"location":"model_registry/#best-practices","title":"Best practices","text":"<ul> <li>Version models (e.g., v1, v2) and record foodspec_version.</li> <li>Store training data summary (labels, date, preprocessing choices) in <code>extra</code>.</li> <li>Keep artifacts (<code>.joblib</code> + <code>.json</code>) under version control or a model registry; retain for QA/regulatory traceability.</li> </ul> <p>See also - <code>cli.md</code> - <code>metrics_interpretation.md</code> - <code>methodsx_protocol.md</code> - <code>oil_auth_tutorial.md</code></p>"},{"location":"oil_auth_tutorial/","title":"Oil authentication workflow","text":"<p>Questions this page answers - Can we classify edible oils from Raman/FTIR spectra? - How do I run the workflow via CSV \u2192 library \u2192 oil-auth? - How do I interpret PCA, confusion matrix, and feature importance? - How should I report the results?</p>"},{"location":"oil_auth_tutorial/#scientific-question","title":"Scientific question","text":"<p>Determine oil type and detect adulteration using vibrational spectra.</p>"},{"location":"oil_auth_tutorial/#end-to-end-example","title":"End-to-end example","text":""},{"location":"oil_auth_tutorial/#cli-path","title":"CLI path","text":"<p>1) Convert CSV to library (wide format example):</p> <pre><code>foodspec csv-to-library data/oils.csv libraries/oils.h5 \\\n  --format wide \\\n  --wavenumber-column wavenumber \\\n  --label-column oil_type \\\n  --modality raman\n</code></pre> <p>2) Run oil authentication (CLI):</p> <pre><code>foodspec oil-auth libraries/oils.h5 \\\n  --label-column oil_type \\\n  --classifier-name rf \\\n  --cv-splits 5 \\\n  --output-dir runs/oils_demo\n</code></pre> <p>Outputs: metrics.json/CSV, confusion_matrix.png, feature importances (if available), report.md.</p>"},{"location":"oil_auth_tutorial/#python-variant","title":"Python variant","text":"<pre><code>from foodspec.data import load_library\nfrom foodspec.apps.oils import run_oil_authentication_workflow\n\nfs = load_library(\"libraries/oils.h5\")\nresult = run_oil_authentication_workflow(fs, label_column=\"oil_type\", classifier_name=\"rf\", cv_splits=5)\nprint(result.cv_metrics.head())\n</code></pre>"},{"location":"oil_auth_tutorial/#inspecting-pca-and-confusion-matrix","title":"Inspecting PCA and confusion matrix","text":"<ul> <li>PCA: visualize clustering of classes; look for separation by oil type.</li> <li>Confusion matrix: check misclassifications; identify similar oils being confused.</li> <li>Feature importance: peak/ratio contributions; focus on chemically meaningful bands (e.g., 1655/1742).</li> </ul>"},{"location":"oil_auth_tutorial/#interpretation","title":"Interpretation","text":"<ul> <li>Accuracy/macro F1 reflect overall performance; use stratified CV (default).  </li> <li>Good accuracy is dataset-dependent; for clean lab spectra expect higher scores; for challenging/adulterated sets, report limitations.</li> <li>Cross-validation reduces optimistic bias and shows variance across folds.</li> </ul>"},{"location":"oil_auth_tutorial/#reporting","title":"Reporting","text":"<ul> <li>Main text: overall accuracy/macro F1, confusion matrix figure, brief preprocessing/model description.  </li> <li>Supplementary: per-class precision/recall/F1, feature importances/ratios, spectra examples, run metadata/configs.</li> <li>For MethodsX/FAIR: include preprocessing steps (baseline, smoothing, normalization, crop), classifier choice, CV design, and dataset provenance.</li> </ul>"},{"location":"oil_auth_tutorial/#optional-comparing-ratios-between-oil-types-with-statistical-tests","title":"Optional: comparing ratios between oil types with statistical tests","text":"<p>You can test whether a specific band ratio differs across oil types (useful for interpretation/papers).</p> <pre><code>import pandas as pd\nfrom scipy.stats import kruskal\nfrom foodspec.features.ratios import compute_ratios\n\n# Assume peak heights already extracted to df_peaks; here we build a ratio\nratio_def = {\"ratio_1655_1745\": (\"peak_1655.0_height\", \"peak_1742.0_height\")}\ndf_ratios = compute_ratios(df_peaks, ratio_def)\ndf_ratios[\"oil_type\"] = fs.metadata[\"oil_type\"].values\n\ngroups = [g[\"ratio_1655_1745\"].values for _, g in df_ratios.groupby(\"oil_type\")]\nstat, p = kruskal(*groups)\nprint(f\"Kruskal\u2013Wallis H={stat:.3f}, p={p:.3g}\")\n</code></pre> <p>Interpretation: a small p-value suggests at least one oil type has a different ratio distribution. Not required for classification, but useful for scientific interpretation and MethodsX-style reporting.</p> <p>See also - csv_to_library.md - metrics_interpretation.md - keyword_index.md - ftir_raman_preprocessing.md</p>"},{"location":"preprocessing_guide/","title":"Preprocessing guide","text":"<p>Preprocessing makes real spectra comparable by removing background, reducing noise, and normalizing scale. Below are the major steps implemented in FoodSpec, the problems they solve, and when to use them.</p>"},{"location":"preprocessing_guide/#baseline-correction","title":"Baseline correction","text":"<ul> <li>ALS (Asymmetric Least Squares): iteratively fits a smooth baseline; subtracts fluorescence (Raman) or sloping background (FTIR).  </li> <li>Math: penalized spline with asymmetry parameter p to favor fitting the lower envelope.  </li> <li>Use when broad background dominates peaks; avoid overfitting (tune \u03bb, p).</li> <li>Rubberband: connects convex hull of the spectrum and subtracts it.  </li> <li>Good for concave baselines; less effective with strong convex fluorescence.</li> <li>Polynomial: low-degree polynomial fit to the whole spectrum.  </li> <li>Use when baseline is globally smooth and simple; avoid high degree to prevent peak distortion.</li> </ul>"},{"location":"preprocessing_guide/#smoothing","title":"Smoothing","text":"<ul> <li>Savitzky\u2013Golay: local polynomial regression; preserves peak shape.  </li> <li>Parameters: odd window length; polyorder &lt; window_length.  </li> <li>Use for moderate noise; avoid very short windows that under-smooth or long windows that flatten peaks.</li> <li>Moving average: simple mean over a window.  </li> <li>Use for gentle denoising; may broaden peaks.</li> </ul>"},{"location":"preprocessing_guide/#normalization-and-scatter-correction","title":"Normalization and scatter correction","text":"<ul> <li>Vector/Area/Max normalization: scale spectra to unit norm or unit area to remove intensity scaling differences.  </li> <li>Use for consistent scaling; avoid if absolute intensity carries meaning.</li> <li>SNV (Standard Normal Variate): subtract mean and divide by std per spectrum.  </li> <li>Removes additive/multiplicative effects; use on diffuse reflectance or when scatter varies; avoid if spectra have near-zero variance.</li> <li>MSC (Multiplicative Scatter Correction): regress each spectrum onto a reference (mean spectrum) and correct slope/intercept.  </li> <li>Use for scatter differences; requires representative reference; avoid if reference is poor or spectra have zeros/NaNs.</li> <li>Internal-peak normalization: scale so a known internal band has mean 1 within a window.  </li> <li>Use when a stable internal standard exists (e.g., known peak unaffected by composition).</li> </ul>"},{"location":"preprocessing_guide/#cropping","title":"Cropping","text":"<ul> <li>Restrict to informative regions (e.g., 600\u20131800 cm\u207b\u00b9 fingerprint).  </li> <li>Reduces noise and irrelevant regions; essential when instrument provides wide ranges.</li> </ul>"},{"location":"preprocessing_guide/#ftir-specific-helpers","title":"FTIR-specific helpers","text":"<ul> <li>AtmosphericCorrector: subtracts water/CO\u2082 components using template bases; useful for MIR air-path measurements.  </li> <li>SimpleATRCorrector: compensates ATR depth effects with a heuristic scaling; use cautiously, mainly for comparative studies.</li> </ul>"},{"location":"preprocessing_guide/#raman-specific-helper","title":"Raman-specific helper","text":"<ul> <li>CosmicRayRemover: detects sharp spikes and interpolates; apply to Raman spectra with cosmic-ray artifacts.</li> </ul>"},{"location":"preprocessing_guide/#example-default-pipelines","title":"Example default pipelines","text":"<ul> <li>Raman: ALS (\u03bb ~ 1e5, p ~ 0.01) \u2192 Savitzky\u2013Golay (window 9, poly 3) \u2192 Vector norm \u2192 Crop 600\u20131800 cm\u207b\u00b9 \u2192 (optional) CosmicRayRemover before baseline.  </li> <li>FTIR (ATR): Rubberband or ALS \u2192 Savitzky\u2013Golay (gentle) \u2192 MSC or SNV \u2192 Optional Atmospheric/ATR corrections \u2192 Crop to target region (e.g., 900\u20131800 cm\u207b\u00b9).  </li> </ul>"},{"location":"preprocessing_guide/#when-it-may-cause-problems","title":"When it may cause problems","text":"<ul> <li>Over-aggressive baseline can distort broad peaks.</li> <li>Too-strong smoothing can flatten narrow peaks.</li> <li>Normalization can hide absolute concentration effects.</li> <li>Scatter corrections assume linear relationships; not ideal if chemical changes alter band shapes dramatically. <p>Status: Archived This page predates the current preprocessing guide. See ftir_raman_preprocessing.md for the recommended, up-to-date guidance.</p> </li> </ul>"},{"location":"protocol_benchmarks/","title":"Protocol benchmarks","text":""},{"location":"protocol_benchmarks/#purpose","title":"Purpose","text":"<p>Protocol benchmarks validate the robustness and generality of the FoodSpec protocol across public datasets. They are a reference point to judge whether new preprocessing or models are \u201cgood enough\u201d to be called a protocol.</p>"},{"location":"protocol_benchmarks/#command","title":"Command","text":"<pre><code>foodspec protocol-benchmarks --output-dir runs/protocol_benchmarks\n</code></pre> <p>Artifacts (timestamped run folder): - <code>classification_metrics.json</code> + <code>classification_confusion_matrix.csv</code> - <code>mixture_metrics.json</code> (regression on mixtures) - <code>report.md</code> summary and <code>run_metadata.json</code> (environment/version info)</p>"},{"location":"protocol_benchmarks/#interpretation","title":"Interpretation","text":"<ul> <li>Classification: look at accuracy/F1 and confusion matrix; high scores suggest strong discriminative power on the benchmark oil dataset.</li> <li>Mixture: R\u00b2 and RMSE indicate fraction-estimation quality; use as a baseline before deploying new pipelines.</li> <li>If errors are present (e.g., missing datasets), the summary will include clear messages\u2014benchmarks should only be compared when datasets are available.</li> </ul>"},{"location":"protocol_benchmarks/#when-to-use","title":"When to use","text":"<ul> <li>Before publishing or claiming protocol stability.</li> <li>After major pipeline changes to ensure regressions are not introduced.</li> <li>To compare alternate preprocessing/model choices against a fixed reference.</li> </ul>"},{"location":"qc_tutorial/","title":"Quality control / novelty detection","text":"<p>Questions this page answers - Is a new batch in-distribution compared to a reference library? - How do I train/apply a one-class model (Python and CLI)? - How do I interpret QC flags? - How should I report QC findings?</p>"},{"location":"qc_tutorial/#use-case","title":"Use case","text":"<p>Train on reference (authentic) spectra and score new samples to flag suspects (outliers/adulterated/off-spec).</p>"},{"location":"qc_tutorial/#running-the-workflow","title":"Running the workflow","text":"<p>CLI:</p> <pre><code>foodspec qc libraries/oils.h5 \\\n  --model-type oneclass_svm \\\n  --output-dir runs/qc_demo\n</code></pre> <p>Outputs: qc_scores.csv (scores + labels), summary.json/report.md.</p> <p>Python:</p> <pre><code>from foodspec.data import load_library\nfrom foodspec.apps.qc import train_qc_model, apply_qc_model\nfs = load_library(\"libraries/oils.h5\")\nmodel = train_qc_model(fs, model_type=\"oneclass_svm\")\nres = apply_qc_model(fs, model=model)\nprint(res.labels_pred.value_counts())\n</code></pre>"},{"location":"qc_tutorial/#interpretation","title":"Interpretation","text":"<ul> <li>Scores above threshold \u2192 \u201cauthentic\u201d; below \u2192 \u201csuspect\u201d.  </li> <li>Check distribution of scores; adjust threshold if needed (domain knowledge).  </li> <li>A QC failure may indicate adulteration, contamination, instrument drift, or out-of-spec batch; verify with orthogonal tests.</li> </ul>"},{"location":"qc_tutorial/#quick-check","title":"Quick check","text":"<pre><code>auth_count = (res.labels_pred == \"authentic\").sum()\nsuspect_count = (res.labels_pred == \"suspect\").sum()\nprint({\"authentic\": auth_count, \"suspect\": suspect_count})\n</code></pre> <p>Interpretation: monitor how many samples fall below threshold; revisit threshold/model if too many false alarms.</p>"},{"location":"qc_tutorial/#reporting","title":"Reporting","text":"<ul> <li>Main: histogram of scores with threshold; counts of authentic vs suspect.  </li> <li>Supplementary: per-batch scores, thresholds used, preprocessing steps, model parameters.  </li> <li>Document reference set composition and any label filters used for training.</li> </ul> <p>See also - csv_to_library.md - metrics_interpretation.md - reporting_guidelines.md - api_reference.md</p>"},{"location":"quickstart_cli/","title":"Quickstart (CLI)","text":"<p>This walkthrough shows a first end-to-end run using the foodspec command-line interface.</p>"},{"location":"quickstart_cli/#1-prepare-data","title":"1) Prepare data","text":"<p>Use a small CSV (wide or long) of spectra or a public dataset you downloaded. Example wide CSV layout:</p> <pre><code>wavenumber,s1,s2\n500,10.1,12.3\n502,10.3,12.4\n...\n</code></pre>"},{"location":"quickstart_cli/#2-convert-csv-hdf5-library","title":"2) Convert CSV \u2192 HDF5 library","text":"<pre><code>foodspec csv-to-library \\\n  data/oils_wide.csv \\\n  libraries/oils_demo.h5 \\\n  --format wide \\\n  --wavenumber-column wavenumber \\\n  --modality raman \\\n  --label-column oil_type\n</code></pre> <p>This creates an HDF5 spectral library usable by all workflows (validated <code>FoodSpectrumSet</code>).</p>"},{"location":"quickstart_cli/#3-run-oil-authentication","title":"3) Run oil authentication","text":"<pre><code>foodspec oil-auth \\\n  libraries/oils_demo.h5 \\\n  --label-column oil_type \\\n  --output-dir runs/oil_demo\n</code></pre> <p>Outputs (timestamped folder): - <code>metrics.json</code> / CSV of CV metrics - <code>confusion_matrix.png</code> - <code>report.md</code> / summary.json</p>"},{"location":"quickstart_cli/#4-inspect-results","title":"4) Inspect results","text":"<ul> <li>Accuracy/F1 in metrics.json</li> <li>Confusion matrix plot shows class separation</li> <li>report.md summarizes run parameters and files</li> </ul> <p>Tips: - Use <code>--classifier-name</code> to switch models (rf, svm_rbf, logreg, etc.). - Add <code>--save-model</code> to persist the fitted pipeline via the model registry. - For long/tidy CSVs, use <code>--format long --sample-id-column ... --intensity-column ...</code>.</p>"},{"location":"quickstart_python/","title":"Quickstart (Python)","text":"<p>This walkthrough shows the main steps: load data, preprocess, explore with PCA, and run a classifier. Replace paths/labels with your own.</p>"},{"location":"quickstart_python/#1-load-and-validate","title":"1) Load and validate","text":"<pre><code>from pathlib import Path\nfrom foodspec.data import load_library\nfrom foodspec.validation import validate_spectrum_set\n\nfs = load_library(Path(\"libraries/oils_demo.h5\"))\nvalidate_spectrum_set(fs)\n</code></pre>"},{"location":"quickstart_python/#2-preprocess-baseline-smoothing-normalization","title":"2) Preprocess (baseline + smoothing + normalization)","text":"<pre><code>from foodspec.preprocess.baseline import ALSBaseline\nfrom foodspec.preprocess.smoothing import SavitzkyGolaySmoother\nfrom foodspec.preprocess.normalization import VectorNormalizer\n\nX = fs.x\nfor step in [\n    ALSBaseline(lambda_=1e5, p=0.01, max_iter=10),\n    SavitzkyGolaySmoother(window_length=9, polyorder=3),\n    VectorNormalizer(norm=\"l2\"),\n]:\n    X = step.fit_transform(X)\n</code></pre>"},{"location":"quickstart_python/#3-explore-with-pca","title":"3) Explore with PCA","text":"<pre><code>import matplotlib.pyplot as plt\nfrom foodspec.chemometrics.pca import run_pca\n\n_, pca_res = run_pca(X, n_components=2)\nplt.scatter(pca_res.scores[:, 0], pca_res.scores[:, 1], c=\"steelblue\")\nplt.xlabel(\"PC1\"); plt.ylabel(\"PC2\"); plt.tight_layout()\nplt.savefig(\"pca_scores.png\", dpi=150)\n</code></pre>"},{"location":"quickstart_python/#4-train-a-quick-classifier-demo","title":"4) Train a quick classifier (demo)","text":"<pre><code>from foodspec.chemometrics.models import make_classifier\n\nclf = make_classifier(\"rf\", random_state=42)\nclf.fit(X, fs.metadata[\"oil_type\"])\nacc = clf.score(X, fs.metadata[\"oil_type\"])\nprint(\"Training accuracy (demo only):\", acc)\n</code></pre> <p>Notes: - Replace <code>libraries/oils_demo.h5</code> and <code>oil_type</code> with your own library/label. - For real features, extract peaks/ratios first (see oil-auth workflow); the above uses preprocessed spectra directly for brevity. - Save plots/metrics for reporting.</p>"},{"location":"releasing/","title":"Releasing","text":"<p>Audience: Developers and maintainers This page documents internal release/versioning steps; it is not required for normal users.</p>"},{"location":"releasing/#release-versioning","title":"Release &amp; versioning","text":"<p>FoodSpec versions are tracked in <code>__version__</code> and <code>pyproject.toml</code>. Releases are published to PyPI and tagged on GitHub.</p>"},{"location":"releasing/#checklist-see-releasingmd-for-details","title":"Checklist (see RELEASING.md for details)","text":"<ul> <li>Update version strings and CHANGELOG.</li> <li>Run full tests and <code>mkdocs build</code>; ensure benchmarks/CLI smoke tests pass.</li> <li>Build wheel/sdist and upload to TestPyPI, then PyPI.</li> <li>Tag the release in Git and create a GitHub release with notes and artifacts.</li> </ul>"},{"location":"releasing/#pypi-and-github-linkage","title":"PyPI and GitHub linkage","text":"<ul> <li>The PyPI package and GitHub release tag should match (e.g., v0.x.y).</li> <li>Include release notes summarizing changes (features, fixes, docs updates).</li> </ul> <p>For coordination, contact Chandrasekar Subramani Narayan; external contributors should propose release candidates via PR.</p>"},{"location":"reporting_guidelines/","title":"Reporting guidelines (MethodsX-style)","text":""},{"location":"reporting_guidelines/#what-to-present-as-core-results","title":"What to present as core results","text":"<ul> <li>Main figures: confusion matrix (classification), PCA scores, key ratio/time trends, predicted vs true plots (mixture/regression).  </li> <li>Main tables: overall accuracy/F1 (classification) or R\u00b2/RMSE (regression/mixture); include fold-averaged metrics.  </li> <li>Preprocessing summary: list methods/parameters (baseline, smoothing, normalization, cropping).  </li> <li>Model and validation: classifier/regressor type, CV design (folds, stratification), seeds.</li> </ul>"},{"location":"reporting_guidelines/#what-belongs-in-supplementary-material","title":"What belongs in supplementary material","text":"<ul> <li>Full per-class precision/recall/F1 tables, additional confusion matrices.</li> <li>Hyperparameters, alternative models tried, sensitivity analyses.</li> <li>Additional spectra/ratios, extended residual plots, run_metadata.json/config files.</li> </ul>"},{"location":"reporting_guidelines/#describing-methods-for-reproducibility","title":"Describing methods for reproducibility","text":"<ul> <li>State data origin, modality, instrument, sample prep conditions.  </li> <li>Describe preprocessing steps with parameters; note wavenumber range.  </li> <li>Specify features used (peaks/ratios/bands) and model choices.  </li> <li>Document validation setup: CV splits, stratification, metrics reported, any held-out test sets.  </li> <li>Reference CLI/Python commands or configs used (e.g., <code>foodspec oil-auth</code> with flags, or script snippets).</li> </ul>"},{"location":"reporting_guidelines/#follow-upsupporting-tests","title":"Follow-up/supporting tests","text":"<ul> <li>Independent dataset or instrument for external validation.</li> <li>Orthogonal analyses (e.g., GC\u2013MS, peroxide/anisidine values, sensory tests) to corroborate spectroscopy results.</li> <li>Robustness checks: new batches, different preprocessing, or small perturbations to confirm stability.</li> </ul> <p>Align with FAIR: keep data + metadata together, cite public datasets/DOIs, share configs and run artifacts when possible.</p>"},{"location":"spectral_basics/","title":"Spectroscopy basics (Raman/FTIR)","text":""},{"location":"spectral_basics/#what-is-a-spectrum","title":"What is a spectrum?","text":"<p>A spectrum is a curve showing how light is absorbed or scattered as a function of the wavenumber (x\u2011axis) with corresponding intensity (y\u2011axis). Peaks and shoulders reveal vibrational modes of chemical bonds in the sample.</p>"},{"location":"spectral_basics/#wavenumber-cm1","title":"Wavenumber (cm\u207b\u00b9)","text":"<ul> <li>Wavenumber is the reciprocal of wavelength (1/\u03bb) and is reported in cm\u207b\u00b9.</li> <li>It is the standard unit in vibrational spectroscopy because it aligns directly with energy levels and vibrational transitions.</li> <li>Axes must be monotonic (increasing or decreasing consistently) for proper interpolation, preprocessing, and modeling.</li> </ul>"},{"location":"spectral_basics/#raman-vs-ftir-intuitive-differences","title":"Raman vs FTIR (intuitive differences)","text":"<ul> <li>Raman: measures inelastic scattering; good for aqueous systems, often affected by fluorescence background; cosmic-ray spikes may appear.</li> <li>FTIR: measures absorbance; sensitive to water and CO\u2082 interference; ATR accessories introduce depth-dependent effects.</li> <li>Both probe molecular vibrations but with different selection rules and sensitivities; preprocessing choices differ accordingly.</li> </ul>"},{"location":"spectral_basics/#typical-spectral-ranges-for-food","title":"Typical spectral ranges for food","text":"<ul> <li>Fingerprint region: ~600\u20131800 cm\u207b\u00b9 (rich in C\u2013C, C\u2013O, C=O, C=C, and CH bending).</li> <li>High wavenumber (CH stretch): ~2800\u20133100 cm\u207b\u00b9 (CH\u2082/CH\u2083 stretching).</li> <li>Some instruments provide wider ranges; crop to informative regions for stability.</li> </ul>"},{"location":"spectral_basics/#how-to-read-a-spectral-plot","title":"How to read a spectral plot","text":"<ul> <li>Axes: x = wavenumber (cm\u207b\u00b9), y = intensity (a.u.). Ensure units are noted.</li> <li>Peaks: sharp or broad maxima; indicate specific vibrational modes (e.g., 1655 cm\u207b\u00b9 for C=C, 1742 cm\u207b\u00b9 for C=O in oils).</li> <li>Shoulders: subtle features adjacent to main peaks; can indicate overlapping bands.</li> <li>Baseline: background level; may slope (FTIR) or rise (Raman fluorescence). Proper baseline correction makes peaks interpretable.</li> <li>Noise: random fluctuations; smoothing reduces noise but should preserve peak shape.</li> </ul>"},{"location":"stats_tests/","title":"Statistical tests for food spectroscopy","text":"<p>You are here: Methods &amp; theory \u2192 Statistical tests for food spectroscopy</p> <p>Questions this page answers - Why do statistical tests matter in Raman/FTIR food studies? - Which tests answer which questions, and what are their assumptions? - How do I run common tests with SciPy on foodspec-derived features? - How should I report test results in a paper?</p> <p>Note: foodspec provides spectral features (peaks, bands, ratios); statistical hypothesis tests are run with SciPy/statsmodels. Dependencies: SciPy (https://scipy.org/), statsmodels (https://www.statsmodels.org/)</p>"},{"location":"stats_tests/#introduction","title":"Introduction","text":"<p>Statistical tests help determine whether observed spectral differences or trends are unlikely to be due to chance. In Raman/FTIR food studies, they support claims about authenticity, treatment effects (heating, storage), or mixture proportions.</p>"},{"location":"stats_tests/#basic-concepts","title":"Basic concepts","text":"<ul> <li>Null hypothesis (H0): no difference/effect/association.</li> <li>p-value: probability of observing data as extreme as yours if H0 is true.</li> <li>Effect size: magnitude of the difference/association (practical relevance).</li> <li>Significance vs practical relevance: a small p-value may not imply a meaningful effect; always consider effect size and domain context.</li> </ul>"},{"location":"stats_tests/#parametric-tests","title":"Parametric tests","text":""},{"location":"stats_tests/#one-sample-t-test","title":"One-sample t-test","text":"<ul> <li>Question: Is the mean of one group different from a hypothesized value?</li> <li>Assumptions: normality of residuals; independent observations.</li> <li>Use case: is a band ratio significantly different from a reference value?</li> <li>Reporting: \u201cA one-sample t-test indicated the mean ratio differed from X (t=\u2026, p=\u2026).\u201d</li> <li>Implementation: use scipy/statsmodels; not directly wrapped in foodspec.</li> </ul>"},{"location":"stats_tests/#two-sample-t-test-independent","title":"Two-sample t-test (independent)","text":"<ul> <li>Question: Do two independent groups have different means?</li> <li>Assumptions: normality, similar variances (or use Welch\u2019s), independence.</li> <li>Use case: compare ratio means between oil A vs oil B.</li> <li>Reporting: \u201cAn independent t-test showed a significant difference between oils (t=\u2026, p=\u2026).\u201d</li> <li>Implementation: external (SciPy); foodspec outputs ratios/metrics you can feed to tests.</li> </ul> <p>Example (SciPy)</p> <pre><code>import pandas as pd\nfrom scipy.stats import ttest_ind\n\n# df has columns: ratio_1655_1745, oil_type\ngroup_a = df.loc[df[\"oil_type\"] == \"olive\", \"ratio_1655_1745\"]\ngroup_b = df.loc[df[\"oil_type\"] == \"sunflower\", \"ratio_1655_1745\"]\nstat, p = ttest_ind(group_a, group_b, equal_var=False)\nprint(f\"t={stat:.3f}, p={p:.3g}\")\n</code></pre> <p>Interpretation: compares mean band ratios between two oil types. If p is small, the difference is unlikely by chance; consider effect size (e.g., Cohen\u2019s d) for practical relevance.</p>"},{"location":"stats_tests/#paired-t-test","title":"Paired t-test","text":"<ul> <li>Question: Do paired measurements differ (e.g., before/after heating)?</li> <li>Assumptions: differences are roughly normal; pairs matched.</li> <li>Use case: spectra before vs after a treatment on the same sample.</li> <li>Reporting: \u201cA paired t-test indicated a change in ratio after heating (t=\u2026, p=\u2026).\u201d</li> <li>Implementation: external; foodspec provides features over time.</li> </ul> <p>Example (SciPy)</p> <pre><code>import pandas as pd\nfrom scipy.stats import ttest_rel\n\n# df has columns: ratio_before, ratio_after for matched samples\nstat, p = ttest_rel(df[\"ratio_before\"], df[\"ratio_after\"])\nprint(f\"paired t={stat:.3f}, p={p:.3g}\")\n</code></pre> <p>Interpretation: tests mean difference within paired samples (e.g., pre/post heating). Small p suggests a systematic change; report effect size if possible.</p>"},{"location":"stats_tests/#one-way-anova","title":"One-way ANOVA","text":"<ul> <li>Question: Do three or more group means differ?</li> <li>Assumptions: normality of residuals, homogeneity of variance, independence.</li> <li>Use case: compare multiple oil types or treatment stages.</li> <li>Reporting: \u201cOne-way ANOVA showed a group effect on ratio_1655_1742 (F=\u2026, p=\u2026).\u201d</li> <li>Implementation: foodspec\u2019s heating workflow can compute ANOVA; otherwise use SciPy/statsmodels.</li> </ul> <p>Example (SciPy)</p> <pre><code>import pandas as pd\nfrom scipy.stats import f_oneway\n\n# df has columns: ratio_1655_1745, oil_type\ngroups = [g[\"ratio_1655_1745\"].values for _, g in df.groupby(\"oil_type\")]\nstat, p = f_oneway(*groups)\nprint(f\"F={stat:.3f}, p={p:.3g}\")\n</code></pre> <p>Interpretation: tests if at least one oil type differs in mean ratio. If p is small, follow with post-hoc tests and report effect size (e.g., \u03b7\u00b2).</p>"},{"location":"stats_tests/#manova-brief","title":"MANOVA (brief)","text":"<ul> <li>Question: Do groups differ across multiple dependent variables simultaneously?</li> <li>Assumptions: multivariate normality, equal covariance matrices, independence.</li> <li>Use case: multiple ratios/PC scores across oil types.</li> <li>Reporting: \u201cMANOVA indicated multivariate differences among oils (Wilks\u2019 \u039b=\u2026, p=\u2026).\u201d</li> <li>Implementation: external; not wrapped in foodspec.</li> </ul>"},{"location":"stats_tests/#non-parametric-tests","title":"Non-parametric tests","text":""},{"location":"stats_tests/#mannwhitney-u","title":"Mann\u2013Whitney U","text":"<ul> <li>Question: Do two independent groups differ in central tendency without normality?</li> <li>Assumptions: independent samples; ordinal/continuous data.</li> <li>Use case: small-sample band ratios with non-normal distributions.</li> <li>Reporting: \u201cMann\u2013Whitney U test found a difference in ratios between groups (U=\u2026, p=\u2026).\u201d</li> </ul>"},{"location":"stats_tests/#kruskalwallis","title":"Kruskal\u2013Wallis","text":"<ul> <li>Question: Are there differences among three or more groups (non-parametric ANOVA)?</li> <li>Assumptions: independent samples; similar-shaped distributions.</li> <li>Use case: compare ratios across several oils when normality is doubtful.</li> <li>Reporting: \u201cKruskal\u2013Wallis test indicated a group effect (H=\u2026, p=\u2026).\u201d</li> </ul> <p>Example (SciPy)</p> <pre><code>import pandas as pd\nfrom scipy.stats import kruskal\n\n# df has columns: ratio_1655_1745, oil_type\ngroups = [g[\"ratio_1655_1745\"].values for _, g in df.groupby(\"oil_type\")]\nstat, p = kruskal(*groups)\nprint(f\"H={stat:.3f}, p={p:.3g}\")\n</code></pre> <p>Interpretation: tests median differences without assuming normality. Small p suggests at least one group differs; consider post-hoc pairwise tests with corrections.</p>"},{"location":"stats_tests/#wilcoxon-signed-rank","title":"Wilcoxon signed-rank","text":"<ul> <li>Question: Paired comparison without normality.</li> <li>Use case: before/after heating on the same samples.</li> <li>Reporting: \u201cWilcoxon signed-rank test showed a shift after treatment (W=\u2026, p=\u2026).\u201d</li> </ul>"},{"location":"stats_tests/#friedman-test","title":"Friedman test","text":"<ul> <li>Question: Repeated measures across &gt;2 conditions (non-parametric).</li> <li>Use case: multiple heating cycles on the same samples.</li> <li>Reporting: \u201cFriedman test detected differences across cycles (\u03c7\u00b2=\u2026, p=\u2026).\u201d</li> </ul>"},{"location":"stats_tests/#post-hoc-multiple-comparisons","title":"Post-hoc &amp; multiple comparisons","text":"<ul> <li>Tukey HSD: parametric pairwise group comparisons after ANOVA.</li> <li>Bonferroni/FDR: adjust p-values when doing many comparisons.</li> <li>Reporting: \u201cPost-hoc Tukey tests (adjusted p) identified differences between A and B.\u201d</li> <li>Implementation: external; apply to foodspec-generated ratio/features as needed.</li> </ul>"},{"location":"stats_tests/#correlation-and-regression","title":"Correlation and regression","text":"<ul> <li>Pearson correlation: linear association (assumes normality of variables).</li> <li>Spearman correlation: rank-based, non-parametric.</li> <li>Simple linear regression: e.g., ratio vs heating time, slope indicates trend.</li> <li>Reporting: \u201cPearson r=\u2026, p=\u2026 between ratio and heating time\u201d; \u201cSlope b=\u2026 (95% CI \u2026)\u201d.</li> <li>Implementation: foodspec heating workflow fits simple trends; additional correlations via SciPy/pandas.</li> </ul> <p>Pearson correlation example (SciPy)</p> <pre><code>import pandas as pd\nfrom scipy.stats import pearsonr\n\n# df has columns: ratio_1655_1745, heating_time\nstat, p = pearsonr(df[\"ratio_1655_1745\"], df[\"heating_time\"])\nprint(f\"r={stat:.3f}, p={p:.3g}\")\n</code></pre> <p>Interpretation: measures linear association between a ratio and time/temperature. Small p suggests a significant linear relationship; r indicates strength/direction.</p> <p>Simple regression example (SciPy)</p> <pre><code>from scipy.stats import linregress\n\nresult = linregress(df[\"heating_time\"], df[\"ratio_1655_1745\"])\nprint(f\"slope={result.slope:.3f}, R\u00b2={result.rvalue**2:.3f}, p={result.pvalue:.3g}\")\n</code></pre> <p>Interpretation: slope sign/magnitude shows trend; p tests if slope differs from zero; R\u00b2 indicates variance explained.</p>"},{"location":"stats_tests/#effect-sizes","title":"Effect sizes","text":"<ul> <li>Cohen\u2019s d: standardized mean difference (two groups).</li> <li>\u03b7\u00b2 / partial \u03b7\u00b2: proportion of variance explained in ANOVA.</li> <li>R\u00b2: proportion of variance explained in regression.</li> <li>Reporting: include effect size alongside p-value for practical relevance.</li> </ul> <p>See also - metrics_interpretation.md - oil_auth_tutorial.md - methodsx_protocol.md - api_reference.md</p>"},{"location":"testing_coverage/","title":"Testing coverage","text":"<p>Audience: Developers and maintainers This page documents internal testing practices and coverage notes; it is not required for normal users.</p>"},{"location":"testing_coverage/#testing-coverage","title":"Testing &amp; coverage","text":"<p>FoodSpec relies on extensive, fast, synthetic tests to keep the protocol trustworthy.</p>"},{"location":"testing_coverage/#running-tests","title":"Running tests","text":"<pre><code>pytest --disable-warnings -q\n</code></pre> <p>With coverage (if configured):</p> <pre><code>pytest --cov=foodspec\n</code></pre>"},{"location":"testing_coverage/#current-status","title":"Current status","text":"<ul> <li>Approximate coverage: ~91% (as of latest run).</li> <li>Scope: core data models, preprocessing, features, chemometrics, apps, CLI workflows, IO, viz, and optional deep-path guards.</li> </ul>"},{"location":"testing_coverage/#why-high-coverage-matters","title":"Why high coverage matters","text":"<ul> <li>Protocol-oriented library: small regressions can invalidate published pipelines.</li> <li>Ensures CLI workflows (oil-auth, heating, mixture, protocol benchmarks) stay stable.</li> <li>Encourages reproducibility: deterministic, synthetic datasets avoid external dependencies.</li> </ul>"},{"location":"testing_coverage/#testing-practices","title":"Testing practices","text":"<ul> <li>Use <code>tmp_path</code> for filesystem; <code>monkeypatch</code> for loaders and external dependencies.</li> <li>No network or large datasets; keep tests CPU-light.</li> <li>Add tests for new public APIs and edge cases (errors as well as happy paths).</li> </ul>"},{"location":"troubleshooting_faq/","title":"Troubleshooting &amp; FAQs","text":""},{"location":"troubleshooting_faq/#common-issues","title":"Common issues","text":"<ul> <li>Missing label column: ensure metadata includes the column passed to <code>--label-column</code> or used in Python (<code>fs.metadata</code>).</li> <li>Non-monotonic wavenumbers: sort axes before creating a library; <code>validate_spectrum_set</code> will fail otherwise.</li> <li>HDF5 load errors: confirm the file was created by foodspec (<code>create_library</code> or <code>foodspec preprocess/csv-to-library</code>).</li> <li>Small class sizes: cross-validation may fail if each class has fewer than 2 samples; add more data or reduce <code>cv_splits</code>.</li> <li>NaNs in data: impute or filter; many models do not accept NaNs.</li> </ul>"},{"location":"troubleshooting_faq/#faqs","title":"FAQs","text":"<ul> <li>Can foodspec handle non-food spectra? Yes; it is domain-agnostic but tuned for food spectroscopy defaults.</li> <li>What accuracy is \u201cgood\u201d? Depends on task and dataset; use protocol benchmarks as a reference and report F1/CM plots.</li> <li>How do I choose preprocessing? Start with ALS + Savitzky-Golay + Vector/MSC; see <code>ftir_raman_preprocessing.md</code>.</li> <li>Where are reports written? CLI commands create timestamped folders under <code>--output-dir</code> with metrics, plots, and markdown summaries.</li> <li>Can I customize models? Yes; use the Python API to build your own pipelines or swap classifiers via CLI flags.</li> </ul>"},{"location":"validation_baseline/","title":"Baseline Correction Validation","text":"<p>This example script (<code>examples/validation_preprocessing_baseline.py</code>) generates synthetic Raman-like spectra with a known polynomial baseline and Gaussian peaks, then compares:</p> <ul> <li>ALSBaseline</li> <li>RubberbandBaseline</li> <li>PolynomialBaseline</li> </ul> <p>It computes: - Non-peak region mean before/after correction - RMSE between estimated and true baseline</p> <p>Run:</p> <pre><code>python examples/validation_preprocessing_baseline.py\n</code></pre> <p>This produces <code>validation_baseline.png</code> and prints a summary table in the console.</p> <p>Status: Archived This page describes an earlier iteration of baseline validation. See the current preprocessing guides in ftir_raman_preprocessing.md and workflow-specific validation in oil_auth_tutorial.md.</p>"},{"location":"validation_chemometrics_oils/","title":"Chemometrics Validation: Oils","text":"<p>This page describes a typical oil authentication pipeline and how to run the protocol benchmarks.</p>"},{"location":"validation_chemometrics_oils/#oil-authentication-workflow","title":"Oil authentication workflow","text":"<p>The <code>run_oil_authentication_workflow</code> function implements a typical pipeline:</p> <ol> <li>Load a spectral library or public dataset.  </li> <li>Apply baseline correction, smoothing, and normalization.  </li> <li>Extract peak/band ratios.  </li> <li>Train and evaluate a classifier (e.g., Random Forest).</li> </ol> <p>Example (Python API):</p> <pre><code>from foodspec.data.libraries import load_library\nfrom foodspec.apps.oils import run_oil_authentication_workflow\n\nfs = load_library(\"libraries/oils_raman.h5\")\nresult = run_oil_authentication_workflow(fs, label_column=\"oil_type\", classifier_name=\"rf\", cv_splits=3)\nprint(result.cv_metrics)\nprint(\"Confusion matrix:\\n\", result.confusion_matrix)\n</code></pre> <p>You can also run the example script:</p> <pre><code>python examples/validation_chemometrics_oils.py\n</code></pre> <p>This produces PCA and confusion matrix plots in the working directory.</p>"},{"location":"validation_chemometrics_oils/#protocol-benchmarks-cli","title":"Protocol benchmarks (CLI)","text":"<p>The <code>foodspec protocol-benchmarks</code> command runs a standardized benchmark suite using public datasets (if available) or example data:</p> <pre><code>foodspec protocol-benchmarks --output-dir ./protocol_benchmarks\n</code></pre> <p>Each run produces a timestamped directory containing:</p> <ul> <li><code>classification_metrics.json</code> \u2013 oil classification metrics</li> <li><code>classification_confusion_matrix.csv</code></li> <li><code>mixture_metrics.json</code> \u2013 EVOO\u2013sunflower mixture regression metrics</li> <li><code>run_metadata.json</code> \u2013 environment and version information</li> <li><code>report.md</code> \u2013 human-readable summary</li> </ul> <p>If datasets are missing, the command reports clear errors; download public datasets into the expected folders (see <code>docs/libraries.md</code>).</p>"},{"location":"validation_peak_ratios/","title":"Peak Ratio Validation","text":"<p>This example script (<code>examples/validation_peak_ratios.py</code>) generates synthetic spectra with two Gaussian peaks at 1655 and 1742 cm^-1 with varying height ratios.</p> <p>It uses: - <code>PeakFeatureExtractor</code> - <code>RatioFeatureGenerator</code></p> <p>Outputs: - Scatter plot of true ratio vs measured ratio. - Correlation and RMSE between true and measured ratios.</p> <p>Run:</p> <pre><code>python examples/validation_peak_ratios.py\n</code></pre> <p>This produces <code>validation_peak_ratios.png</code> and prints summary statistics.</p> <p>Status: Archived This page reflects older peak-ratio validation examples. Refer to the current workflows in oil_auth_tutorial.md and metrics guidance in metrics_interpretation.md.</p>"},{"location":"design/01_overview/","title":"foodspec: Gold-Standard Spectroscopy Toolkit for Food Science","text":""},{"location":"design/01_overview/#1-vision-and-scope","title":"1. Vision and scope","text":"<ul> <li>Deliver a reproducible, production-ready Python toolkit for Raman and FTIR spectroscopy in food science, spanning pre-processing, feature extraction, chemometrics/ML, and domain modules.</li> <li>Serve both research and industrial pipelines: from exploratory analysis of single spectra to automated batch processing of hyperspectral maps in QA/QC and surveillance.</li> <li>Integrate with spectral libraries and curated example datasets to anchor methods in referenceable ground truth.</li> <li>Prioritize transparency and auditability: configuration-driven pipelines, provenance capture, and shareable analysis artifacts (HDF5/NetCDF).</li> </ul>"},{"location":"design/01_overview/#2-key-user-personas-and-use-cases","title":"2. Key user personas and use cases","text":"<ul> <li>Analytical chemist (R&amp;D): Explore spectra, apply baseline/ATR/atmospheric corrections, compare to reference libraries, extract peak ratios for authenticity checks, publishable figures.</li> <li>Food quality engineer (production/QC): Run validated pipelines on batches/hyperspectral maps, flag outliers or adulteration, generate reports and trend dashboards, integrate with LIMS/MES.</li> <li>Data scientist (ML/chemometrics): Build PCA/PLS/PLS-DA/SVM/RF/XGBoost models, optimize preprocessing chains, perform one-class novelty detection, deploy models as reproducible configs.</li> <li>Regulatory/compliance analyst: Trace model versions and datasets, reproduce decisions, export locked-down reports and provenance trails.</li> <li>Academia/education: Use example datasets and notebooks to teach spectroscopy fundamentals, band assignments, and chemometric workflows.</li> </ul>"},{"location":"design/01_overview/#3-scientific-requirements-spectroscopy-food-science","title":"3. Scientific requirements (spectroscopy + food science)","text":"<ul> <li>Pre-processing: Baseline correction (polynomial/AsLS/airPLS), smoothing (Savitzky-Golay), normalization (vector/area/peak), derivatives (1st/2nd), ATR correction, atmospheric (CO\u2082/H\u2082O) removal, cosmic ray handling.</li> <li>Feature extraction: Peak finding (adaptive prominence/width), band area/integration, peak ratios, spectral descriptors (centroid, FWHM, skewness, kurtosis), region-of-interest utilities.</li> <li>Chemometrics and ML: PCA, clustering (k-means/hierarchical/DBSCAN), PLS/PLS-DA, SVM, random forest, XGBoost, linear/nonlinear regression, one-class methods, mixture analysis (NNLS, MCR-ALS).</li> <li>Data modalities: Single spectra and hyperspectral maps (Raman/FTIR imaging); support wavelength/wavenumber axes, metadata (instrument, acquisition params), and mask handling.</li> <li>Food-specific modules: Edible oils adulteration and grading, oil uptake in fried products, heating/degradation markers, dairy/meat authentication, microbial detection, moisture/fat/protein estimation.</li> <li>Libraries/datasets: Interfaces to spectral libraries; packaged example datasets for each application module with metadata and expected outputs for tests/tutorials.</li> </ul>"},{"location":"design/01_overview/#4-software-requirements-architecture-quality-performance","title":"4. Software requirements (architecture, quality, performance)","text":"<ul> <li>Architecture: Modular packages for <code>io</code>, <code>preprocess</code>, <code>features</code>, <code>chemometrics</code>, <code>maps</code>, <code>applications</code>, <code>pipelines</code>, and <code>viz</code>; composable, stateless functions with optional pipeline objects/configs.</li> <li>Data model: Core spectral object for 1D spectra with axes, units, metadata; hyperspectral cube type with spatial axes and masks; immutable data classes with type hints and validation.</li> <li>APIs: Functional API for building chains; declarative configs (YAML/JSON) to define pipelines; I/O to HDF5/NetCDF for reproducibility; adapters for numpy/pandas/xarray.</li> <li>Quality: Full type hints, docstrings, doctests; unit/integration tests with fixtures for spectra and maps; CI running lint (ruff/flake8), type-check (mypy/pyright), tests, coverage, and docs build.</li> <li>Performance: Vectorized operations; optional numba/Cython for hot paths; chunked processing for hyperspectral maps; graceful degradation with progress reporting and caching.</li> <li>Docs and UX: User guides, API reference, cookbook notebooks; reproducible examples; CLI entry points for common tasks; plotting utilities for spectra, residuals, score/loadings, and map overlays.</li> </ul>"},{"location":"design/01_overview/#5-high-level-architecture-modules-data-model-pipelines","title":"5. High-level architecture (modules, data model, pipelines)","text":"<ul> <li>Core types: <code>Spectrum1D</code> (intensity, axis, units, metadata), <code>HyperMap</code> (cube with spatial dims, mask), <code>Provenance</code> (pipeline steps, versions), <code>Dataset</code> (collections with labels/splits).</li> <li>I/O (<code>io</code>): Read/write common vendor-neutral formats and exports to HDF5/NetCDF; dataset loaders for packaged examples; library adapters.</li> <li>Pre-processing (<code>preprocess</code>): Baseline, smoothing, normalization, derivatives, ATR/atmospheric corrections, cosmic ray removal; step objects with fit/transform for reproducibility.</li> <li>Feature extraction (<code>features</code>): Peak detection, band areas, ratios, spectral descriptors; ROI utilities and feature tables.</li> <li>Chemometrics/ML (<code>chemometrics</code>): PCA, clustering, PLS/PLS-DA, SVM, RF, XGBoost, regression, one-class methods, NNLS/MCR-ALS for mixtures; compatible with scikit-learn interfaces.</li> <li>Maps (<code>maps</code>): Hyperspectral processing (per-pixel or block), masking, spectral flattening, and map-level visualizations; memory-aware chunking.</li> <li>Pipelines/configs (<code>pipelines</code>): Declarative pipeline builder consuming configs; stores provenance; exports/imports pipeline definitions; deterministic seeding and version stamping.</li> <li>Applications (<code>applications</code>): Domain-specific workflows and reference configs for oils, oil-in-chips, heating degradation, dairy/meat authentication, microbial detection; bundled examples and benchmarks.</li> <li>Visualization (<code>viz</code>): Plotting helpers for spectra, baselines, residuals, PCA/PLS scores/loadings, clustering maps, classification outputs, and hyperspectral overlays.</li> </ul>"},{"location":"design/01_overview/#6-roadmap-phases","title":"6. Roadmap phases","text":"<ul> <li>MVP: Core data models; I/O for spectra; baseline/smoothing/normalization/derivatives; peak finding and band areas; PCA and PLS; simple config-driven pipelines; edible oils example dataset; basic docs and CLI.</li> <li>v1.0: Full pre-processing suite (ATR/atmospheric/cosmic); robust feature descriptors; clustering, SVM, RF, XGBoost; one-class novelty; NNLS mixture analysis; hyperspectral support with chunking; provenance/HDF5/NetCDF; expanded application modules and tutorials; CI with lint/type/test/docs.</li> <li>v2.0: MCR-ALS, advanced mixture and outlier methods; model registry/versioning; performance accelerators (numba/Cython); interactive dashboards; extended spectral library adapters; additional food matrices (fermentation, beverages), and validation datasets; deployment guides and API stability guarantees.</li> </ul>"}]}